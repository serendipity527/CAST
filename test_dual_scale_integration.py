"""
æµ‹è¯• DualScaleResidualHead åœ¨ TimeLLM ä¸­çš„é›†æˆ
éªŒè¯æ¨¡å‹èƒ½å¦æ­£ç¡®åŠ è½½å’Œä½¿ç”¨æ–°çš„è¾“å‡ºå¤´
"""

import torch
import argparse
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from models.TimeLLM import Model


def create_test_config():
    """åˆ›å»ºæµ‹è¯•é…ç½®"""
    class Config:
        def __init__(self):
            # åŸºæœ¬é…ç½®
            self.task_name = 'long_term_forecast'
            self.seq_len = 512
            self.pred_len = 96
            self.label_len = 48
            self.enc_in = 7
            self.dec_in = 7
            self.c_out = 7
            
            # æ¨¡å‹é…ç½®
            self.d_model = 64
            self.d_ff = 256
            self.n_heads = 8
            self.dropout = 0.1
            
            # LLM é…ç½®
            self.llm_model = 'GPT2'
            self.llm_dim = 768
            self.llm_layers = 6
            
            # Patch é…ç½®
            self.patch_len = 16
            self.stride = 8
            
            # å°æ³¢é…ç½®
            self.wavelet_mode = 'wist'
            self.wavelet_type = 'haar'
            self.wavelet_level = 2
            self.hf_dropout = 0.2
            self.mf_dropout = 0.2
            self.use_freq_attention = 1
            self.use_soft_threshold = 1
            self.pyramid_fusion = 1
            
            # è¾“å‡ºå¤´é…ç½® - æµ‹è¯•ä¸‰ç§æ¨¡å¼
            self.use_dual_scale_head = 0  # å°†åœ¨æµ‹è¯•ä¸­ä¿®æ”¹
            self.detail_dropout = 0.1
            
            self.use_freq_decoupled_head = 0  # å°†åœ¨æµ‹è¯•ä¸­ä¿®æ”¹
            self.mid_dropout = 0.2
            self.high_dropout = 0.5
            self.head_soft_threshold = 1
            self.head_soft_threshold_init = 0.1
            self.head_use_conv = 0
            self.use_deep_supervision = 0
            
            # å…¶ä»–
            self.prompt_domain = 0
            self.content = "Test content"
    
    return Config()


def test_model_creation_and_forward():
    """æµ‹è¯•æ¨¡å‹åˆ›å»ºå’Œå‰å‘ä¼ æ’­"""
    print("=" * 80)
    print("TimeLLM + DualScaleResidualHead é›†æˆæµ‹è¯•")
    print("=" * 80)
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"\nè®¾å¤‡: {device}")
    
    # æµ‹è¯•å‚æ•°
    B = 2  # å° batch é¿å…å†…å­˜é—®é¢˜
    
    # ========== æµ‹è¯• 1: FlattenHead (åŸç‰ˆ) ==========
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: FlattenHead (åŸç‰ˆ)")
    print("=" * 60)
    
    config = create_test_config()
    config.use_dual_scale_head = 0
    config.use_freq_decoupled_head = 0
    
    try:
        model_flatten = Model(config).to(device)
        
        # æ¨¡æ‹Ÿè¾“å…¥
        x_enc = torch.randn(B, config.seq_len, config.enc_in, device=device)
        x_mark_enc = torch.randn(B, config.seq_len, 4, device=device)  # æ—¶é—´ç‰¹å¾
        x_dec = torch.randn(B, config.label_len + config.pred_len, config.dec_in, device=device)
        x_mark_dec = torch.randn(B, config.label_len + config.pred_len, 4, device=device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output_flatten = model_flatten(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        print(f"âœ… FlattenHead è¾“å‡ºå½¢çŠ¶: {output_flatten.shape}")
        expected_shape = (B, config.pred_len, config.c_out)
        assert output_flatten.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output_flatten.shape}"
        
        flatten_params = sum(p.numel() for p in model_flatten.parameters())
        print(f"âœ… FlattenHead æ€»å‚æ•°: {flatten_params:,}")
        
    except Exception as e:
        print(f"âŒ FlattenHead æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # ========== æµ‹è¯• 2: DualScaleResidualHead ==========
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: DualScaleResidualHead")
    print("=" * 60)
    
    config = create_test_config()
    config.use_dual_scale_head = 1
    config.use_freq_decoupled_head = 0
    
    try:
        model_dual = Model(config).to(device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output_dual = model_dual(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        print(f"âœ… DualScaleHead è¾“å‡ºå½¢çŠ¶: {output_dual.shape}")
        assert output_dual.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output_dual.shape}"
        
        dual_params = sum(p.numel() for p in model_dual.parameters())
        print(f"âœ… DualScaleHead æ€»å‚æ•°: {dual_params:,}")
        print(f"âœ… å‚æ•°å¢åŠ : {dual_params - flatten_params:,} ({(dual_params/flatten_params-1)*100:.2f}%)")
        
    except Exception as e:
        print(f"âŒ DualScaleHead æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # ========== æµ‹è¯• 3: TriBandDecoupledHead (å¯¹æ¯”) ==========
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: TriBandDecoupledHead (å¯¹æ¯”)")
    print("=" * 60)
    
    config = create_test_config()
    config.use_dual_scale_head = 0
    config.use_freq_decoupled_head = 1
    
    try:
        model_triband = Model(config).to(device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output_triband = model_triband(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        print(f"âœ… TriBandHead è¾“å‡ºå½¢çŠ¶: {output_triband.shape}")
        assert output_triband.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output_triband.shape}"
        
        triband_params = sum(p.numel() for p in model_triband.parameters())
        print(f"âœ… TriBandHead æ€»å‚æ•°: {triband_params:,}")
        print(f"âœ… ç›¸æ¯” Flatten: {triband_params - flatten_params:,} ({(triband_params/flatten_params-1)*100:.2f}%)")
        
    except Exception as e:
        print(f"âŒ TriBandHead æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # ========== æµ‹è¯• 4: ä¼˜å…ˆçº§éªŒè¯ ==========
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: ä¼˜å…ˆçº§éªŒè¯ (DualScale ä¼˜äº TriBand)")
    print("=" * 60)
    
    config = create_test_config()
    config.use_dual_scale_head = 1  # åŒæ—¶å¼€å¯ä¸¤ä¸ª
    config.use_freq_decoupled_head = 1
    
    try:
        model_priority = Model(config).to(device)
        
        # æ£€æŸ¥å®é™…ä½¿ç”¨çš„è¾“å‡ºå¤´ç±»å‹
        output_head_type = type(model_priority.output_projection).__name__
        print(f"âœ… åŒæ—¶å¼€å¯æ—¶ä½¿ç”¨: {output_head_type}")
        assert output_head_type == 'DualScaleResidualHead', f"ä¼˜å…ˆçº§é”™è¯¯ï¼Œåº”è¯¥ä½¿ç”¨ DualScaleResidualHead"
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output_priority = model_priority(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        print(f"âœ… ä¼˜å…ˆçº§æµ‹è¯•è¾“å‡ºå½¢çŠ¶: {output_priority.shape}")
        assert output_priority.shape == expected_shape
        
    except Exception as e:
        print(f"âŒ ä¼˜å…ˆçº§æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # ========== æµ‹è¯• 5: å†…å­˜å’Œæ€§èƒ½å¯¹æ¯” ==========
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: å†…å­˜å’Œæ€§èƒ½å¯¹æ¯”")
    print("=" * 60)
    
    def benchmark_model(model, name, x_enc, x_mark_enc, x_dec, x_mark_dec, rounds=10):
        """ç®€å•çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        import time
        
        model.eval()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(3):
                _ = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        # è®¡æ—¶
        start_time = time.time()
        with torch.no_grad():
            for _ in range(rounds):
                output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / rounds
        return avg_time, output
    
    flatten_time, _ = benchmark_model(model_flatten, "FlattenHead", x_enc, x_mark_enc, x_dec, x_mark_dec)
    dual_time, _ = benchmark_model(model_dual, "DualScaleHead", x_enc, x_mark_enc, x_dec, x_mark_dec)
    triband_time, _ = benchmark_model(model_triband, "TriBandHead", x_enc, x_mark_enc, x_dec, x_mark_dec)
    
    print(f"â±ï¸  FlattenHead å¹³å‡æ¨ç†æ—¶é—´: {flatten_time:.4f}s")
    print(f"â±ï¸  DualScaleHead å¹³å‡æ¨ç†æ—¶é—´: {dual_time:.4f}s (ç›¸å¯¹: {dual_time/flatten_time:.2f}x)")
    print(f"â±ï¸  TriBandHead å¹³å‡æ¨ç†æ—¶é—´: {triband_time:.4f}s (ç›¸å¯¹: {triband_time/flatten_time:.2f}x)")
    
    # ========== æ€»ç»“ ==========
    print("\n" + "=" * 80)
    print("ğŸ“Š é›†æˆæµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print(f"âœ… æ‰€æœ‰è¾“å‡ºå¤´ç±»å‹å·¥ä½œæ­£å¸¸")
    print(f"âœ… è¾“å‡ºå½¢çŠ¶ä¸€è‡´: {expected_shape}")
    print(f"âœ… ä¼˜å…ˆçº§é€»è¾‘æ­£ç¡®: DualScale > TriBand > Flatten")
    print(f"âœ… å‚æ•°é‡å¯¹æ¯”:")
    print(f"   - FlattenHead: {flatten_params:,}")
    print(f"   - DualScaleHead: {dual_params:,} (+{(dual_params/flatten_params-1)*100:.1f}%)")
    print(f"   - TriBandHead: {triband_params:,} (+{(triband_params/flatten_params-1)*100:.1f}%)")
    print(f"âœ… æ€§èƒ½å¯¹æ¯” (ç›¸å¯¹ FlattenHead):")
    print(f"   - DualScaleHead: {dual_time/flatten_time:.2f}x")
    print(f"   - TriBandHead: {triband_time/flatten_time:.2f}x")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    success = test_model_creation_and_forward()
    
    if success:
        print("\nğŸ‰ TimeLLM + DualScaleResidualHead é›†æˆæˆåŠŸï¼")
        print("\nğŸ“‹ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•å®é™…è®­ç»ƒ:")
        print("HF_ENDPOINT=https://hf-mirror.com CUDA_VISIBLE_DEVICES=4 python run_main.py \\")
        print("  --task_name long_term_forecast --is_training 1 \\")
        print("  --root_path ./dataset/ETT-small --data_path ETTh1.csv \\")
        print("  --model_id ETTh1_512_96 --model TimeLLM --data ETTh1 --features M \\")
        print("  --seq_len 512 --label_len 48 --pred_len 96 \\")
        print("  --d_model 64 --d_ff 256 --batch_size 24 --learning_rate 0.0001 \\")
        print("  --llm_model GPT2 --llm_dim 768 --llm_layers 6 --train_epochs 15 \\")
        print("  --wavelet_mode=wist --wavelet_type=haar --wavelet_level=2 \\")
        print("  --use_dual_scale_head=1 --detail_dropout=0.1 \\")
        print("  --model_comment 'WIST-PE-haar-DualScaleHead'")
        print("\n" + "=" * 80)
    else:
        print("\nâŒ é›†æˆæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
