#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯• FrequencyChannelAttentionV2 æ¨¡å—

éªŒè¯å†…å®¹:
1. æ¨¡å—èƒ½å¤Ÿæ­£å¸¸åˆå§‹åŒ–
2. å‰å‘ä¼ æ’­çš„è¾“å…¥è¾“å‡ºå½¢çŠ¶æ­£ç¡®
3. V1 å’Œ V2 çš„è¾“å‡ºå½¢çŠ¶ä¸€è‡´ï¼ˆä½†æƒé‡å½¢çŠ¶ä¸åŒï¼‰
4. V2 çš„æ³¨æ„åŠ›æƒé‡æ˜¯ Patch-wise çš„ï¼ˆæ¯ä¸ª Patch æœ‰ç‹¬ç«‹çš„é¢‘ç‡æƒé‡ï¼‰
5. æ¢¯åº¦èƒ½å¤Ÿæ­£å¸¸åå‘ä¼ æ’­
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
from layers.Embed import FrequencyChannelAttention, FrequencyChannelAttentionV2


def test_basic_forward():
    """æµ‹è¯•åŸºæœ¬çš„å‰å‘ä¼ æ’­"""
    print("=" * 60)
    print("æµ‹è¯• 1: åŸºæœ¬å‰å‘ä¼ æ’­")
    print("=" * 60)
    
    # å‚æ•°è®¾ç½®
    batch_size = 4
    n_vars = 7
    num_patches = 64
    d_model = 32
    num_bands = 3  # ä¾‹å¦‚ level=2: cA, cD_2, cD_1
    
    B_N = batch_size * n_vars
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„é¢‘æ®µ embeddings
    band_embeddings = [
        torch.randn(B_N, num_patches, d_model) for _ in range(num_bands)
    ]
    
    # åˆå§‹åŒ– V2 æ¨¡å—
    print("\nåˆå§‹åŒ– FrequencyChannelAttentionV2...")
    attn_v2 = FrequencyChannelAttentionV2(
        num_bands=num_bands,
        d_model=d_model,
        reduction=4,
        kernel_size=3
    )
    
    # å‰å‘ä¼ æ’­
    output, attention_weights = attn_v2(band_embeddings)
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_output_shape = (B_N, num_patches, d_model)
    expected_weight_shape = (B_N, num_patches, num_bands)  # V2 æ˜¯ Patch-wise
    
    print(f"\nè¾“å…¥: {num_bands} ä¸ªé¢‘æ®µ, æ¯ä¸ªå½¢çŠ¶ ({B_N}, {num_patches}, {d_model})")
    print(f"è¾“å‡ºå½¢çŠ¶: {tuple(output.shape)}, æœŸæœ›: {expected_output_shape}")
    print(f"æƒé‡å½¢çŠ¶: {tuple(attention_weights.shape)}, æœŸæœ›: {expected_weight_shape}")
    
    assert output.shape == expected_output_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape} != {expected_output_shape}"
    assert attention_weights.shape == expected_weight_shape, f"æƒé‡å½¢çŠ¶é”™è¯¯: {attention_weights.shape} != {expected_weight_shape}"
    
    print("âœ… æµ‹è¯• 1 é€šè¿‡!")
    return True


def test_v1_vs_v2_comparison():
    """å¯¹æ¯” V1 å’Œ V2 çš„å·®å¼‚"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: V1 vs V2 å¯¹æ¯”")
    print("=" * 60)
    
    # å‚æ•°è®¾ç½®
    batch_size = 2
    n_vars = 3
    num_patches = 32
    d_model = 64
    num_bands = 2  # åŒé€šé“
    
    B_N = batch_size * n_vars
    
    # åˆ›å»ºç›¸åŒçš„è¾“å…¥
    torch.manual_seed(42)
    band_embeddings = [
        torch.randn(B_N, num_patches, d_model) for _ in range(num_bands)
    ]
    
    # åˆå§‹åŒ– V1 å’Œ V2
    print("\nåˆå§‹åŒ– V1 (GAP)...")
    attn_v1 = FrequencyChannelAttention(
        num_bands=num_bands,
        d_model=d_model,
        reduction=4
    )
    
    print("\nåˆå§‹åŒ– V2 (1D Conv)...")
    attn_v2 = FrequencyChannelAttentionV2(
        num_bands=num_bands,
        d_model=d_model,
        reduction=4,
        kernel_size=3
    )
    
    # å‰å‘ä¼ æ’­
    output_v1, weights_v1 = attn_v1(band_embeddings)
    output_v2, weights_v2 = attn_v2(band_embeddings)
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶ä¸€è‡´
    print(f"\nV1 è¾“å‡ºå½¢çŠ¶: {tuple(output_v1.shape)}")
    print(f"V2 è¾“å‡ºå½¢çŠ¶: {tuple(output_v2.shape)}")
    assert output_v1.shape == output_v2.shape, "V1 å’Œ V2 çš„è¾“å‡ºå½¢çŠ¶åº”è¯¥ä¸€è‡´"
    
    # éªŒè¯æƒé‡å½¢çŠ¶å·®å¼‚
    print(f"\nV1 æƒé‡å½¢çŠ¶: {tuple(weights_v1.shape)} (Instance-wise: æ‰€æœ‰ Patch å…±äº«æƒé‡)")
    print(f"V2 æƒé‡å½¢çŠ¶: {tuple(weights_v2.shape)} (Patch-wise: æ¯ä¸ª Patch ç‹¬ç«‹æƒé‡)")
    
    # V1: (B*N, num_bands)
    # V2: (B*N, num_patches, num_bands)
    assert weights_v1.shape == (B_N, num_bands), f"V1 æƒé‡å½¢çŠ¶é”™è¯¯"
    assert weights_v2.shape == (B_N, num_patches, num_bands), f"V2 æƒé‡å½¢çŠ¶é”™è¯¯"
    
    # éªŒè¯æƒé‡å’Œä¸º 1
    print(f"\nV1 æƒé‡å’Œ (åº”ä¸º 1.0): {weights_v1.sum(dim=-1).mean().item():.4f}")
    print(f"V2 æƒé‡å’Œ (åº”ä¸º 1.0): {weights_v2.sum(dim=-1).mean().item():.4f}")
    
    assert torch.allclose(weights_v1.sum(dim=-1), torch.ones(B_N), atol=1e-5), "V1 æƒé‡å’Œåº”ä¸º 1"
    assert torch.allclose(weights_v2.sum(dim=-1), torch.ones(B_N, num_patches), atol=1e-5), "V2 æƒé‡å’Œåº”ä¸º 1"
    
    print("âœ… æµ‹è¯• 2 é€šè¿‡!")
    return True


def test_patch_wise_weights():
    """éªŒè¯ V2 çš„ Patch-wise æƒé‡ç¡®å®æ˜¯ä¸åŒçš„"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: Patch-wise æƒé‡å·®å¼‚æ€§")
    print("=" * 60)
    
    # å‚æ•°è®¾ç½®
    batch_size = 1
    n_vars = 1
    num_patches = 16
    d_model = 32
    num_bands = 2
    
    B_N = batch_size * n_vars
    
    # åˆ›å»ºæœ‰æ˜æ˜¾å·®å¼‚çš„è¾“å…¥
    # å‰åŠéƒ¨åˆ† Patch é«˜é¢‘ä¸»å¯¼ï¼ŒååŠéƒ¨åˆ†ä½é¢‘ä¸»å¯¼
    torch.manual_seed(123)
    
    low_freq = torch.randn(B_N, num_patches, d_model)
    high_freq = torch.randn(B_N, num_patches, d_model)
    
    # è®©å‰åŠéƒ¨åˆ†é«˜é¢‘ä¿¡å·æ›´å¼º
    high_freq[:, :num_patches//2, :] *= 5.0
    # è®©ååŠéƒ¨åˆ†ä½é¢‘ä¿¡å·æ›´å¼º
    low_freq[:, num_patches//2:, :] *= 5.0
    
    band_embeddings = [low_freq, high_freq]
    
    # åˆå§‹åŒ– V2
    attn_v2 = FrequencyChannelAttentionV2(
        num_bands=num_bands,
        d_model=d_model,
        reduction=4,
        kernel_size=3
    )
    
    # è®¾ç½®ä¸º eval æ¨¡å¼ä»¥è·å¾—ç¡®å®šæ€§è¾“å‡º
    attn_v2.eval()
    
    with torch.no_grad():
        output, weights = attn_v2(band_embeddings)
    
    # æ£€æŸ¥ä¸åŒ Patch çš„æƒé‡æ˜¯å¦ä¸åŒ
    weights_first_half = weights[:, :num_patches//2, :].mean(dim=1)  # (B_N, num_bands)
    weights_second_half = weights[:, num_patches//2:, :].mean(dim=1)  # (B_N, num_bands)
    
    print(f"\nå‰åŠéƒ¨åˆ† Patch çš„å¹³å‡æƒé‡ (ä½é¢‘, é«˜é¢‘): {weights_first_half[0].tolist()}")
    print(f"ååŠéƒ¨åˆ† Patch çš„å¹³å‡æƒé‡ (ä½é¢‘, é«˜é¢‘): {weights_second_half[0].tolist()}")
    
    # æ£€æŸ¥æƒé‡æ˜¯å¦æœ‰å˜åŒ–ï¼ˆä¸æ˜¯æ‰€æœ‰ Patch éƒ½ä¸€æ ·ï¼‰
    weight_variance = weights.var(dim=1).mean()
    print(f"\næƒé‡åœ¨ Patch ç»´åº¦ä¸Šçš„æ–¹å·®: {weight_variance.item():.6f}")
    
    # åªè¦æ–¹å·®ä¸ä¸º 0ï¼Œå°±è¯´æ˜ä¸åŒ Patch æœ‰ä¸åŒçš„æƒé‡
    # æ³¨æ„ï¼šç”±äºåˆå§‹åŒ–æ˜¯å‡åŒ€çš„ï¼Œåˆå§‹æ–¹å·®å¯èƒ½å¾ˆå°ï¼Œä½†ä¸åº”è¯¥ä¸º 0
    print(f"æƒé‡æ˜¯å¦æœ‰ Patch çº§åˆ«çš„å·®å¼‚: {'æ˜¯' if weight_variance > 0 else 'å¦'}")
    
    print("âœ… æµ‹è¯• 3 é€šè¿‡!")
    return True


def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦èƒ½å¦æ­£å¸¸åå‘ä¼ æ’­"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: æ¢¯åº¦åå‘ä¼ æ’­")
    print("=" * 60)
    
    # å‚æ•°è®¾ç½®
    batch_size = 2
    n_vars = 3
    num_patches = 16
    d_model = 32
    num_bands = 3
    
    B_N = batch_size * n_vars
    
    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„è¾“å…¥
    band_embeddings = [
        torch.randn(B_N, num_patches, d_model, requires_grad=True) 
        for _ in range(num_bands)
    ]
    
    # åˆå§‹åŒ– V2
    attn_v2 = FrequencyChannelAttentionV2(
        num_bands=num_bands,
        d_model=d_model,
        reduction=4,
        kernel_size=3
    )
    
    # å‰å‘ä¼ æ’­
    output, weights = attn_v2(band_embeddings)
    
    # è®¡ç®—ä¸€ä¸ªç®€å•çš„ loss
    loss = output.sum()
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ¢¯åº¦
    print("\næ£€æŸ¥è¾“å…¥æ¢¯åº¦:")
    for i, emb in enumerate(band_embeddings):
        has_grad = emb.grad is not None and emb.grad.abs().sum() > 0
        print(f"  é¢‘æ®µ {i} æ¢¯åº¦: {'âœ… æœ‰' if has_grad else 'âŒ æ— '}")
        assert has_grad, f"é¢‘æ®µ {i} åº”è¯¥æœ‰æ¢¯åº¦"
    
    # æ£€æŸ¥æ¨¡å—å‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦
    print("\næ£€æŸ¥æ¨¡å—å‚æ•°æ¢¯åº¦:")
    for name, param in attn_v2.named_parameters():
        has_grad = param.grad is not None and param.grad.abs().sum() > 0
        print(f"  {name}: {'âœ… æœ‰' if has_grad else 'âŒ æ— '}")
    
    print("âœ… æµ‹è¯• 4 é€šè¿‡!")
    return True


def test_different_kernel_sizes():
    """æµ‹è¯•ä¸åŒçš„å·ç§¯æ ¸å¤§å°"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: ä¸åŒå·ç§¯æ ¸å¤§å°")
    print("=" * 60)
    
    # å‚æ•°è®¾ç½®
    batch_size = 2
    n_vars = 2
    num_patches = 32
    d_model = 32
    num_bands = 2
    
    B_N = batch_size * n_vars
    
    kernel_sizes = [1, 3, 5, 7]
    
    for ks in kernel_sizes:
        print(f"\næµ‹è¯• kernel_size={ks}...")
        
        # åˆ›å»ºè¾“å…¥
        band_embeddings = [
            torch.randn(B_N, num_patches, d_model) for _ in range(num_bands)
        ]
        
        # åˆå§‹åŒ–
        attn_v2 = FrequencyChannelAttentionV2(
            num_bands=num_bands,
            d_model=d_model,
            reduction=4,
            kernel_size=ks
        )
        
        # å‰å‘ä¼ æ’­
        output, weights = attn_v2(band_embeddings)
        
        # éªŒè¯å½¢çŠ¶
        assert output.shape == (B_N, num_patches, d_model), f"kernel_size={ks} è¾“å‡ºå½¢çŠ¶é”™è¯¯"
        assert weights.shape == (B_N, num_patches, num_bands), f"kernel_size={ks} æƒé‡å½¢çŠ¶é”™è¯¯"
        
        print(f"  âœ… kernel_size={ks} é€šè¿‡")
    
    print("\nâœ… æµ‹è¯• 5 é€šè¿‡!")
    return True


def test_parameter_count():
    """å¯¹æ¯” V1 å’Œ V2 çš„å‚æ•°é‡"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: å‚æ•°é‡å¯¹æ¯”")
    print("=" * 60)
    
    d_model = 64
    num_bands = 3
    
    attn_v1 = FrequencyChannelAttention(num_bands=num_bands, d_model=d_model, reduction=4)
    attn_v2 = FrequencyChannelAttentionV2(num_bands=num_bands, d_model=d_model, reduction=4, kernel_size=3)
    
    params_v1 = sum(p.numel() for p in attn_v1.parameters())
    params_v2 = sum(p.numel() for p in attn_v2.parameters())
    
    print(f"\nV1 å‚æ•°é‡: {params_v1:,}")
    print(f"V2 å‚æ•°é‡: {params_v2:,}")
    print(f"V2 ç›¸æ¯” V1 å¢åŠ : {params_v2 - params_v1:,} ({(params_v2/params_v1 - 1)*100:.1f}%)")
    
    print("\nâœ… æµ‹è¯• 6 é€šè¿‡!")
    return True


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("FrequencyChannelAttentionV2 æ¨¡å—æµ‹è¯•")
    print("=" * 60)
    
    all_passed = True
    
    try:
        all_passed &= test_basic_forward()
        all_passed &= test_v1_vs_v2_comparison()
        all_passed &= test_patch_wise_weights()
        all_passed &= test_gradient_flow()
        all_passed &= test_different_kernel_sizes()
        all_passed &= test_parameter_count()
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        all_passed = False
    
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
    print("=" * 60)
