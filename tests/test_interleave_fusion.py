#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
äº¤é”™æ‹¼æ¥èåˆæµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. DualReprogrammingLayer çš„ interleave èåˆæ–¹æ³•
2. åºåˆ—é•¿åº¦ç¿»å€éªŒè¯
3. è¾“å‡ºå¤´é€‚é…éªŒè¯
4. ç«¯åˆ°ç«¯æµ‹è¯•
5. ä¸å…¶ä»–èåˆæ–¹æ³•å¯¹æ¯”
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
from transformers import GPT2Tokenizer, GPT2Model

from models.TimeLLM import Model, DualReprogrammingLayer


class TestConfig:
    """æµ‹è¯•é…ç½®ç±»"""
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.task_name = 'long_term_forecast'
        self.llm_model = 'GPT2'
        self.llm_dim = 768
        self.llm_layers = 2
        self.d_model = 16
        self.n_heads = 4
        self.d_ff = 32
        self.dropout = 0.1
        self.patch_len = 16
        self.stride = 8
        self.seq_len = 96
        self.pred_len = 96
        self.enc_in = 7
        self.dec_in = 7
        self.c_out = 7
        
        # å°æ³¢é…ç½®
        self.wavelet_mode = 'wist'
        self.wavelet_type = 'haar'
        self.wavelet_level = 2
        
        # åˆ†ç¦»åŸå‹é…ç½®
        self.use_dual_prototypes = 1
        self.dual_proto_trend_tokens = 500
        self.dual_proto_detail_tokens = 500
        self.dual_proto_fusion_method = 'interleave'
        self.dual_proto_gate_bias_init = 0.0
        
        # è¯­ä¹‰ç­›é€‰æ˜ å°„é…ç½®
        self.use_semantic_filtered_mapping = 1
        self.dual_proto_trend_seed_words = 300
        self.dual_proto_detail_seed_words = 700
        self.dual_proto_seed_semantic_filter = 1
        
        # MLPæ˜ å°„å±‚é…ç½®
        self.dual_proto_mlp_hidden_dim = 2048  # æµ‹è¯•æ—¶ä½¿ç”¨è¾ƒå°çš„ç»´åº¦
        self.dual_proto_mlp_dropout = 0.1
        
        # Prompté…ç½®
        self.prompt_domain = 0
        self.content = 'Test dataset description'
        
        # å…¶ä»–é…ç½®
        self.use_cwpr = 0
        self.use_dual_scale_head = 0
        self.use_freq_decoupled_head = 0


def test_interleave_fusion_basic():
    """æµ‹è¯•1: äº¤é”™æ‹¼æ¥åŸºæœ¬åŠŸèƒ½"""
    print("=" * 70)
    print("æµ‹è¯•1: äº¤é”™æ‹¼æ¥åŸºæœ¬åŠŸèƒ½")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»º DualReprogrammingLayer
    layer = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='interleave',
        gate_bias_init=0.0
    )
    
    print(f"\nèåˆæ–¹æ³•: {layer.fusion_method}")
    
    if layer.fusion_method != 'interleave':
        print(f"âŒ èåˆæ–¹æ³•ä¸æ­£ç¡®: {layer.fusion_method} != interleave")
        return False
    print("âœ… èåˆæ–¹æ³•æ­£ç¡®: interleave")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  - trend_embedding: {trend_embedding.shape}")
    print(f"  - detail_embedding: {detail_embedding.shape}")
    print(f"  - trend_prototypes: {trend_prototypes.shape}")
    print(f"  - detail_prototypes: {detail_prototypes.shape}")
    
    # å‰å‘ä¼ æ’­
    layer.eval()
    with torch.no_grad():
        output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"é¢„æœŸå½¢çŠ¶: ({batch_size}, {2*seq_len}, {d_llm})")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶ï¼šåºåˆ—é•¿åº¦åº”è¯¥ç¿»å€
    if output.shape != (batch_size, 2*seq_len, d_llm):
        print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®: {output.shape} != ({batch_size}, {2*seq_len}, {d_llm})")
        return False
    print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®ï¼ˆåºåˆ—é•¿åº¦ç¿»å€ï¼‰")
    
    # éªŒè¯è¾“å‡ºå€¼
    if torch.isnan(output).any():
        print("âŒ è¾“å‡ºåŒ…å«NaNå€¼")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— NaNï¼‰")
    
    if torch.isinf(output).any():
        print("âŒ è¾“å‡ºåŒ…å«Infå€¼")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— Infï¼‰")
    
    return True


def test_interleave_ordering():
    """æµ‹è¯•2: éªŒè¯äº¤é”™é¡ºåº"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•2: éªŒè¯äº¤é”™é¡ºåº [T1, D1, T2, D2, ...]")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 1
    seq_len = 5
    num_prototypes = 100
    
    # åˆ›å»ºå±‚
    layer = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='interleave',
        gate_bias_init=0.0
    )
    
    # åˆ›å»ºç‰¹æ®Šçš„æµ‹è¯•è¾“å…¥ï¼šè¶‹åŠ¿å’Œç»†èŠ‚æœ‰æ˜æ˜¾åŒºåˆ«
    trend_embedding = torch.ones(batch_size, seq_len, d_model) * 1.0
    detail_embedding = torch.ones(batch_size, seq_len, d_model) * 2.0
    
    # åˆ›å»ºç®€å•çš„åŸå‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    trend_prototypes = torch.eye(d_llm)[:num_prototypes]  # å•ä½çŸ©é˜µ
    detail_prototypes = torch.eye(d_llm)[:num_prototypes] * 2  # 2å€å•ä½çŸ©é˜µ
    
    layer.eval()
    with torch.no_grad():
        output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"é¢„æœŸ: (1, {2*seq_len}, {d_llm})")
    
    # éªŒè¯äº¤é”™é¡ºåºï¼šæ£€æŸ¥ç›¸é‚»ä½ç½®çš„å·®å¼‚
    # ç”±äºåŸå‹ä¸åŒï¼Œè¶‹åŠ¿å’Œç»†èŠ‚çš„è¾“å‡ºåº”è¯¥ä¸åŒ
    print(f"\nè¾“å‡ºç»Ÿè®¡:")
    print(f"  - ä½ç½®0 (åº”è¯¥æ˜¯T1): å‡å€¼={output[0, 0, :].mean().item():.6f}")
    print(f"  - ä½ç½®1 (åº”è¯¥æ˜¯D1): å‡å€¼={output[0, 1, :].mean().item():.6f}")
    print(f"  - ä½ç½®2 (åº”è¯¥æ˜¯T2): å‡å€¼={output[0, 2, :].mean().item():.6f}")
    print(f"  - ä½ç½®3 (åº”è¯¥æ˜¯D2): å‡å€¼={output[0, 3, :].mean().item():.6f}")
    
    # éªŒè¯ç›¸é‚»ä½ç½®ä¸åŒï¼ˆå› ä¸ºè¾“å…¥ä¸åŒï¼Œè¾“å‡ºåº”è¯¥ä¸åŒï¼‰
    if torch.allclose(output[0, 0, :], output[0, 1, :], atol=1e-3):
        print("âš ï¸  è­¦å‘Š: ç›¸é‚»ä½ç½®è¾“å‡ºè¿‡äºæ¥è¿‘ï¼Œå¯èƒ½äº¤é”™é¡ºåºæœ‰é—®é¢˜")
    else:
        print("âœ… ç›¸é‚»ä½ç½®è¾“å‡ºä¸åŒï¼ˆäº¤é”™é¡ºåºæ­£ç¡®ï¼‰")
    
    # éªŒè¯å¶æ•°ä½ç½®å’Œå¥‡æ•°ä½ç½®çš„æ¨¡å¼
    even_positions = output[0, 0::2, :]  # T1, T2, T3, ...
    odd_positions = output[0, 1::2, :]   # D1, D2, D3, ...
    
    print(f"\nå¶æ•°ä½ç½®ï¼ˆè¶‹åŠ¿ï¼‰ç»Ÿè®¡: å‡å€¼={even_positions.mean().item():.6f}, æ ‡å‡†å·®={even_positions.std().item():.6f}")
    print(f"å¥‡æ•°ä½ç½®ï¼ˆç»†èŠ‚ï¼‰ç»Ÿè®¡: å‡å€¼={odd_positions.mean().item():.6f}, æ ‡å‡†å·®={odd_positions.std().item():.6f}")
    
    return True


def test_fusion_methods_comparison():
    """æµ‹è¯•3: ä¸åŒèåˆæ–¹æ³•å¯¹æ¯”"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•3: ä¸åŒèåˆæ–¹æ³•å¯¹æ¯”")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»ºç›¸åŒçš„è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    # æµ‹è¯•ä¸åŒçš„èåˆæ–¹æ³•
    fusion_methods = ['mean', 'weighted', 'adaptive_gate', 'interleave']
    results = {}
    
    for method in fusion_methods:
        layer = DualReprogrammingLayer(
            d_model=d_model,
            n_heads=n_heads,
            d_keys=d_model // n_heads,
            d_llm=d_llm,
            attention_dropout=0.1,
            fusion_method=method,
            gate_bias_init=0.0
        )
        layer.eval()
        
        with torch.no_grad():
            output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
        
        results[method] = {
            'output': output,
            'shape': output.shape,
            'mean': output.mean().item(),
            'std': output.std().item(),
        }
        
        print(f"\n{method} èåˆ:")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  ç»Ÿè®¡: å‡å€¼={results[method]['mean']:.6f}, æ ‡å‡†å·®={results[method]['std']:.6f}")
    
    # éªŒè¯ interleave çš„åºåˆ—é•¿åº¦æ˜¯å…¶ä»–çš„2å€
    interleave_shape = results['interleave']['shape']
    other_shape = results['mean']['shape']
    
    if interleave_shape[1] == 2 * other_shape[1]:
        print(f"\nâœ… interleave åºåˆ—é•¿åº¦æ­£ç¡®: {interleave_shape[1]} = 2 Ã— {other_shape[1]}")
    else:
        print(f"\nâŒ interleave åºåˆ—é•¿åº¦ä¸æ­£ç¡®: {interleave_shape[1]} != 2 Ã— {other_shape[1]}")
        return False
    
    # éªŒè¯ä¸åŒæ–¹æ³•äº§ç”Ÿä¸åŒçš„è¾“å‡º
    mean_output = results['mean']['output']
    interleave_output = results['interleave']['output']
    
    # interleave çš„å‰ L ä¸ªä½ç½®åº”è¯¥ä¸ mean ä¸åŒï¼ˆå› ä¸ºå®ƒä»¬æ˜¯ç‹¬ç«‹çš„è¶‹åŠ¿å’Œç»†èŠ‚ï¼‰
    if torch.allclose(mean_output, interleave_output[:, :seq_len, :], atol=1e-3):
        print("âš ï¸  è­¦å‘Š: interleave çš„å‰åŠéƒ¨åˆ†ä¸ mean è¿‡äºæ¥è¿‘")
    else:
        print("âœ… interleave è¾“å‡ºä¸ mean ä¸åŒï¼ˆç¬¦åˆé¢„æœŸï¼‰")
    
    return True


def test_output_head_adaptation():
    """æµ‹è¯•4: è¾“å‡ºå¤´é€‚é…éªŒè¯"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•4: è¾“å‡ºå¤´é€‚é…éªŒè¯")
    print("=" * 70)
    
    configs = TestConfig()
    
    try:
        model = Model(configs)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æ£€æŸ¥ head_nf æ˜¯å¦æ­£ç¡®ç¿»å€
    expected_head_nf = configs.d_ff * 2 * model.patch_nums
    actual_head_nf = model.head_nf
    
    print(f"\n[æ£€æŸ¥1] head_nf è®¡ç®—:")
    print(f"  - patch_nums: {model.patch_nums}")
    print(f"  - d_ff: {configs.d_ff}")
    print(f"  - é¢„æœŸ head_nf (2*patch_nums*d_ff): {expected_head_nf}")
    print(f"  - å®é™… head_nf: {actual_head_nf}")
    
    if actual_head_nf != expected_head_nf:
        print(f"âŒ head_nf ä¸æ­£ç¡®: {actual_head_nf} != {expected_head_nf}")
        return False
    print("âœ… head_nf æ­£ç¡®ï¼ˆå·²ç¿»å€ï¼‰")
    
    # æ£€æŸ¥è¾“å‡ºå¤´çš„è¾“å…¥ç»´åº¦
    if hasattr(model.output_projection, 'linear'):
        # FlattenHead
        linear_in_features = model.output_projection.linear.in_features
        print(f"\n[æ£€æŸ¥2] FlattenHead Linear è¾“å…¥ç»´åº¦:")
        print(f"  - å®é™…: {linear_in_features}")
        print(f"  - é¢„æœŸ: {expected_head_nf}")
        
        if linear_in_features != expected_head_nf:
            print(f"âŒ FlattenHead Linear è¾“å…¥ç»´åº¦ä¸æ­£ç¡®")
            return False
        print("âœ… FlattenHead Linear è¾“å…¥ç»´åº¦æ­£ç¡®")
    
    return True


def test_end_to_end():
    """æµ‹è¯•5: ç«¯åˆ°ç«¯æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•5: ç«¯åˆ°ç«¯æµ‹è¯•ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰")
    print("=" * 70)
    
    configs = TestConfig()
    
    try:
        model = Model(configs)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æ£€æŸ¥èåˆæ–¹æ³•
    if hasattr(model, 'fusion_method') and model.fusion_method == 'interleave':
        print(f"âœ… èåˆæ–¹æ³•æ­£ç¡®: {model.fusion_method}")
    else:
        print(f"âŒ èåˆæ–¹æ³•ä¸æ­£ç¡®æˆ–æœªè®¾ç½®")
        return False
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    x_enc = torch.randn(batch_size, configs.seq_len, configs.enc_in)
    x_mark_enc = torch.zeros(batch_size, configs.seq_len, 4)
    x_dec = torch.randn(batch_size, configs.pred_len, configs.enc_in)
    x_mark_dec = torch.zeros(batch_size, configs.pred_len, 4)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  - x_enc: {x_enc.shape}")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    try:
        with torch.no_grad():
            output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"é¢„æœŸå½¢çŠ¶: ({batch_size}, {configs.pred_len}, {configs.enc_in})")
        
        if output.shape != (batch_size, configs.pred_len, configs.enc_in):
            print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…")
            return False
        print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
        
        # æ£€æŸ¥è¾“å‡ºå€¼
        if torch.isnan(output).any():
            print("âŒ è¾“å‡ºåŒ…å«NaNå€¼")
            return False
        print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— NaNï¼‰")
        
        if torch.isinf(output).any():
            print("âŒ è¾“å‡ºåŒ…å«Infå€¼")
            return False
        print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— Infï¼‰")
        
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def test_sequence_length_doubling():
    """æµ‹è¯•6: åºåˆ—é•¿åº¦ç¿»å€éªŒè¯"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•6: åºåˆ—é•¿åº¦ç¿»å€éªŒè¯")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»ºå±‚
    layer = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='interleave',
        gate_bias_init=0.0
    )
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    layer.eval()
    with torch.no_grad():
        output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\nè¾“å…¥åºåˆ—é•¿åº¦: {seq_len}")
    print(f"è¾“å‡ºåºåˆ—é•¿åº¦: {output.shape[1]}")
    print(f"é¢„æœŸåºåˆ—é•¿åº¦: {2 * seq_len}")
    
    if output.shape[1] != 2 * seq_len:
        print(f"âŒ åºåˆ—é•¿åº¦æœªç¿»å€: {output.shape[1]} != {2 * seq_len}")
        return False
    print("âœ… åºåˆ—é•¿åº¦æ­£ç¡®ç¿»å€")
    
    # éªŒè¯äº¤é”™é¡ºåºï¼šæ£€æŸ¥å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†çš„æ¨¡å¼
    first_half = output[:, :seq_len, :]  # åº”è¯¥æ˜¯ [T1, D1, T2, D2, ...] çš„å‰åŠéƒ¨åˆ†
    second_half = output[:, seq_len:, :]  # åº”è¯¥æ˜¯ [T1, D1, T2, D2, ...] çš„ååŠéƒ¨åˆ†
    
    print(f"\nå‰åŠéƒ¨åˆ†ç»Ÿè®¡: å‡å€¼={first_half.mean().item():.6f}, æ ‡å‡†å·®={first_half.std().item():.6f}")
    print(f"ååŠéƒ¨åˆ†ç»Ÿè®¡: å‡å€¼={second_half.mean().item():.6f}, æ ‡å‡†å·®={second_half.std().item():.6f}")
    
    # éªŒè¯äº¤é”™ï¼šå¶æ•°ä½ç½®åº”è¯¥æ˜¯è¶‹åŠ¿ï¼Œå¥‡æ•°ä½ç½®åº”è¯¥æ˜¯ç»†èŠ‚
    even_pos = output[:, 0::2, :]  # æ‰€æœ‰å¶æ•°ä½ç½®
    odd_pos = output[:, 1::2, :]   # æ‰€æœ‰å¥‡æ•°ä½ç½®
    
    print(f"\nå¶æ•°ä½ç½®ï¼ˆè¶‹åŠ¿ï¼‰ç»Ÿè®¡: å‡å€¼={even_pos.mean().item():.6f}, æ ‡å‡†å·®={even_pos.std().item():.6f}")
    print(f"å¥‡æ•°ä½ç½®ï¼ˆç»†èŠ‚ï¼‰ç»Ÿè®¡: å‡å€¼={odd_pos.mean().item():.6f}, æ ‡å‡†å·®={odd_pos.std().item():.6f}")
    
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 70)
    print("äº¤é”™æ‹¼æ¥èåˆå®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    
    tests = [
        ("äº¤é”™æ‹¼æ¥åŸºæœ¬åŠŸèƒ½", test_interleave_fusion_basic),
        ("éªŒè¯äº¤é”™é¡ºåº", test_interleave_ordering),
        ("ä¸åŒèåˆæ–¹æ³•å¯¹æ¯”", test_fusion_methods_comparison),
        ("è¾“å‡ºå¤´é€‚é…éªŒè¯", test_output_head_adaptation),
        ("ç«¯åˆ°ç«¯æµ‹è¯•", test_end_to_end),
        ("åºåˆ—é•¿åº¦ç¿»å€éªŒè¯", test_sequence_length_doubling),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\nâŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status}: {test_name}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼äº¤é”™æ‹¼æ¥èåˆå®ç°æ­£ç¡®ï¼")
        return True
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)

