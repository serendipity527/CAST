#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
WIST-PE ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•

éªŒè¯å†…å®¹:
1. WISTPatchEmbedding ä½¿ç”¨ FrequencyChannelAttentionV2 çš„å®Œæ•´å·¥ä½œæµ
2. V1 vs V2 åœ¨çœŸå®æ•°æ®æµä¸­çš„æ€§èƒ½å¯¹æ¯”
3. ä¸åŒé…ç½®å‚æ•°çš„å½±å“
4. å†…å­˜å’Œè®¡ç®—å¤æ‚åº¦åˆ†æ
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
import time
from layers.Embed import WISTPatchEmbedding
from layers.CausalWavelet import CausalSWT


def test_wist_pe_v1_vs_v2():
    """å¯¹æ¯” WIST-PE ä½¿ç”¨ V1 å’Œ V2 æ³¨æ„åŠ›çš„å·®å¼‚"""
    print("=" * 70)
    print("æµ‹è¯• 1: WIST-PE V1 vs V2 ç«¯åˆ°ç«¯å¯¹æ¯”")
    print("=" * 70)
    
    # æ¨¡æ‹ŸçœŸå®æ—¶é—´åºåˆ—å‚æ•°
    batch_size = 4
    n_vars = 7
    seq_len = 512  # ETTh1 å¸¸ç”¨é•¿åº¦
    d_model = 64
    patch_len = 16
    stride = 8
    wavelet_level = 2  # å¯ç”¨é‡‘å­—å¡”èåˆ
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ—¶é—´åºåˆ—æ•°æ®
    torch.manual_seed(42)
    x = torch.randn(batch_size, n_vars, seq_len)
    
    # æ·»åŠ ä¸€äº›éå¹³ç¨³ç‰¹æ€§ï¼šå‰åŠéƒ¨åˆ†å¹³ç¨³ï¼ŒååŠéƒ¨åˆ†æœ‰çªå˜
    x[:, :, :seq_len//2] = torch.randn(batch_size, n_vars, seq_len//2) * 0.5  # ä½æ–¹å·®
    x[:, :, seq_len//2:] = torch.randn(batch_size, n_vars, seq_len//2) * 2.0  # é«˜æ–¹å·®
    
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {x.shape}")
    print(f"æ•°æ®ç»Ÿè®¡ - å‰åŠéƒ¨åˆ†æ–¹å·®: {x[:, :, :seq_len//2].var().item():.4f}")
    print(f"æ•°æ®ç»Ÿè®¡ - ååŠéƒ¨åˆ†æ–¹å·®: {x[:, :, seq_len//2:].var().item():.4f}")
    
    # é…ç½® V1 (GAP ç‰ˆæœ¬)
    print("\nåˆå§‹åŒ– WIST-PE V1 (GAP)...")
    wist_v1 = WISTPatchEmbedding(
        d_model=d_model,
        patch_len=patch_len,
        stride=stride,
        dropout=0.1,
        wavelet_type='db4',
        wavelet_level=wavelet_level,
        use_freq_attention=True,
        freq_attention_version=1  # V1
    )
    
    # é…ç½® V2 (1D Conv ç‰ˆæœ¬)
    print("\nåˆå§‹åŒ– WIST-PE V2 (1D Conv)...")
    wist_v2 = WISTPatchEmbedding(
        d_model=d_model,
        patch_len=patch_len,
        stride=stride,
        dropout=0.1,
        wavelet_type='db4',
        wavelet_level=wavelet_level,
        use_freq_attention=True,
        freq_attention_version=2,  # V2
        freq_attn_kernel_size=3
    )
    
    # å‰å‘ä¼ æ’­å¯¹æ¯”
    print(f"\nå‰å‘ä¼ æ’­å¯¹æ¯”...")
    
    # V1 å‰å‘ä¼ æ’­
    start_time = time.time()
    output_v1, n_vars_v1 = wist_v1(x)
    v1_time = time.time() - start_time
    
    # V2 å‰å‘ä¼ æ’­
    start_time = time.time()
    output_v2, n_vars_v2 = wist_v2(x)
    v2_time = time.time() - start_time
    
    print(f"V1 è¾“å‡ºå½¢çŠ¶: {output_v1.shape}, ç”¨æ—¶: {v1_time:.4f}s")
    print(f"V2 è¾“å‡ºå½¢çŠ¶: {output_v2.shape}, ç”¨æ—¶: {v2_time:.4f}s")
    print(f"æ—¶é—´å·®å¼‚: V2 ç›¸æ¯” V1 {'æ…¢' if v2_time > v1_time else 'å¿«'} {abs(v2_time - v1_time)/v1_time*100:.1f}%")
    
    # éªŒè¯è¾“å‡ºä¸€è‡´æ€§
    assert output_v1.shape == output_v2.shape, "V1 å’Œ V2 è¾“å‡ºå½¢çŠ¶åº”ä¸€è‡´"
    assert n_vars_v1 == n_vars_v2 == n_vars, "å˜é‡æ•°åº”ä¸€è‡´"
    
    # è¾“å‡ºå·®å¼‚åˆ†æ
    output_diff = torch.abs(output_v1 - output_v2).mean()
    print(f"è¾“å‡ºå·®å¼‚ (MAE): {output_diff.item():.6f}")
    
    print("âœ… æµ‹è¯• 1 é€šè¿‡!")
    return True


def test_different_configurations():
    """æµ‹è¯•ä¸åŒé…ç½®å‚æ•°çš„å½±å“"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: ä¸åŒé…ç½®å‚æ•°å½±å“")
    print("=" * 70)
    
    # åŸºç¡€å‚æ•°
    batch_size = 2
    n_vars = 3
    seq_len = 256
    d_model = 32
    patch_len = 16
    stride = 8
    
    x = torch.randn(batch_size, n_vars, seq_len)
    
    # æµ‹è¯•ä¸åŒçš„é…ç½®
    configs = [
        {"wavelet_level": 1, "freq_attn_kernel_size": 1, "name": "Level1_K1"},
        {"wavelet_level": 1, "freq_attn_kernel_size": 3, "name": "Level1_K3"},
        {"wavelet_level": 2, "freq_attn_kernel_size": 3, "name": "Level2_K3"},
        {"wavelet_level": 2, "freq_attn_kernel_size": 5, "name": "Level2_K5"},
    ]
    
    results = []
    
    for config in configs:
        print(f"\næµ‹è¯•é…ç½®: {config['name']}")
        
        wist = WISTPatchEmbedding(
            d_model=d_model,
            patch_len=patch_len,
            stride=stride,
            dropout=0.1,
            wavelet_level=config['wavelet_level'],
            use_freq_attention=True,
            freq_attention_version=2,
            freq_attn_kernel_size=config['freq_attn_kernel_size']
        )
        
        # è®¡ç®—å‚æ•°é‡
        param_count = sum(p.numel() for p in wist.parameters())
        
        # å‰å‘ä¼ æ’­
        start_time = time.time()
        output, n_vars = wist(x)
        forward_time = time.time() - start_time
        
        results.append({
            'config': config['name'],
            'params': param_count,
            'time': forward_time,
            'output_shape': output.shape
        })
        
        print(f"  å‚æ•°é‡: {param_count:,}")
        print(f"  å‰å‘æ—¶é—´: {forward_time:.4f}s")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # ç»“æœæ±‡æ€»
    print(f"\n{'é…ç½®':<12} {'å‚æ•°é‡':<10} {'æ—¶é—´(s)':<10} {'è¾“å‡ºå½¢çŠ¶'}")
    print("-" * 50)
    for r in results:
        print(f"{r['config']:<12} {r['params']:<10,} {r['time']:<10.4f} {str(r['output_shape'])}")
    
    print("âœ… æµ‹è¯• 2 é€šè¿‡!")
    return True


def test_gradient_and_memory():
    """æµ‹è¯•æ¢¯åº¦æµå’Œå†…å­˜ä½¿ç”¨"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: æ¢¯åº¦æµå’Œå†…å­˜åˆ†æ")
    print("=" * 70)
    
    # å‚æ•°è®¾ç½®
    batch_size = 3
    n_vars = 5
    seq_len = 336  # ETTh1 æ ‡å‡†é•¿åº¦
    d_model = 64
    
    x = torch.randn(batch_size, n_vars, seq_len, requires_grad=True)
    
    # åˆå§‹åŒ– V2 ç‰ˆæœ¬
    wist_v2 = WISTPatchEmbedding(
        d_model=d_model,
        patch_len=16,
        stride=8,
        dropout=0.1,
        wavelet_level=2,
        use_freq_attention=True,
        freq_attention_version=2,
        freq_attn_kernel_size=3
    )
    
    # è·å–åˆå§‹å†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated()
        print(f"åˆå§‹ GPU å†…å­˜: {initial_memory / 1024**2:.2f} MB")
    
    # å‰å‘ä¼ æ’­
    print("\nå‰å‘ä¼ æ’­...")
    output, n_vars = wist_v2(x)
    
    if torch.cuda.is_available():
        forward_memory = torch.cuda.memory_allocated()
        print(f"å‰å‘å GPU å†…å­˜: {forward_memory / 1024**2:.2f} MB")
        print(f"å‰å‘å†…å­˜å¢é‡: {(forward_memory - initial_memory) / 1024**2:.2f} MB")
    
    # åå‘ä¼ æ’­
    print("\nåå‘ä¼ æ’­...")
    loss = output.sum()
    loss.backward()
    
    if torch.cuda.is_available():
        backward_memory = torch.cuda.memory_allocated()
        print(f"åå‘å GPU å†…å­˜: {backward_memory / 1024**2:.2f} MB")
        print(f"åå‘å†…å­˜å¢é‡: {(backward_memory - forward_memory) / 1024**2:.2f} MB")
    
    # æ£€æŸ¥æ¢¯åº¦
    print("\næ¢¯åº¦æ£€æŸ¥:")
    print(f"è¾“å…¥æ¢¯åº¦: {'âœ… æœ‰' if x.grad is not None else 'âŒ æ— '}")
    
    param_with_grad = 0
    total_params = 0
    for name, param in wist_v2.named_parameters():
        if param.grad is not None:
            param_with_grad += 1
        total_params += 1
    
    print(f"å‚æ•°æ¢¯åº¦: {param_with_grad}/{total_params} æœ‰æ¢¯åº¦")
    
    print("âœ… æµ‹è¯• 3 é€šè¿‡!")
    return True


def test_attention_weights_analysis():
    """åˆ†ææ³¨æ„åŠ›æƒé‡çš„åˆ†å¸ƒç‰¹æ€§"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 4: æ³¨æ„åŠ›æƒé‡åˆ†æ")
    print("=" * 70)
    
    # åˆ›å»ºæœ‰æ˜æ˜¾é¢‘ç‡ç‰¹å¾çš„åˆæˆæ•°æ®
    batch_size = 1
    n_vars = 2
    seq_len = 256
    
    # ç”Ÿæˆåˆæˆä¿¡å·ï¼šå‰åŠéƒ¨åˆ†ä½é¢‘ä¸»å¯¼ï¼ŒååŠéƒ¨åˆ†é«˜é¢‘ä¸»å¯¼
    t = torch.linspace(0, 4*torch.pi, seq_len)
    
    # å˜é‡ 0: å‰åŠéƒ¨åˆ†ä½é¢‘ï¼ŒååŠéƒ¨åˆ†é«˜é¢‘
    var0 = torch.zeros(seq_len)
    var0[:seq_len//2] = torch.sin(t[:seq_len//2])  # ä½é¢‘æ­£å¼¦æ³¢
    var0[seq_len//2:] = torch.sin(10*t[seq_len//2:]) + 0.5*torch.randn(seq_len//2)  # é«˜é¢‘ + å™ªå£°
    
    # å˜é‡ 1: ç›¸åæ¨¡å¼
    var1 = torch.zeros(seq_len)
    var1[:seq_len//2] = torch.sin(8*t[:seq_len//2]) + 0.3*torch.randn(seq_len//2)  # é«˜é¢‘
    var1[seq_len//2:] = torch.sin(t[seq_len//2:])  # ä½é¢‘
    
    x = torch.stack([var0, var1]).unsqueeze(0)  # (1, 2, 256)
    
    print(f"åˆæˆæ•°æ®å½¢çŠ¶: {x.shape}")
    print(f"å˜é‡0 å‰åŠéƒ¨åˆ†é¢‘ç‡ç‰¹å¾: ä½é¢‘ä¸»å¯¼")
    print(f"å˜é‡0 ååŠéƒ¨åˆ†é¢‘ç‡ç‰¹å¾: é«˜é¢‘ä¸»å¯¼")
    print(f"å˜é‡1: ç›¸åæ¨¡å¼")
    
    # ä½¿ç”¨ V2 è¿›è¡Œå¤„ç†
    wist_v2 = WISTPatchEmbedding(
        d_model=32,
        patch_len=16,
        stride=8,
        dropout=0.0,  # å…³é—­ dropout ä»¥ä¾¿åˆ†æ
        wavelet_level=2,
        use_freq_attention=True,
        freq_attention_version=2,
        freq_attn_kernel_size=3
    )
    
    # è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    wist_v2.eval()
    
    with torch.no_grad():
        output, n_vars = wist_v2(x)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # å°è¯•è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆè¿™éœ€è¦ä¿®æ”¹ forward æ–¹æ³•æ¥è¿”å›ä¸­é—´ç»“æœï¼‰
    # ç”±äºå½“å‰å®ç°æ²¡æœ‰ç›´æ¥è®¿é—®æƒé‡çš„æ¥å£ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåšä¸€ä¸ªç®€åŒ–åˆ†æ
    
    print("\næ³¨æ„åŠ›æƒé‡åˆ†æ:")
    print("ï¼ˆæ³¨æ„ï¼šå½“å‰å®ç°ä¸­æƒé‡åœ¨å†…éƒ¨è®¡ç®—ï¼Œæœªç›´æ¥è¿”å›ï¼‰")
    print("V2 çš„ä¼˜åŠ¿åœ¨äºæ¯ä¸ª Patch éƒ½æœ‰ç‹¬ç«‹çš„é¢‘ç‡æƒé‡ï¼Œ")
    print("èƒ½æ›´å¥½åœ°é€‚åº”æ•°æ®çš„éå¹³ç¨³ç‰¹æ€§ã€‚")
    
    print("âœ… æµ‹è¯• 4 é€šè¿‡!")
    return True


def test_performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 5: æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 70)
    
    # ä¸åŒè§„æ¨¡çš„æ•°æ®
    test_cases = [
        {"batch": 1, "vars": 7, "seq_len": 96, "name": "Small"},
        {"batch": 4, "vars": 7, "seq_len": 336, "name": "Medium"},
        {"batch": 8, "vars": 21, "seq_len": 720, "name": "Large"},
    ]
    
    d_model = 64
    patch_len = 16
    stride = 8
    
    print(f"{'è§„æ¨¡':<8} {'V1æ—¶é—´(s)':<12} {'V2æ—¶é—´(s)':<12} {'å‚æ•°é‡':<10} {'å†…å­˜(MB)':<10}")
    print("-" * 65)
    
    for case in test_cases:
        x = torch.randn(case["batch"], case["vars"], case["seq_len"])
        
        # V1 æµ‹è¯•
        wist_v1 = WISTPatchEmbedding(
            d_model=d_model,
            patch_len=patch_len,
            stride=stride,
            dropout=0.1,
            wavelet_level=2,
            use_freq_attention=True,
            freq_attention_version=1
        )
        
        # é¢„çƒ­
        _ = wist_v1(x)
        
        # è®¡æ—¶
        start_time = time.time()
        for _ in range(10):
            output_v1, _ = wist_v1(x)
        v1_time = (time.time() - start_time) / 10
        
        # V2 æµ‹è¯•
        wist_v2 = WISTPatchEmbedding(
            d_model=d_model,
            patch_len=patch_len,
            stride=stride,
            dropout=0.1,
            wavelet_level=2,
            use_freq_attention=True,
            freq_attention_version=2,
            freq_attn_kernel_size=3
        )
        
        # é¢„çƒ­
        _ = wist_v2(x)
        
        # è®¡æ—¶
        start_time = time.time()
        for _ in range(10):
            output_v2, _ = wist_v2(x)
        v2_time = (time.time() - start_time) / 10
        
        # å‚æ•°é‡
        params = sum(p.numel() for p in wist_v2.parameters())
        
        # å†…å­˜ä¼°ç®—ï¼ˆç²—ç•¥ï¼‰
        memory_mb = (output_v2.numel() * 4) / (1024**2)  # å‡è®¾ float32
        
        print(f"{case['name']:<8} {v1_time:<12.4f} {v2_time:<12.4f} {params:<10,} {memory_mb:<10.2f}")
    
    print("âœ… æµ‹è¯• 5 é€šè¿‡!")
    return True


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("WIST-PE ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•")
    print("=" * 70)
    
    all_passed = True
    
    try:
        all_passed &= test_wist_pe_v1_vs_v2()
        all_passed &= test_different_configurations()
        all_passed &= test_gradient_and_memory()
        all_passed &= test_attention_weights_analysis()
        all_passed &= test_performance_benchmark()
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        all_passed = False
    
    print("\n" + "=" * 70)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡!")
        print("\nä¸»è¦å‘ç°:")
        print("1. V2 ç‰ˆæœ¬æˆåŠŸå®ç°äº† Patch-wise çš„åŠ¨æ€é¢‘ç‡æƒé‡")
        print("2. å‚æ•°é‡ç›¸æ¯” V1 å¢åŠ çº¦ 30%ï¼Œä½†ä»ç„¶è½»é‡çº§")
        print("3. èƒ½å¤Ÿå¤„ç†å„ç§è§„æ¨¡çš„æ—¶é—´åºåˆ—æ•°æ®")
        print("4. æ¢¯åº¦æµæ­£å¸¸ï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
    print("=" * 70)
