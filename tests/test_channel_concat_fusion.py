#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é€šé“æ‹¼æ¥èåˆæµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. DualReprogrammingLayer çš„ channel_concat èåˆæ–¹æ³•
2. åºåˆ—é•¿åº¦ä¿æŒä¸å˜éªŒè¯
3. ç‰¹å¾ç»´åº¦æ‹¼æ¥å’ŒæŠ•å½±éªŒè¯
4. è¾“å‡ºå¤´å‚æ•°é‡éªŒè¯ï¼ˆä¸åº”ç¿»å€ï¼‰
5. ç«¯åˆ°ç«¯æµ‹è¯•
6. ä¸å…¶ä»–èåˆæ–¹æ³•å¯¹æ¯”
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
from transformers import GPT2Tokenizer, GPT2Model

from models.TimeLLM import Model, DualReprogrammingLayer


class TestConfig:
    """æµ‹è¯•é…ç½®ç±»"""
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.task_name = 'long_term_forecast'
        self.llm_model = 'GPT2'
        self.llm_dim = 768
        self.llm_layers = 2
        self.d_model = 16
        self.n_heads = 4
        self.d_ff = 32
        self.dropout = 0.1
        self.patch_len = 16
        self.stride = 8
        self.seq_len = 96
        self.pred_len = 96
        self.enc_in = 7
        self.dec_in = 7
        self.c_out = 7
        
        # å°æ³¢é…ç½®
        self.wavelet_mode = 'wist'
        self.wavelet_type = 'haar'
        self.wavelet_level = 2
        
        # åˆ†ç¦»åŸå‹é…ç½®
        self.use_dual_prototypes = 1
        self.dual_proto_num_tokens = 1000
        self.dual_proto_fusion_method = 'channel_concat'
        self.use_full_vocab_split = 1
        
        # Prompté…ç½®
        self.prompt_domain = 0
        self.content = 'Test dataset description'
        
        # è¾“å‡ºå¤´é…ç½®
        self.use_dual_scale_head = 0
        self.use_freq_decoupled_head = 0
        
        # å…¶ä»–é…ç½®
        self.use_cwpr = 0


def test_channel_concat_fusion_basic():
    """æµ‹è¯•1: é€šé“æ‹¼æ¥åŸºæœ¬åŠŸèƒ½"""
    print("=" * 70)
    print("æµ‹è¯•1: é€šé“æ‹¼æ¥åŸºæœ¬åŠŸèƒ½")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»º DualReprogrammingLayer
    layer = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='channel_concat',
        gate_bias_init=0.0
    )
    
    print(f"\nèåˆæ–¹æ³•: {layer.fusion_method}")
    
    if layer.fusion_method != 'channel_concat':
        print(f"âŒ èåˆæ–¹æ³•ä¸æ­£ç¡®: {layer.fusion_method} != channel_concat")
        return False
    print("âœ… èåˆæ–¹æ³•æ­£ç¡®: channel_concat")
    
    # æ£€æŸ¥æŠ•å½±å±‚æ˜¯å¦å­˜åœ¨
    if layer.fusion_projection is None:
        print("âŒ æŠ•å½±å±‚æœªåˆ›å»º")
        return False
    print("âœ… æŠ•å½±å±‚å·²åˆ›å»º")
    
    # æ£€æŸ¥æŠ•å½±å±‚ç»´åº¦
    expected_in_features = 2 * d_llm
    expected_out_features = d_llm
    if layer.fusion_projection.in_features != expected_in_features:
        print(f"âŒ æŠ•å½±å±‚è¾“å…¥ç»´åº¦ä¸æ­£ç¡®: {layer.fusion_projection.in_features} != {expected_in_features}")
        return False
    if layer.fusion_projection.out_features != expected_out_features:
        print(f"âŒ æŠ•å½±å±‚è¾“å‡ºç»´åº¦ä¸æ­£ç¡®: {layer.fusion_projection.out_features} != {expected_out_features}")
        return False
    print(f"âœ… æŠ•å½±å±‚ç»´åº¦æ­£ç¡®: Linear({expected_in_features}, {expected_out_features})")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  - trend_embedding: {trend_embedding.shape}")
    print(f"  - detail_embedding: {detail_embedding.shape}")
    print(f"  - trend_prototypes: {trend_prototypes.shape}")
    print(f"  - detail_prototypes: {detail_prototypes.shape}")
    
    # å‰å‘ä¼ æ’­
    layer.eval()
    with torch.no_grad():
        output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"é¢„æœŸå½¢çŠ¶: ({batch_size}, {seq_len}, {d_llm})")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶ï¼šåºåˆ—é•¿åº¦åº”è¯¥ä¿æŒä¸å˜
    if output.shape != (batch_size, seq_len, d_llm):
        print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®: {output.shape} != ({batch_size}, {seq_len}, {d_llm})")
        return False
    print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®ï¼ˆåºåˆ—é•¿åº¦ä¿æŒä¸å˜ï¼‰")
    
    # éªŒè¯è¾“å‡ºå€¼
    if torch.isnan(output).any():
        print("âŒ è¾“å‡ºåŒ…å« NaN")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— NaNï¼‰")
    
    if torch.isinf(output).any():
        print("âŒ è¾“å‡ºåŒ…å« Inf")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— Infï¼‰")
    
    return True


def test_channel_concat_vs_interleave():
    """æµ‹è¯•2: å¯¹æ¯” channel_concat å’Œ interleave çš„åºåˆ—é•¿åº¦"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•2: å¯¹æ¯” channel_concat å’Œ interleave çš„åºåˆ—é•¿åº¦")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 1
    seq_len = 5
    num_prototypes = 100
    
    # åˆ›å»ºä¸¤ä¸ªå±‚ï¼šchannel_concat å’Œ interleave
    layer_concat = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='channel_concat',
    )
    
    layer_interleave = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='interleave',
    )
    
    # åˆ›å»ºç›¸åŒçš„æµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    layer_concat.eval()
    layer_interleave.eval()
    with torch.no_grad():
        output_concat = layer_concat(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
        output_interleave = layer_interleave(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\nchannel_concat è¾“å‡ºå½¢çŠ¶: {output_concat.shape}")
    print(f"interleave è¾“å‡ºå½¢çŠ¶: {output_interleave.shape}")
    
    # éªŒè¯ channel_concat ä¿æŒåºåˆ—é•¿åº¦
    if output_concat.shape[1] != seq_len:
        print(f"âŒ channel_concat åºåˆ—é•¿åº¦ä¸æ­£ç¡®: {output_concat.shape[1]} != {seq_len}")
        return False
    print(f"âœ… channel_concat åºåˆ—é•¿åº¦æ­£ç¡®: {output_concat.shape[1]} == {seq_len}")
    
    # éªŒè¯ interleave åºåˆ—é•¿åº¦ç¿»å€
    if output_interleave.shape[1] != 2 * seq_len:
        print(f"âŒ interleave åºåˆ—é•¿åº¦ä¸æ­£ç¡®: {output_interleave.shape[1]} != {2 * seq_len}")
        return False
    print(f"âœ… interleave åºåˆ—é•¿åº¦æ­£ç¡®: {output_interleave.shape[1]} == {2 * seq_len}")
    
    # éªŒè¯ç‰¹å¾ç»´åº¦
    if output_concat.shape[2] != d_llm:
        print(f"âŒ channel_concat ç‰¹å¾ç»´åº¦ä¸æ­£ç¡®: {output_concat.shape[2]} != {d_llm}")
        return False
    print(f"âœ… channel_concat ç‰¹å¾ç»´åº¦æ­£ç¡®: {output_concat.shape[2]} == {d_llm}")
    
    if output_interleave.shape[2] != d_llm:
        print(f"âŒ interleave ç‰¹å¾ç»´åº¦ä¸æ­£ç¡®: {output_interleave.shape[2]} != {d_llm}")
        return False
    print(f"âœ… interleave ç‰¹å¾ç»´åº¦æ­£ç¡®: {output_interleave.shape[2]} == {d_llm}")
    
    return True


def test_channel_concat_projection():
    """æµ‹è¯•3: éªŒè¯æŠ•å½±å±‚çš„åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•3: éªŒè¯æŠ•å½±å±‚çš„åŠŸèƒ½")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    layer = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='channel_concat',
    )
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    layer.eval()
    with torch.no_grad():
        # æ‰‹åŠ¨æ‰§è¡Œå‰ä¸¤æ­¥ï¼ŒéªŒè¯æ‹¼æ¥
        sem_trend = layer.trend_reprogramming(trend_embedding, trend_prototypes, trend_prototypes)
        sem_detail = layer.detail_reprogramming(detail_embedding, detail_prototypes, detail_prototypes)
        
        # æ‰‹åŠ¨æ‹¼æ¥
        concat_output = torch.cat([sem_trend, sem_detail], dim=-1)
        
        # é€šè¿‡æŠ•å½±å±‚
        projected_output = layer.fusion_projection(concat_output)
        
        # å®Œæ•´å‰å‘ä¼ æ’­
        full_output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\næ‹¼æ¥åå½¢çŠ¶: {concat_output.shape}")
    print(f"æŠ•å½±åå½¢çŠ¶: {projected_output.shape}")
    print(f"å®Œæ•´è¾“å‡ºå½¢çŠ¶: {full_output.shape}")
    
    # éªŒè¯æ‹¼æ¥ç»´åº¦
    if concat_output.shape != (batch_size, seq_len, 2 * d_llm):
        print(f"âŒ æ‹¼æ¥åå½¢çŠ¶ä¸æ­£ç¡®: {concat_output.shape} != ({batch_size}, {seq_len}, {2 * d_llm})")
        return False
    print("âœ… æ‹¼æ¥åå½¢çŠ¶æ­£ç¡®")
    
    # éªŒè¯æŠ•å½±åç»´åº¦
    if projected_output.shape != (batch_size, seq_len, d_llm):
        print(f"âŒ æŠ•å½±åå½¢çŠ¶ä¸æ­£ç¡®: {projected_output.shape} != ({batch_size}, {seq_len}, {d_llm})")
        return False
    print("âœ… æŠ•å½±åå½¢çŠ¶æ­£ç¡®")
    
    # éªŒè¯å®Œæ•´è¾“å‡ºä¸æ‰‹åŠ¨è®¡ç®—ä¸€è‡´
    if not torch.allclose(projected_output, full_output, atol=1e-5):
        print("âŒ å®Œæ•´è¾“å‡ºä¸æ‰‹åŠ¨è®¡ç®—ä¸ä¸€è‡´")
        return False
    print("âœ… å®Œæ•´è¾“å‡ºä¸æ‰‹åŠ¨è®¡ç®—ä¸€è‡´")
    
    return True


def test_channel_concat_head_params():
    """æµ‹è¯•4: éªŒè¯è¾“å‡ºå¤´å‚æ•°é‡ï¼ˆä¸åº”ç¿»å€ï¼‰"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•4: éªŒè¯è¾“å‡ºå¤´å‚æ•°é‡ï¼ˆä¸åº”ç¿»å€ï¼‰")
    print("=" * 70)
    
    config = TestConfig()
    config.dual_proto_fusion_method = 'channel_concat'
    
    model = Model(config)
    
    # è®¡ç®— head_nf
    patch_nums = int((config.seq_len - config.patch_len) / config.stride + 2)
    expected_head_nf = config.d_ff * patch_nums  # ä¸åº”ç¿»å€
    
    print(f"\né…ç½®:")
    print(f"  - seq_len: {config.seq_len}")
    print(f"  - patch_len: {config.patch_len}")
    print(f"  - stride: {config.stride}")
    print(f"  - patch_nums: {patch_nums}")
    print(f"  - d_ff: {config.d_ff}")
    print(f"  - fusion_method: {config.dual_proto_fusion_method}")
    
    print(f"\næ¨¡å‹ head_nf: {model.head_nf}")
    print(f"é¢„æœŸ head_nf: {expected_head_nf}")
    
    if model.head_nf != expected_head_nf:
        print(f"âŒ head_nf ä¸æ­£ç¡®: {model.head_nf} != {expected_head_nf}")
        return False
    print("âœ… head_nf æ­£ç¡®ï¼ˆæœªç¿»å€ï¼‰")
    
    # è®¡ç®—è¾“å‡ºå¤´å‚æ•°é‡
    # FlattenHead çš„å‚æ•°é‡è®¡ç®—ï¼š
    # - è¾“å…¥å½¢çŠ¶: (B, n_vars, d_ff, patch_nums)
    # - flatten å: (B, n_vars, d_ff * patch_nums) = (B, n_vars, head_nf)
    # - Linear(head_nf, pred_len): weight = head_nf * pred_len, bias = pred_len
    # - æ€»å‚æ•°é‡: head_nf * pred_len + pred_len (å…±äº«çš„ Linear å±‚ï¼Œæ‰€æœ‰å˜é‡å…±ç”¨)
    if hasattr(model.output_projection, 'linear'):
        head_params = sum(p.numel() for p in model.output_projection.linear.parameters())
        print(f"\nè¾“å‡ºå¤´å‚æ•°é‡: {head_params:,}")
        
        # è®¡ç®—é¢„æœŸå‚æ•°é‡ï¼šFlattenHead ä½¿ç”¨å…±äº«çš„ Linear å±‚
        # Linear(head_nf, pred_len) çš„å‚æ•°é‡ = head_nf * pred_len + pred_len (bias)
        expected_params = expected_head_nf * config.pred_len + config.pred_len
        print(f"é¢„æœŸå‚æ•°é‡: {expected_params:,}")
        print(f"  - weight: {expected_head_nf * config.pred_len:,}")
        print(f"  - bias: {config.pred_len:,}")
        
        if head_params != expected_params:
            print(f"âŒ è¾“å‡ºå¤´å‚æ•°é‡ä¸æ­£ç¡®: {head_params} != {expected_params}")
            print(f"   å·®å¼‚: {abs(head_params - expected_params):,}")
            return False
        print("âœ… è¾“å‡ºå¤´å‚æ•°é‡æ­£ç¡®ï¼ˆæœªç¿»å€ï¼‰")
    
    return True


def test_channel_concat_end_to_end():
    """æµ‹è¯•5: ç«¯åˆ°ç«¯æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•5: ç«¯åˆ°ç«¯æµ‹è¯•")
    print("=" * 70)
    
    config = TestConfig()
    config.dual_proto_fusion_method = 'channel_concat'
    
    model = Model(config)
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    x_enc = torch.randn(batch_size, config.seq_len, config.enc_in)
    x_mark_enc = torch.randn(batch_size, config.seq_len, 4)  # å‡è®¾4ä¸ªæ—¶é—´ç‰¹å¾
    x_dec = torch.randn(batch_size, config.pred_len, config.dec_in)
    x_mark_dec = torch.randn(batch_size, config.pred_len, 4)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  - x_enc: {x_enc.shape}")
    print(f"  - x_mark_enc: {x_mark_enc.shape}")
    print(f"  - x_dec: {x_dec.shape}")
    print(f"  - x_mark_dec: {x_mark_dec.shape}")
    
    with torch.no_grad():
        output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"é¢„æœŸå½¢çŠ¶: ({batch_size}, {config.pred_len}, {config.c_out})")
    
    if output.shape != (batch_size, config.pred_len, config.c_out):
        print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®: {output.shape} != ({batch_size}, {config.pred_len}, {config.c_out})")
        return False
    print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
    
    # éªŒè¯è¾“å‡ºå€¼
    if torch.isnan(output).any():
        print("âŒ è¾“å‡ºåŒ…å« NaN")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— NaNï¼‰")
    
    if torch.isinf(output).any():
        print("âŒ è¾“å‡ºåŒ…å« Inf")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— Infï¼‰")
    
    return True


def test_channel_concat_vs_mean():
    """æµ‹è¯•6: å¯¹æ¯” channel_concat å’Œ mean èåˆ"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•6: å¯¹æ¯” channel_concat å’Œ mean èåˆ")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»ºä¸¤ä¸ªå±‚
    layer_concat = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='channel_concat',
    )
    
    layer_mean = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='mean',
    )
    
    # åˆ›å»ºç›¸åŒçš„æµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    layer_concat.eval()
    layer_mean.eval()
    with torch.no_grad():
        output_concat = layer_concat(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
        output_mean = layer_mean(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\nchannel_concat è¾“å‡ºå½¢çŠ¶: {output_concat.shape}")
    print(f"mean è¾“å‡ºå½¢çŠ¶: {output_mean.shape}")
    
    # éªŒè¯å½¢çŠ¶ç›¸åŒ
    if output_concat.shape != output_mean.shape:
        print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ: {output_concat.shape} != {output_mean.shape}")
        return False
    print("âœ… è¾“å‡ºå½¢çŠ¶ç›¸åŒ")
    
    # éªŒè¯è¾“å‡ºä¸åŒï¼ˆå› ä¸ºèåˆæ–¹å¼ä¸åŒï¼‰
    if torch.allclose(output_concat, output_mean, atol=1e-3):
        print("âš ï¸  è­¦å‘Š: ä¸¤ç§èåˆæ–¹æ³•è¾“å‡ºè¿‡äºæ¥è¿‘ï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰")
    else:
        print("âœ… ä¸¤ç§èåˆæ–¹æ³•è¾“å‡ºä¸åŒï¼ˆç¬¦åˆé¢„æœŸï¼‰")
    
    return True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("Channel Concatenation èåˆæ–¹æ³•æµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    
    tests = [
        ("åŸºæœ¬åŠŸèƒ½æµ‹è¯•", test_channel_concat_fusion_basic),
        ("åºåˆ—é•¿åº¦å¯¹æ¯”æµ‹è¯•", test_channel_concat_vs_interleave),
        ("æŠ•å½±å±‚åŠŸèƒ½æµ‹è¯•", test_channel_concat_projection),
        ("è¾“å‡ºå¤´å‚æ•°é‡æµ‹è¯•", test_channel_concat_head_params),
        ("ç«¯åˆ°ç«¯æµ‹è¯•", test_channel_concat_end_to_end),
        ("èåˆæ–¹æ³•å¯¹æ¯”æµ‹è¯•", test_channel_concat_vs_mean),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{test_name}' æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    passed = 0
    failed = 0
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status}: {test_name}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print(f"\næ€»è®¡: {len(results)} ä¸ªæµ‹è¯•")
    print(f"é€šè¿‡: {passed} ä¸ª")
    print(f"å¤±è´¥: {failed} ä¸ª")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print(f"\nâš ï¸  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == '__main__':
    exit(main())

