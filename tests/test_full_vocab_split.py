#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å…¨è¯è¡¨åˆ‡åˆ†åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•å…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. vocab_splitter å‡½æ•°æµ‹è¯•
2. TimeLLM æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•
3. å‰å‘ä¼ æ’­æµ‹è¯•
4. å‚æ•°éªŒè¯
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
from transformers import GPT2Tokenizer, GPT2Model
from utils.vocab_splitter import split_full_vocab_by_semantics, print_vocab_split_samples
from models.TimeLLM import Model


class TestConfig:
    """æµ‹è¯•é…ç½®ç±»"""
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.task_name = 'long_term_forecast'
        self.enc_in = 7  # ETTh1
        self.dec_in = 7
        self.c_out = 7
        self.seq_len = 96
        self.pred_len = 96
        self.d_model = 16
        self.n_heads = 4
        self.e_layers = 2
        self.d_layers = 1
        self.d_ff = 32
        self.dropout = 0.1
        self.activation = 'gelu'
        self.output_attention = False
        self.llm_model = 'GPT2'
        self.llm_dim = 768  # GPT2 çš„åµŒå…¥ç»´åº¦
        self.llm_layers = 2
        self.patch_len = 16
        self.stride = 8
        
        # å°æ³¢é…ç½®
        self.wavelet_mode = 'none'
        self.use_haar_wavelet = 0
        
        # Prompté…ç½®
        self.prompt_domain = 0
        self.content = 'Test dataset description'
        
        # åˆ†ç¦»åŸå‹é…ç½®
        self.use_dual_prototypes = 1
        self.dual_proto_trend_tokens = 1000
        self.dual_proto_detail_tokens = 1000
        self.dual_proto_fusion_method = 'mean'
        
        # å…¨è¯è¡¨åˆ‡åˆ†é…ç½®
        self.use_full_vocab_split = 1
        self.use_semantic_filtered_mapping = 0  # å¿…é¡»ä¸º0ï¼Œå› ä¸ºäº’æ–¥
        
        # å…¶ä»–é…ç½®
        self.use_cwpr = 0
        self.use_dual_scale_head = 0
        self.use_freq_decoupled_head = 0


def test_vocab_splitter():
    """æµ‹è¯• vocab_splitter å‡½æ•°"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 1: vocab_splitter å‡½æ•°")
    print("=" * 70)
    
    try:
        # åŠ è½½æ¨¡å‹
        print("\n[æ­¥éª¤1] åŠ è½½ GPT2 æ¨¡å‹...")
        tokenizer = GPT2Tokenizer.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=False
        )
        model = GPT2Model.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=False
        )
        word_embeddings = model.get_input_embeddings().weight
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¯è¡¨å¤§å°: {len(tokenizer):,}, åµŒå…¥ç»´åº¦: {word_embeddings.shape[1]}")
        
        # æ‰§è¡Œåˆ‡åˆ†
        print("\n[æ­¥éª¤2] æ‰§è¡Œå…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†...")
        trend_indices, detail_indices = split_full_vocab_by_semantics(
            tokenizer=tokenizer,
            word_embeddings=word_embeddings,
            trend_anchors=None,
            detail_anchors=None,
            verbose=True
        )
        
        # éªŒè¯ç»“æœ
        print("\n[æ­¥éª¤3] éªŒè¯åˆ‡åˆ†ç»“æœ...")
        vocab_size = len(tokenizer)
        
        # æ£€æŸ¥ä¸ç›¸äº¤
        trend_set = set(trend_indices.cpu().tolist())
        detail_set = set(detail_indices.cpu().tolist())
        overlap = trend_set & detail_set
        
        assert len(overlap) == 0, f"âŒ å‘ç° {len(overlap)} ä¸ªé‡å è¯ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰"
        print("âœ… ä¸¤ä¸ªè¯é›†å®Œå…¨ä¸ç›¸äº¤")
        
        # æ£€æŸ¥è¦†ç›–
        total = len(trend_set) + len(detail_set)
        assert total == vocab_size, f"âŒ è¯è¡¨è¦†ç›–ä¸å®Œæ•´: {total} != {vocab_size}"
        print(f"âœ… è¯è¡¨å®Œå…¨è¦†ç›–: {total} = {vocab_size}")
        
        # æ£€æŸ¥æ¯”ä¾‹
        trend_ratio = len(trend_indices) / vocab_size
        detail_ratio = len(detail_indices) / vocab_size
        print(f"âœ… è¶‹åŠ¿æ¡¶å æ¯”: {trend_ratio*100:.1f}%")
        print(f"âœ… ç»†èŠ‚æ¡¶å æ¯”: {detail_ratio*100:.1f}%")
        
        # æ‰“å°æ ·æœ¬
        print("\n[æ­¥éª¤4] æ‰“å°åˆ‡åˆ†ç»“æœæ ·æœ¬...")
        print_vocab_split_samples(tokenizer, trend_indices, detail_indices, max_print=20)
        
        print("\nâœ… vocab_splitter å‡½æ•°æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ vocab_splitter å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: TimeLLM æ¨¡å‹åˆå§‹åŒ–ï¼ˆå…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼‰")
    print("=" * 70)
    
    try:
        configs = TestConfig()
        
        print("\n[æ­¥éª¤1] åˆå§‹åŒ–æ¨¡å‹...")
        model = Model(configs)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # éªŒè¯å…¨è¯è¡¨åˆ‡åˆ†ç›¸å…³å±æ€§
        print("\n[æ­¥éª¤2] éªŒè¯å…¨è¯è¡¨åˆ‡åˆ†ç›¸å…³å±æ€§...")
        
        assert hasattr(model, 'use_full_vocab_split'), "âŒ ç¼ºå°‘ use_full_vocab_split å±æ€§"
        assert model.use_full_vocab_split == True, f"âŒ use_full_vocab_split åº”ä¸º Trueï¼Œå®é™…ä¸º {model.use_full_vocab_split}"
        print("âœ… use_full_vocab_split å±æ€§æ­£ç¡®")
        
        assert hasattr(model, 'trend_vocab_embeddings'), "âŒ ç¼ºå°‘ trend_vocab_embeddings Buffer"
        assert hasattr(model, 'detail_vocab_embeddings'), "âŒ ç¼ºå°‘ detail_vocab_embeddings Buffer"
        print("âœ… åˆ‡åˆ†åçš„ embeddings Buffer å·²æ³¨å†Œ")
        
        assert hasattr(model, 'trend_mapping'), "âŒ ç¼ºå°‘ trend_mapping å±‚"
        assert hasattr(model, 'detail_mapping'), "âŒ ç¼ºå°‘ detail_mapping å±‚"
        print("âœ… æ˜ å°„å±‚å·²åˆ›å»º")
        
        # éªŒè¯æ˜ å°„å±‚ç±»å‹ï¼ˆåº”è¯¥æ˜¯ Linearï¼Œä¸æ˜¯ MLPï¼‰
        assert isinstance(model.trend_mapping, nn.Linear), f"âŒ trend_mapping åº”ä¸º Linearï¼Œå®é™…ä¸º {type(model.trend_mapping)}"
        assert isinstance(model.detail_mapping, nn.Linear), f"âŒ detail_mapping åº”ä¸º Linearï¼Œå®é™…ä¸º {type(model.detail_mapping)}"
        print("âœ… æ˜ å°„å±‚ç±»å‹æ­£ç¡®ï¼ˆLinearï¼Œå’ŒåŸç‰ˆTimeLLMä¸€æ ·ï¼‰")
        
        # éªŒè¯æ˜ å°„å±‚ç»´åº¦
        trend_vocab_size = model.trend_vocab_embeddings.shape[0]
        detail_vocab_size = model.detail_vocab_embeddings.shape[0]
        
        assert model.trend_mapping.in_features == trend_vocab_size, \
            f"âŒ trend_mapping è¾“å…¥ç»´åº¦ä¸åŒ¹é…: {model.trend_mapping.in_features} != {trend_vocab_size}"
        assert model.trend_mapping.out_features == configs.dual_proto_trend_tokens, \
            f"âŒ trend_mapping è¾“å‡ºç»´åº¦ä¸åŒ¹é…: {model.trend_mapping.out_features} != {configs.dual_proto_trend_tokens}"
        
        assert model.detail_mapping.in_features == detail_vocab_size, \
            f"âŒ detail_mapping è¾“å…¥ç»´åº¦ä¸åŒ¹é…: {model.detail_mapping.in_features} != {detail_vocab_size}"
        assert model.detail_mapping.out_features == configs.dual_proto_detail_tokens, \
            f"âŒ detail_mapping è¾“å‡ºç»´åº¦ä¸åŒ¹é…: {model.detail_mapping.out_features} != {configs.dual_proto_detail_tokens}"
        
        print(f"âœ… æ˜ å°„å±‚ç»´åº¦æ­£ç¡®:")
        print(f"   - è¶‹åŠ¿æ˜ å°„: Linear({trend_vocab_size:,} â†’ {configs.dual_proto_trend_tokens})")
        print(f"   - ç»†èŠ‚æ˜ å°„: Linear({detail_vocab_size:,} â†’ {configs.dual_proto_detail_tokens})")
        
        # è®¡ç®—å‚æ•°é‡
        trend_params = trend_vocab_size * configs.dual_proto_trend_tokens
        detail_params = detail_vocab_size * configs.dual_proto_detail_tokens
        total_params = trend_params + detail_params
        print(f"âœ… å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
        
        print("\nâœ… æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: å‰å‘ä¼ æ’­ï¼ˆå…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼‰")
    print("=" * 70)
    
    try:
        configs = TestConfig()
        
        print("\n[æ­¥éª¤1] åˆå§‹åŒ–æ¨¡å‹...")
        model = Model(configs)
        model.eval()
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        print("\n[æ­¥éª¤2] åˆ›å»ºæµ‹è¯•è¾“å…¥...")
        batch_size = 2
        seq_len = configs.seq_len
        n_vars = configs.enc_in
        
        x_enc = torch.randn(batch_size, seq_len, n_vars)
        x_mark_enc = torch.zeros(batch_size, seq_len, 4)  # æ—¶é—´æˆ³ç‰¹å¾
        x_dec = torch.randn(batch_size, configs.pred_len, n_vars)
        x_mark_dec = torch.zeros(batch_size, configs.pred_len, 4)
        
        print(f"âœ… è¾“å…¥å½¢çŠ¶: x_enc {x_enc.shape}, x_mark_enc {x_mark_enc.shape}")
        
        # å‰å‘ä¼ æ’­
        print("\n[æ­¥éª¤3] æ‰§è¡Œå‰å‘ä¼ æ’­...")
        with torch.no_grad():
            output = model.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        print("\n[æ­¥éª¤4] éªŒè¯è¾“å‡ºå½¢çŠ¶...")
        expected_shape = (batch_size, configs.pred_len, n_vars)
        assert output.shape == expected_shape, \
            f"âŒ è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape} != {expected_shape}"
        print(f"âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {output.shape}")
        
        # éªŒè¯è¾“å‡ºä¸æ˜¯ NaN æˆ– Inf
        assert not torch.isnan(output).any(), "âŒ è¾“å‡ºåŒ…å« NaN"
        assert not torch.isinf(output).any(), "âŒ è¾“å‡ºåŒ…å« Inf"
        print("âœ… è¾“å‡ºå€¼æœ‰æ•ˆï¼ˆæ—  NaN/Infï¼‰")
        
        print("\nâœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_prototype_generation():
    """æµ‹è¯•åŸå‹ç”Ÿæˆ"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 4: åŸå‹ç”Ÿæˆï¼ˆå…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼‰")
    print("=" * 70)
    
    try:
        configs = TestConfig()
        
        print("\n[æ­¥éª¤1] åˆå§‹åŒ–æ¨¡å‹...")
        model = Model(configs)
        model.eval()
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æ‰‹åŠ¨ç”ŸæˆåŸå‹ï¼ˆæ¨¡æ‹Ÿ forward ä¸­çš„é€»è¾‘ï¼‰
        print("\n[æ­¥éª¤2] ç”Ÿæˆè¶‹åŠ¿å’Œç»†èŠ‚åŸå‹...")
        with torch.no_grad():
            trend_prototypes = model.trend_mapping(
                model.trend_vocab_embeddings.permute(1, 0)
            ).permute(1, 0)
            
            detail_prototypes = model.detail_mapping(
                model.detail_vocab_embeddings.permute(1, 0)
            ).permute(1, 0)
        
        # éªŒè¯åŸå‹å½¢çŠ¶
        print("\n[æ­¥éª¤3] éªŒè¯åŸå‹å½¢çŠ¶...")
        d_llm = model.d_llm
        
        expected_trend_shape = (configs.dual_proto_trend_tokens, d_llm)
        expected_detail_shape = (configs.dual_proto_detail_tokens, d_llm)
        
        assert trend_prototypes.shape == expected_trend_shape, \
            f"âŒ è¶‹åŠ¿åŸå‹å½¢çŠ¶é”™è¯¯: {trend_prototypes.shape} != {expected_trend_shape}"
        assert detail_prototypes.shape == expected_detail_shape, \
            f"âŒ ç»†èŠ‚åŸå‹å½¢çŠ¶é”™è¯¯: {detail_prototypes.shape} != {expected_detail_shape}"
        
        print(f"âœ… è¶‹åŠ¿åŸå‹å½¢çŠ¶: {trend_prototypes.shape}")
        print(f"âœ… ç»†èŠ‚åŸå‹å½¢çŠ¶: {detail_prototypes.shape}")
        
        # éªŒè¯åŸå‹å€¼
        assert not torch.isnan(trend_prototypes).any(), "âŒ è¶‹åŠ¿åŸå‹åŒ…å« NaN"
        assert not torch.isnan(detail_prototypes).any(), "âŒ ç»†èŠ‚åŸå‹åŒ…å« NaN"
        assert not torch.isinf(trend_prototypes).any(), "âŒ è¶‹åŠ¿åŸå‹åŒ…å« Inf"
        assert not torch.isinf(detail_prototypes).any(), "âŒ ç»†èŠ‚åŸå‹åŒ…å« Inf"
        print("âœ… åŸå‹å€¼æœ‰æ•ˆï¼ˆæ—  NaN/Infï¼‰")
        
        print("\nâœ… åŸå‹ç”Ÿæˆæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ åŸå‹ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_mutual_exclusivity():
    """æµ‹è¯•äº’æ–¥æ€§ï¼šuse_full_vocab_split å’Œ use_semantic_filtered_mapping ä¸èƒ½åŒæ—¶å¯ç”¨"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 5: äº’æ–¥æ€§éªŒè¯")
    print("=" * 70)
    
    try:
        configs = TestConfig()
        configs.use_full_vocab_split = 1
        configs.use_semantic_filtered_mapping = 1  # åŒæ—¶å¯ç”¨ï¼Œåº”è¯¥æŠ¥é”™
        
        print("\n[æ­¥éª¤1] å°è¯•åŒæ—¶å¯ç”¨ use_full_vocab_split å’Œ use_semantic_filtered_mapping...")
        try:
            model = Model(configs)
            print("âŒ åº”è¯¥æŠ›å‡º ValueErrorï¼Œä½†æ²¡æœ‰æŠ›å‡º")
            return False
        except ValueError as e:
            if "ä¸èƒ½åŒæ—¶å¯ç”¨" in str(e):
                print(f"âœ… æ­£ç¡®æŠ›å‡º ValueError: {e}")
                return True
            else:
                print(f"âŒ æŠ›å‡º ValueError ä½†æ¶ˆæ¯ä¸æ­£ç¡®: {e}")
                return False
        except Exception as e:
            print(f"âŒ æŠ›å‡ºæ„å¤–çš„å¼‚å¸¸: {e}")
            return False
        
    except Exception as e:
        print(f"\nâŒ äº’æ–¥æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 70)
    print("å…¨è¯è¡¨åˆ‡åˆ†åŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    
    results = []
    
    # æµ‹è¯• 1: vocab_splitter å‡½æ•°
    results.append(("vocab_splitter å‡½æ•°", test_vocab_splitter()))
    
    # æµ‹è¯• 2: æ¨¡å‹åˆå§‹åŒ–
    results.append(("æ¨¡å‹åˆå§‹åŒ–", test_model_initialization()))
    
    # æµ‹è¯• 3: å‰å‘ä¼ æ’­
    results.append(("å‰å‘ä¼ æ’­", test_forward_pass()))
    
    # æµ‹è¯• 4: åŸå‹ç”Ÿæˆ
    results.append(("åŸå‹ç”Ÿæˆ", test_prototype_generation()))
    
    # æµ‹è¯• 5: äº’æ–¥æ€§
    results.append(("äº’æ–¥æ€§éªŒè¯", test_mutual_exclusivity()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    passed = 0
    failed = 0
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print("=" * 70)
    print(f"æ€»è®¡: {passed} ä¸ªé€šè¿‡, {failed} ä¸ªå¤±è´¥")
    print("=" * 70)
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print(f"\nâš ï¸  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)

