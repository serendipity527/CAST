#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¯­ä¹‰ç­›é€‰æ˜ å°„åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. è¯­ä¹‰ç­›é€‰æ˜ å°„åŠŸèƒ½æ˜¯å¦æ­£ç¡®å¯ç”¨
2. Buffer æ˜¯å¦æ­£ç¡®æ³¨å†Œ
3. æ˜ å°„å±‚ç»´åº¦æ˜¯å¦æ­£ç¡®
4. å‰å‘ä¼ æ’­æ˜¯å¦æ­£å¸¸å·¥ä½œ
5. ç§å­è¯æ˜¯å¦ä¸ç›¸äº¤
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
from transformers import GPT2Config, GPT2Model, GPT2Tokenizer

from models.TimeLLM import Model
from utils.seed_word_selector import select_seed_words, print_seed_words


class TestConfig:
    """æµ‹è¯•é…ç½®ç±»"""
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.task_name = 'long_term_forecast'
        self.llm_model = 'GPT2'
        self.llm_dim = 768
        self.llm_layers = 2
        self.d_model = 16
        self.n_heads = 4
        self.d_ff = 32
        self.dropout = 0.1
        self.patch_len = 16
        self.stride = 8
        self.seq_len = 96
        self.pred_len = 96
        self.enc_in = 7
        self.dec_in = 7
        self.c_out = 7
        
        # å°æ³¢é…ç½®
        self.wavelet_mode = 'wist'
        self.wavelet_type = 'haar'
        self.wavelet_level = 2
        
        # åˆ†ç¦»åŸå‹é…ç½®
        self.use_dual_prototypes = 1
        self.dual_proto_trend_tokens = 500
        self.dual_proto_detail_tokens = 500
        self.dual_proto_fusion_method = 'weighted'
        
        # è¯­ä¹‰ç­›é€‰æ˜ å°„é…ç½®
        self.use_semantic_filtered_mapping = 1
        self.dual_proto_trend_seed_words = 300
        self.dual_proto_detail_seed_words = 700
        self.dual_proto_seed_semantic_filter = 1
        
        # Prompté…ç½®
        self.prompt_domain = 0
        self.content = 'Test dataset description'
        
        # å…¶ä»–é…ç½®
        self.use_cwpr = 0
        self.use_dual_scale_head = 0
        self.use_freq_decoupled_head = 0


def test_semantic_filtered_mapping():
    """æµ‹è¯•è¯­ä¹‰ç­›é€‰æ˜ å°„åŠŸèƒ½"""
    print("=" * 70)
    print("æµ‹è¯•è¯­ä¹‰ç­›é€‰æ˜ å°„åŠŸèƒ½")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    configs = TestConfig()
    
    # æµ‹è¯•1: æ¨¡å‹åˆå§‹åŒ–
    print("\n[æµ‹è¯•1] æ¨¡å‹åˆå§‹åŒ–...")
    try:
        model = Model(configs)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•2: æ£€æŸ¥è¯­ä¹‰ç­›é€‰æ˜ å°„æ˜¯å¦å¯ç”¨
    print("\n[æµ‹è¯•2] æ£€æŸ¥è¯­ä¹‰ç­›é€‰æ˜ å°„é…ç½®...")
    if not hasattr(model, 'use_semantic_filtered_mapping'):
        print("âŒ æ¨¡å‹ç¼ºå°‘ use_semantic_filtered_mapping å±æ€§")
        return False
    
    if not model.use_semantic_filtered_mapping:
        print("âŒ è¯­ä¹‰ç­›é€‰æ˜ å°„æœªå¯ç”¨")
        return False
    
    print(f"âœ… è¯­ä¹‰ç­›é€‰æ˜ å°„å·²å¯ç”¨: {model.use_semantic_filtered_mapping}")
    
    # æµ‹è¯•3: æ£€æŸ¥ Buffer æ˜¯å¦æ­£ç¡®æ³¨å†Œ
    print("\n[æµ‹è¯•3] æ£€æŸ¥ Buffer æ³¨å†Œ...")
    if not hasattr(model, 'trend_seed_embeddings'):
        print("âŒ ç¼ºå°‘ trend_seed_embeddings Buffer")
        return False
    
    if not hasattr(model, 'detail_seed_embeddings'):
        print("âŒ ç¼ºå°‘ detail_seed_embeddings Buffer")
        return False
    
    trend_seed_emb = model.trend_seed_embeddings
    detail_seed_emb = model.detail_seed_embeddings
    
    print(f"âœ… trend_seed_embeddings shape: {trend_seed_emb.shape}")
    print(f"âœ… detail_seed_embeddings shape: {detail_seed_emb.shape}")
    
    # æ£€æŸ¥ Buffer æ˜¯å¦ä¸å‚ä¸æ¢¯åº¦æ›´æ–°
    if trend_seed_emb.requires_grad or detail_seed_emb.requires_grad:
        print("âš ï¸  è­¦å‘Š: Buffer çš„ requires_grad ä¸º Trueï¼Œåº”è¯¥ä¸º False")
    else:
        print("âœ… Buffer ä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼ˆrequires_grad=Falseï¼‰")
    
    # æµ‹è¯•4: æ£€æŸ¥æ˜ å°„å±‚ç»´åº¦
    print("\n[æµ‹è¯•4] æ£€æŸ¥æ˜ å°„å±‚ç»´åº¦...")
    if model.trend_mapping is None or model.detail_mapping is None:
        print("âŒ æ˜ å°„å±‚æœªåˆå§‹åŒ–")
        return False
    
    trend_input_size = model.trend_mapping.in_features
    trend_output_size = model.trend_mapping.out_features
    detail_input_size = model.detail_mapping.in_features
    detail_output_size = model.detail_mapping.out_features
    
    print(f"âœ… è¶‹åŠ¿æ˜ å°„å±‚: {trend_input_size} â†’ {trend_output_size}")
    print(f"âœ… ç»†èŠ‚æ˜ å°„å±‚: {detail_input_size} â†’ {detail_output_size}")
    
    # éªŒè¯ç»´åº¦åŒ¹é…
    if trend_input_size != trend_seed_emb.shape[0]:
        print(f"âŒ è¶‹åŠ¿æ˜ å°„å±‚è¾“å…¥ç»´åº¦ä¸åŒ¹é…: {trend_input_size} != {trend_seed_emb.shape[0]}")
        return False
    
    if detail_input_size != detail_seed_emb.shape[0]:
        print(f"âŒ ç»†èŠ‚æ˜ å°„å±‚è¾“å…¥ç»´åº¦ä¸åŒ¹é…: {detail_input_size} != {detail_seed_emb.shape[0]}")
        return False
    
    if trend_output_size != configs.dual_proto_trend_tokens:
        print(f"âŒ è¶‹åŠ¿æ˜ å°„å±‚è¾“å‡ºç»´åº¦ä¸åŒ¹é…: {trend_output_size} != {configs.dual_proto_trend_tokens}")
        return False
    
    if detail_output_size != configs.dual_proto_detail_tokens:
        print(f"âŒ ç»†èŠ‚æ˜ å°„å±‚è¾“å‡ºç»´åº¦ä¸åŒ¹é…: {detail_output_size} != {configs.dual_proto_detail_tokens}")
        return False
    
    print("âœ… æ˜ å°„å±‚ç»´åº¦æ­£ç¡®")
    
    # æµ‹è¯•5: æ£€æŸ¥ç§å­è¯æ˜¯å¦ä¸ç›¸äº¤
    print("\n[æµ‹è¯•5] æ£€æŸ¥ç§å­è¯ä¸ç›¸äº¤æ€§...")
    # ä» Buffer ä¸­æ¢å¤åŸå§‹ç´¢å¼•ï¼ˆéœ€è¦é‡æ–°ç­›é€‰æ¥éªŒè¯ï¼‰
    # è¿™é‡Œæˆ‘ä»¬ç›´æ¥æµ‹è¯•å‰å‘ä¼ æ’­ï¼Œçœ‹æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
    
    # æµ‹è¯•6: å‰å‘ä¼ æ’­æµ‹è¯•
    print("\n[æµ‹è¯•6] å‰å‘ä¼ æ’­æµ‹è¯•...")
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        seq_len = configs.seq_len
        n_vars = configs.enc_in
        
        x_enc = torch.randn(batch_size, seq_len, n_vars)
        x_mark_enc = torch.zeros(batch_size, seq_len, 4)  # æ—¶é—´ç‰¹å¾
        x_dec = torch.randn(batch_size, configs.pred_len, n_vars)
        x_mark_dec = torch.zeros(batch_size, configs.pred_len, 4)
        
        model.eval()
        with torch.no_grad():
            # æµ‹è¯•åŸå‹ç”Ÿæˆï¼ˆä½¿ç”¨ä¸ forward ä¸­ç›¸åŒçš„é€»è¾‘ï¼‰
            if model.use_semantic_filtered_mapping:
                # è¯­ä¹‰ç­›é€‰æ˜ å°„æ¨¡å¼ï¼šéœ€è¦è½¬ç½®
                # trend_seed_embeddings: (num_trend_seed_words, d_llm) -> è½¬ç½® -> (d_llm, num_trend_seed_words)
                # Linear(num_trend_seed_words, num_trend_tokens) -> (d_llm, num_trend_tokens) -> è½¬ç½®å› (num_trend_tokens, d_llm)
                trend_prototypes = model.trend_mapping(model.trend_seed_embeddings.permute(1, 0)).permute(1, 0)
                detail_prototypes = model.detail_mapping(model.detail_seed_embeddings.permute(1, 0)).permute(1, 0)
            else:
                trend_prototypes = model.trend_mapping(model.word_embeddings.permute(1, 0)).permute(1, 0)
                detail_prototypes = model.detail_mapping(model.word_embeddings.permute(1, 0)).permute(1, 0)
            
            print(f"âœ… è¶‹åŠ¿åŸå‹ shape: {trend_prototypes.shape}")
            print(f"âœ… ç»†èŠ‚åŸå‹ shape: {detail_prototypes.shape}")
            
            # å®Œæ•´å‰å‘ä¼ æ’­
            output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡º shape: {output.shape}")
            
            # éªŒè¯è¾“å‡ºç»´åº¦
            expected_shape = (batch_size, configs.pred_len, n_vars)
            if output.shape != expected_shape:
                print(f"âŒ è¾“å‡ºç»´åº¦ä¸åŒ¹é…: {output.shape} != {expected_shape}")
                return False
            
            print("âœ… è¾“å‡ºç»´åº¦æ­£ç¡®")
            
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æµ‹è¯•7: å¯¹æ¯”åŸç‰ˆæ˜ å°„å’Œè¯­ä¹‰ç­›é€‰æ˜ å°„
    print("\n[æµ‹è¯•7] å¯¹æ¯”åŸç‰ˆæ˜ å°„å’Œè¯­ä¹‰ç­›é€‰æ˜ å°„...")
    try:
        # åˆ›å»ºåŸç‰ˆé…ç½®ï¼ˆä¸ä½¿ç”¨è¯­ä¹‰ç­›é€‰ï¼‰
        configs_original = TestConfig()
        configs_original.use_semantic_filtered_mapping = 0
        
        model_original = Model(configs_original)
        model_original.eval()
        
        # æ¯”è¾ƒæ˜ å°„å±‚è¾“å…¥ç»´åº¦
        original_trend_input = model_original.trend_mapping.in_features
        original_detail_input = model_original.detail_mapping.in_features
        
        print(f"åŸç‰ˆæ˜ å°„ - è¶‹åŠ¿è¾“å…¥ç»´åº¦: {original_trend_input} (æ•´ä¸ªè¯è¡¨)")
        print(f"åŸç‰ˆæ˜ å°„ - ç»†èŠ‚è¾“å…¥ç»´åº¦: {original_detail_input} (æ•´ä¸ªè¯è¡¨)")
        print(f"è¯­ä¹‰ç­›é€‰æ˜ å°„ - è¶‹åŠ¿è¾“å…¥ç»´åº¦: {trend_input_size} (ç§å­è¯)")
        print(f"è¯­ä¹‰ç­›é€‰æ˜ å°„ - ç»†èŠ‚è¾“å…¥ç»´åº¦: {detail_input_size} (ç§å­è¯)")
        
        if trend_input_size < original_trend_input and detail_input_size < original_detail_input:
            print("âœ… è¯­ä¹‰ç­›é€‰æ˜ å°„æˆåŠŸå‡å°‘äº†æ˜ å°„å±‚è¾“å…¥ç»´åº¦")
        else:
            print("âš ï¸  è­¦å‘Š: è¯­ä¹‰ç­›é€‰æ˜ å°„æœªå‡å°‘è¾“å…¥ç»´åº¦")
        
    except Exception as e:
        print(f"âš ï¸  å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 70)
    return True


def test_seed_word_selector():
    """æµ‹è¯•ç§å­è¯ç­›é€‰å·¥å…·"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç§å­è¯ç­›é€‰å·¥å…·")
    print("=" * 70)
    
    try:
        # åŠ è½½ tokenizer å’Œ model
        print("\n[æ­¥éª¤1] åŠ è½½æ¨¡å‹...")
        tokenizer = GPT2Tokenizer.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=False
        )
        
        model = GPT2Model.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=False
        )
        
        word_embeddings = model.get_input_embeddings().weight
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¯è¡¨å¤§å°: {len(tokenizer)}, åµŒå…¥ç»´åº¦: {word_embeddings.shape[1]}")
        
        # æµ‹è¯•ç­›é€‰
        print("\n[æ­¥éª¤2] ç­›é€‰ç§å­è¯...")
        trend_indices, detail_indices = select_seed_words(
            tokenizer=tokenizer,
            word_embeddings=word_embeddings,
            num_trend_words=300,
            num_detail_words=700,
            use_semantic_filter=True,
            ensure_disjoint=True
        )
        
        print(f"âœ… è¶‹åŠ¿ç§å­è¯æ•°é‡: {len(trend_indices)}")
        print(f"âœ… ç»†èŠ‚ç§å­è¯æ•°é‡: {len(detail_indices)}")
        
        # æ£€æŸ¥ä¸ç›¸äº¤
        trend_set = set(trend_indices.cpu().tolist())
        detail_set = set(detail_indices.cpu().tolist())
        overlap = trend_set & detail_set
        
        if overlap:
            print(f"âŒ å‘ç° {len(overlap)} ä¸ªé‡å è¯")
            return False
        else:
            print("âœ… ä¸¤ä¸ªè¯é›†å®Œå…¨ä¸ç›¸äº¤")
        
        # æ‰“å°éƒ¨åˆ†ç§å­è¯
        print("\n[æ­¥éª¤3] æ‰“å°éƒ¨åˆ†ç§å­è¯...")
        print_seed_words(tokenizer, trend_indices, detail_indices, max_print=20)
        
        print("\nâœ… ç§å­è¯ç­›é€‰å·¥å…·æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ç§å­è¯ç­›é€‰å·¥å…·æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    print("=" * 70)
    print("è¯­ä¹‰ç­›é€‰æ˜ å°„åŠŸèƒ½å®Œæ•´æµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•1: ç§å­è¯ç­›é€‰å·¥å…·
    test1_passed = test_seed_word_selector()
    
    # æµ‹è¯•2: è¯­ä¹‰ç­›é€‰æ˜ å°„åŠŸèƒ½
    test2_passed = test_semantic_filtered_mapping()
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    print(f"ç§å­è¯ç­›é€‰å·¥å…·æµ‹è¯•: {'âœ… é€šè¿‡' if test1_passed else 'âŒ å¤±è´¥'}")
    print(f"è¯­ä¹‰ç­›é€‰æ˜ å°„åŠŸèƒ½æµ‹è¯•: {'âœ… é€šè¿‡' if test2_passed else 'âŒ å¤±è´¥'}")
    
    if test1_passed and test2_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)

