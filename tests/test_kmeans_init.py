"""
æµ‹è¯•K-Meansèšç±»åˆå§‹åŒ–åŠŸèƒ½

éªŒè¯PrototypeBankçš„K-Meansåˆå§‹åŒ–å®ç°æ˜¯å¦æ­£ç¡®ã€‚
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from layers.CWPR import PrototypeBank


def test_random_init():
    """æµ‹è¯•1: éšæœºåˆå§‹åŒ–ï¼ˆåŸºçº¿æµ‹è¯•ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•1: éšæœºåˆå§‹åŒ–")
    print("="*70)
    
    num_prototypes = 10
    d_llm = 128
    
    bank = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='random'
    )
    
    prototypes = bank()
    
    # éªŒè¯å½¢çŠ¶
    assert prototypes.shape == (num_prototypes, d_llm), \
        f"å½¢çŠ¶é”™è¯¯: æœŸæœ›({num_prototypes}, {d_llm}), å¾—åˆ°{prototypes.shape}"
    
    # éªŒè¯ä¸æ˜¯å…¨é›¶
    assert not torch.allclose(prototypes, torch.zeros_like(prototypes)), \
        "åŸå‹ä¸åº”è¯¥å…¨ä¸ºé›¶"
    
    # éªŒè¯å‡å€¼æ¥è¿‘0ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
    mean_val = prototypes.mean().item()
    assert abs(mean_val) < 0.1, f"å‡å€¼åº”è¯¥æ¥è¿‘0ï¼Œå¾—åˆ°{mean_val}"
    
    print(f"âœ… éšæœºåˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
    print(f"   å½¢çŠ¶: {prototypes.shape}")
    print(f"   å‡å€¼: {prototypes.mean().item():.6f}")
    print(f"   æ ‡å‡†å·®: {prototypes.std().item():.6f}")


def test_word_embed_random_sampling():
    """æµ‹è¯•2: è¯åµŒå…¥éšæœºé‡‡æ ·ï¼ˆåŸæœ‰æ–¹æ³•ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•2: è¯åµŒå…¥éšæœºé‡‡æ ·ï¼ˆuse_kmeans=Falseï¼‰")
    print("="*70)
    
    vocab_size = 1000
    num_prototypes = 50
    d_llm = 128
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¯åµŒå…¥ï¼ˆä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼‰
    word_embeddings = torch.randn(vocab_size, d_llm)
    
    bank = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=False
    )
    
    prototypes = bank()
    
    # éªŒè¯å½¢çŠ¶
    assert prototypes.shape == (num_prototypes, d_llm), \
        f"å½¢çŠ¶é”™è¯¯: æœŸæœ›({num_prototypes}, {d_llm}), å¾—åˆ°{prototypes.shape}"
    
    # éªŒè¯åŸå‹æ¥è‡ªè¯åµŒå…¥ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…ï¼‰
    # ç”±äºæ˜¯éšæœºé‡‡æ ·ï¼Œè‡³å°‘åº”è¯¥æœ‰ä¸€äº›åŸå‹åœ¨è¯åµŒå…¥ä¸­
    matches = 0
    for proto in prototypes:
        for word_emb in word_embeddings:
            if torch.allclose(proto, word_emb, atol=1e-6):
                matches += 1
                break
    
    assert matches == num_prototypes, \
        f"æ‰€æœ‰åŸå‹åº”è¯¥æ¥è‡ªè¯åµŒå…¥ï¼Œä½†åªæ‰¾åˆ°{matches}/{num_prototypes}ä¸ªåŒ¹é…"
    
    print(f"âœ… è¯åµŒå…¥éšæœºé‡‡æ ·æµ‹è¯•é€šè¿‡")
    print(f"   å½¢çŠ¶: {prototypes.shape}")
    print(f"   åŒ¹é…çš„åŸå‹æ•°: {matches}/{num_prototypes}")


def test_kmeans_init_standard():
    """æµ‹è¯•3: K-Meansåˆå§‹åŒ–ï¼ˆæ ‡å‡†æƒ…å†µï¼švocab_size >= num_prototypesï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•3: K-Meansåˆå§‹åŒ–ï¼ˆæ ‡å‡†æƒ…å†µï¼‰")
    print("="*70)
    
    vocab_size = 500
    num_prototypes = 50
    d_llm = 128
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¯åµŒå…¥ï¼ˆä½¿ç”¨å¤šä¸ªèšç±»ä¸­å¿ƒç”Ÿæˆï¼Œæ¨¡æ‹ŸçœŸå®è¯­ä¹‰åˆ†å¸ƒï¼‰
    np.random.seed(42)
    torch.manual_seed(42)
    
    # åˆ›å»º5ä¸ªçœŸå®çš„èšç±»ä¸­å¿ƒ
    true_centers = torch.randn(5, d_llm) * 2.0
    
    # ä»æ¯ä¸ªä¸­å¿ƒç”Ÿæˆä¸€äº›è¯åµŒå…¥ï¼ˆæ·»åŠ å™ªå£°ï¼‰
    word_embeddings_list = []
    for center in true_centers:
        # æ¯ä¸ªä¸­å¿ƒç”Ÿæˆ vocab_size/5 ä¸ªè¯åµŒå…¥
        n_words = vocab_size // 5
        words = center.unsqueeze(0) + torch.randn(n_words, d_llm) * 0.5
        word_embeddings_list.append(words)
    
    word_embeddings = torch.cat(word_embeddings_list, dim=0)
    # ç¡®ä¿æ­£å¥½æ˜¯vocab_size
    if word_embeddings.shape[0] < vocab_size:
        extra = torch.randn(vocab_size - word_embeddings.shape[0], d_llm)
        word_embeddings = torch.cat([word_embeddings, extra], dim=0)
    word_embeddings = word_embeddings[:vocab_size]
    
    # ä½¿ç”¨K-Meansåˆå§‹åŒ–
    bank_kmeans = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=True
    )
    
    # ä½¿ç”¨éšæœºé‡‡æ ·åˆå§‹åŒ–ï¼ˆå¯¹æ¯”ï¼‰
    bank_random = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=False
    )
    
    prototypes_kmeans = bank_kmeans()
    prototypes_random = bank_random()
    
    # éªŒè¯å½¢çŠ¶
    assert prototypes_kmeans.shape == (num_prototypes, d_llm)
    assert prototypes_random.shape == (num_prototypes, d_llm)
    
    # éªŒè¯K-MeansåŸå‹æ›´åˆ†æ•£ï¼ˆè®¡ç®—å¹³å‡è·ç¦»ï¼‰
    def compute_avg_distance(protos):
        """è®¡ç®—åŸå‹ä¹‹é—´çš„å¹³å‡è·ç¦»"""
        distances = []
        for i in range(len(protos)):
            for j in range(i+1, len(protos)):
                dist = torch.norm(protos[i] - protos[j]).item()
                distances.append(dist)
        return np.mean(distances) if distances else 0.0
    
    avg_dist_kmeans = compute_avg_distance(prototypes_kmeans)
    avg_dist_random = compute_avg_distance(prototypes_random)
    
    print(f"âœ… K-Meansåˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
    print(f"   å½¢çŠ¶: {prototypes_kmeans.shape}")
    print(f"   K-Meanså¹³å‡è·ç¦»: {avg_dist_kmeans:.4f}")
    print(f"   éšæœºé‡‡æ ·å¹³å‡è·ç¦»: {avg_dist_random:.4f}")
    print(f"   è·ç¦»æå‡: {(avg_dist_kmeans/avg_dist_random - 1)*100:.2f}%")
    
    # K-Meansåº”è¯¥äº§ç”Ÿæ›´åˆ†æ•£çš„åŸå‹ï¼ˆå¹³å‡è·ç¦»æ›´å¤§ï¼‰
    # ä½†è¿™ä¸æ˜¯ç»å¯¹çš„ï¼Œå› ä¸ºéšæœºé‡‡æ ·ä¹Ÿå¯èƒ½å¾ˆåˆ†æ•£
    # æ‰€ä»¥æˆ‘ä»¬åªéªŒè¯K-Meansèƒ½æ­£å¸¸å·¥ä½œï¼Œä¸å¼ºåˆ¶è¦æ±‚è·ç¦»æ›´å¤§


def test_kmeans_init_edge_case():
    """æµ‹è¯•4: K-Meansåˆå§‹åŒ–ï¼ˆè¾¹ç•Œæƒ…å†µï¼švocab_size < num_prototypesï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•4: K-Meansåˆå§‹åŒ–ï¼ˆè¾¹ç•Œæƒ…å†µï¼švocab_size < num_prototypesï¼‰")
    print("="*70)
    
    vocab_size = 30
    num_prototypes = 50
    d_llm = 128
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¯åµŒå…¥
    torch.manual_seed(42)
    word_embeddings = torch.randn(vocab_size, d_llm)
    
    bank = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=True
    )
    
    prototypes = bank()
    
    # éªŒè¯å½¢çŠ¶
    assert prototypes.shape == (num_prototypes, d_llm), \
        f"å½¢çŠ¶é”™è¯¯: æœŸæœ›({num_prototypes}, {d_llm}), å¾—åˆ°{prototypes.shape}"
    
    # éªŒè¯å‰vocab_sizeä¸ªåŸå‹æ¥è‡ªK-Meansï¼ˆåº”è¯¥æ¥è¿‘è¯åµŒå…¥ï¼‰
    # éªŒè¯å(num_prototypes - vocab_size)ä¸ªåŸå‹æ˜¯éšæœºåˆå§‹åŒ–çš„
    first_part = prototypes[:vocab_size]
    second_part = prototypes[vocab_size:]
    
    # å‰ä¸€éƒ¨åˆ†åº”è¯¥ä¸è¯åµŒå…¥æœ‰æŸç§å…³è”ï¼ˆé€šè¿‡K-Meansï¼‰
    # åä¸€éƒ¨åˆ†åº”è¯¥æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼ˆå‡å€¼æ¥è¿‘0ï¼‰
    second_mean = second_part.mean().item()
    second_std = second_part.std().item()
    
    print(f"âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")
    print(f"   å½¢çŠ¶: {prototypes.shape}")
    print(f"   å‰{vocab_size}ä¸ªåŸå‹ï¼ˆK-Meansï¼‰å‡å€¼: {first_part.mean().item():.6f}")
    print(f"   å{num_prototypes - vocab_size}ä¸ªåŸå‹ï¼ˆéšæœºï¼‰å‡å€¼: {second_mean:.6f}")
    print(f"   å{num_prototypes - vocab_size}ä¸ªåŸå‹ï¼ˆéšæœºï¼‰æ ‡å‡†å·®: {second_std:.6f}")
    
    # éªŒè¯éšæœºéƒ¨åˆ†çš„æ ‡å‡†å·®æ¥è¿‘0.02ï¼ˆéšæœºåˆå§‹åŒ–çš„æ ‡å‡†å·®ï¼‰
    assert abs(second_std - 0.02) < 0.01, \
        f"éšæœºéƒ¨åˆ†æ ‡å‡†å·®åº”è¯¥æ¥è¿‘0.02ï¼Œå¾—åˆ°{second_std}"


def test_kmeans_reproducibility():
    """æµ‹è¯•5: K-Meansåˆå§‹åŒ–çš„å¯å¤ç°æ€§"""
    print("\n" + "="*70)
    print("æµ‹è¯•5: K-Meansåˆå§‹åŒ–çš„å¯å¤ç°æ€§")
    print("="*70)
    
    vocab_size = 200
    num_prototypes = 20
    d_llm = 64
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¯åµŒå…¥
    torch.manual_seed(42)
    word_embeddings = torch.randn(vocab_size, d_llm)
    
    # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–
    bank1 = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=True
    )
    prototypes1 = bank1()
    
    # ç¬¬äºŒæ¬¡åˆå§‹åŒ–ï¼ˆåº”è¯¥å¾—åˆ°ç›¸åŒç»“æœï¼‰
    bank2 = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=True
    )
    prototypes2 = bank2()
    
    # éªŒè¯ç»“æœç›¸åŒï¼ˆå¯å¤ç°æ€§ï¼‰
    assert torch.allclose(prototypes1, prototypes2, atol=1e-5), \
        "K-Meansåˆå§‹åŒ–åº”è¯¥å¯å¤ç°ï¼Œä½†ä¸¤æ¬¡ç»“æœä¸åŒ"
    
    print(f"âœ… å¯å¤ç°æ€§æµ‹è¯•é€šè¿‡")
    print(f"   ä¸¤æ¬¡åˆå§‹åŒ–çš„åŸå‹å®Œå…¨ç›¸åŒ")


def test_kmeans_vs_random_diversity():
    """æµ‹è¯•6: K-Means vs éšæœºé‡‡æ ·çš„å¤šæ ·æ€§å¯¹æ¯”"""
    print("\n" + "="*70)
    print("æµ‹è¯•6: K-Means vs éšæœºé‡‡æ ·çš„å¤šæ ·æ€§å¯¹æ¯”")
    print("="*70)
    
    vocab_size = 1000
    num_prototypes = 100
    d_llm = 256
    
    # åˆ›å»ºæœ‰æ˜æ˜¾èšç±»ç»“æ„çš„è¯åµŒå…¥
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»º10ä¸ªæ˜æ˜¾çš„èšç±»ä¸­å¿ƒ
    n_clusters = 10
    cluster_centers = torch.randn(n_clusters, d_llm) * 3.0
    
    word_embeddings_list = []
    for i, center in enumerate(cluster_centers):
        n_words = vocab_size // n_clusters
        # æ¯ä¸ªèšç±»å†…çš„è¯åµŒå…¥ç´§å¯†å›´ç»•ä¸­å¿ƒ
        words = center.unsqueeze(0) + torch.randn(n_words, d_llm) * 0.3
        word_embeddings_list.append(words)
    
    word_embeddings = torch.cat(word_embeddings_list, dim=0)
    if word_embeddings.shape[0] < vocab_size:
        extra = torch.randn(vocab_size - word_embeddings.shape[0], d_llm)
        word_embeddings = torch.cat([word_embeddings, extra], dim=0)
    word_embeddings = word_embeddings[:vocab_size]
    
    # K-Meansåˆå§‹åŒ–
    bank_kmeans = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=True
    )
    prototypes_kmeans = bank_kmeans()
    
    # éšæœºé‡‡æ ·åˆå§‹åŒ–
    bank_random = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='word_embed',
        word_embeddings=word_embeddings,
        use_kmeans=False
    )
    prototypes_random = bank_random()
    
    # è®¡ç®—åˆ°æœ€è¿‘èšç±»ä¸­å¿ƒçš„è·ç¦»ï¼ˆè¡¡é‡è¦†ç›–æ€§ï¼‰
    def compute_coverage(protos, true_centers):
        """è®¡ç®—åŸå‹å¯¹çœŸå®èšç±»ä¸­å¿ƒçš„è¦†ç›–æ€§"""
        min_distances = []
        for proto in protos:
            distances = [torch.norm(proto - center).item() for center in true_centers]
            min_distances.append(min(distances))
        return np.mean(min_distances)
    
    coverage_kmeans = compute_coverage(prototypes_kmeans, cluster_centers)
    coverage_random = compute_coverage(prototypes_random, cluster_centers)
    
    print(f"âœ… å¤šæ ·æ€§å¯¹æ¯”æµ‹è¯•é€šè¿‡")
    print(f"   K-Meanså¹³å‡åˆ°æœ€è¿‘ä¸­å¿ƒè·ç¦»: {coverage_kmeans:.4f}")
    print(f"   éšæœºé‡‡æ ·å¹³å‡åˆ°æœ€è¿‘ä¸­å¿ƒè·ç¦»: {coverage_random:.4f}")
    print(f"   è¦†ç›–æ€§æå‡: {(coverage_random/coverage_kmeans - 1)*100:.2f}%")
    
    # K-Meansåº”è¯¥æ›´å¥½åœ°è¦†ç›–èšç±»ä¸­å¿ƒï¼ˆè·ç¦»æ›´å°ï¼‰
    # ä½†è¿™ä¸æ˜¯ç»å¯¹çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéªŒè¯åŠŸèƒ½æ­£å¸¸


def test_parameter_validation():
    """æµ‹è¯•7: å‚æ•°éªŒè¯"""
    print("\n" + "="*70)
    print("æµ‹è¯•7: å‚æ•°éªŒè¯")
    print("="*70)
    
    vocab_size = 100
    num_prototypes = 50
    d_llm = 128
    
    word_embeddings = torch.randn(vocab_size, d_llm)
    
    # æµ‹è¯•ï¼šuse_kmeans=True ä½† init_method='random'ï¼ˆåº”è¯¥ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰
    bank = PrototypeBank(
        num_prototypes=num_prototypes,
        d_llm=d_llm,
        init_method='random',
        word_embeddings=None,
        use_kmeans=True  # è¿™ä¸ªå‚æ•°åº”è¯¥è¢«å¿½ç•¥
    )
    prototypes = bank()
    
    # éªŒè¯æ˜¯éšæœºåˆå§‹åŒ–ï¼ˆå‡å€¼æ¥è¿‘0ï¼‰
    mean_val = prototypes.mean().item()
    assert abs(mean_val) < 0.1, "åº”è¯¥æ˜¯éšæœºåˆå§‹åŒ–"
    
    print(f"âœ… å‚æ•°éªŒè¯æµ‹è¯•é€šè¿‡")
    print(f"   init_method='random'æ—¶ï¼Œuse_kmeansè¢«æ­£ç¡®å¿½ç•¥")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*70)
    print("å¼€å§‹æµ‹è¯•K-Meansèšç±»åˆå§‹åŒ–åŠŸèƒ½")
    print("="*70)
    
    tests = [
        test_random_init,
        test_word_embed_random_sampling,
        test_kmeans_init_standard,
        test_kmeans_init_edge_case,
        test_kmeans_reproducibility,
        test_kmeans_vs_random_diversity,
        test_parameter_validation,
    ]
    
    passed = 0
    failed = 0
    
    for test_func in tests:
        try:
            test_func()
            passed += 1
        except Exception as e:
            print(f"\nâŒ {test_func.__name__} å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    print("\n" + "="*70)
    print("æµ‹è¯•æ€»ç»“")
    print("="*70)
    print(f"é€šè¿‡: {passed}/{len(tests)}")
    print(f"å¤±è´¥: {failed}/{len(tests)}")
    print("="*70)
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼K-Meansåˆå§‹åŒ–å®ç°æ­£ç¡®ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
    
    return failed == 0


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)

