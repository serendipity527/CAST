#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é—¨æ§èåˆæµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. AdaptiveFusionGate åŸºæœ¬åŠŸèƒ½æµ‹è¯•
2. é—¨æ§æƒé‡è®¡ç®—æ­£ç¡®æ€§
3. DualReprogrammingLayer ä½¿ç”¨ adaptive_gate èåˆ
4. ä¸åŒèåˆæ–¹æ³•å¯¹æ¯”
5. ç«¯åˆ°ç«¯æµ‹è¯•
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
from transformers import GPT2Tokenizer, GPT2Model

from models.TimeLLM import Model, AdaptiveFusionGate, DualReprogrammingLayer


class TestConfig:
    """æµ‹è¯•é…ç½®ç±»"""
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.task_name = 'long_term_forecast'
        self.llm_model = 'GPT2'
        self.llm_dim = 768
        self.llm_layers = 2
        self.d_model = 16
        self.n_heads = 4
        self.d_ff = 32
        self.dropout = 0.1
        self.patch_len = 16
        self.stride = 8
        self.seq_len = 96
        self.pred_len = 96
        self.enc_in = 7
        self.dec_in = 7
        self.c_out = 7
        
        # å°æ³¢é…ç½®
        self.wavelet_mode = 'wist'
        self.wavelet_type = 'haar'
        self.wavelet_level = 2
        
        # åˆ†ç¦»åŸå‹é…ç½®
        self.use_dual_prototypes = 1
        self.dual_proto_trend_tokens = 500
        self.dual_proto_detail_tokens = 500
        self.dual_proto_fusion_method = 'adaptive_gate'
        self.dual_proto_gate_bias_init = 0.0
        
        # è¯­ä¹‰ç­›é€‰æ˜ å°„é…ç½®
        self.use_semantic_filtered_mapping = 1
        self.dual_proto_trend_seed_words = 300
        self.dual_proto_detail_seed_words = 700
        self.dual_proto_seed_semantic_filter = 1
        
        # MLPæ˜ å°„å±‚é…ç½®
        self.dual_proto_mlp_hidden_dim = 2048  # æµ‹è¯•æ—¶ä½¿ç”¨è¾ƒå°çš„ç»´åº¦
        self.dual_proto_mlp_dropout = 0.1
        
        # Prompté…ç½®
        self.prompt_domain = 0
        self.content = 'Test dataset description'
        
        # å…¶ä»–é…ç½®
        self.use_cwpr = 0
        self.use_dual_scale_head = 0
        self.use_freq_decoupled_head = 0


def test_adaptive_fusion_gate_basic():
    """æµ‹è¯•1: AdaptiveFusionGate åŸºæœ¬åŠŸèƒ½"""
    print("=" * 70)
    print("æµ‹è¯•1: AdaptiveFusionGate åŸºæœ¬åŠŸèƒ½")
    print("=" * 70)
    
    d_model = 16
    batch_size = 2
    seq_len = 10
    
    # åˆ›å»ºé—¨æ§ç½‘ç»œ
    gate = AdaptiveFusionGate(d_model, gate_bias_init=0.0)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  - trend_embedding: {trend_embedding.shape}")
    print(f"  - detail_embedding: {detail_embedding.shape}")
    
    # å‰å‘ä¼ æ’­
    gate_weights = gate(trend_embedding, detail_embedding)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {gate_weights.shape}")
    print(f"é¢„æœŸå½¢çŠ¶: ({batch_size}, {seq_len}, 1)")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    if gate_weights.shape != (batch_size, seq_len, 1):
        print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®: {gate_weights.shape} != ({batch_size}, {seq_len}, 1)")
        return False
    print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
    
    # éªŒè¯é—¨æ§æƒé‡èŒƒå›´ [0, 1]
    if (gate_weights < 0).any() or (gate_weights > 1).any():
        print("âŒ é—¨æ§æƒé‡è¶…å‡ºèŒƒå›´ [0, 1]")
        print(f"   æœ€å°å€¼: {gate_weights.min().item():.6f}")
        print(f"   æœ€å¤§å€¼: {gate_weights.max().item():.6f}")
        return False
    print("âœ… é—¨æ§æƒé‡åœ¨æœ‰æ•ˆèŒƒå›´ [0, 1] å†…")
    
    # éªŒè¯ä¸åŒä½ç½®çš„é—¨æ§æƒé‡ä¸åŒï¼ˆåº”è¯¥æ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€è®¡ç®—ï¼‰
    if torch.allclose(gate_weights, gate_weights[0, 0, 0].expand_as(gate_weights), atol=1e-5):
        print("âš ï¸  è­¦å‘Š: æ‰€æœ‰ä½ç½®çš„é—¨æ§æƒé‡ç›¸åŒï¼Œå¯èƒ½æ²¡æœ‰æ­£ç¡®è®¡ç®—")
    else:
        print("âœ… ä¸åŒä½ç½®çš„é—¨æ§æƒé‡ä¸åŒï¼ˆåŠ¨æ€è®¡ç®—ç”Ÿæ•ˆï¼‰")
        print(f"   é—¨æ§æƒé‡ç»Ÿè®¡: å‡å€¼={gate_weights.mean().item():.4f}, "
              f"æ ‡å‡†å·®={gate_weights.std().item():.4f}, "
              f"èŒƒå›´=[{gate_weights.min().item():.4f}, {gate_weights.max().item():.4f}]")
    
    return True


def test_adaptive_fusion_gate_bias_init():
    """æµ‹è¯•2: é—¨æ§åç½®åˆå§‹åŒ–å½±å“"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•2: é—¨æ§åç½®åˆå§‹åŒ–å½±å“")
    print("=" * 70)
    
    d_model = 16
    batch_size = 2
    seq_len = 10
    
    # åˆ›å»ºç›¸åŒçš„è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    
    # æµ‹è¯•ä¸åŒçš„åç½®åˆå§‹åŒ–
    bias_values = [-2.0, 0.0, 2.0]
    results = []
    
    for bias_init in bias_values:
        gate = AdaptiveFusionGate(d_model, gate_bias_init=bias_init)
        gate.eval()
        
        with torch.no_grad():
            gate_weights = gate(trend_embedding, detail_embedding)
            mean_weight = gate_weights.mean().item()
            results.append((bias_init, mean_weight))
        
        print(f"\nåç½®åˆå§‹åŒ–: {bias_init}")
        print(f"  å¹³å‡é—¨æ§æƒé‡: {mean_weight:.4f}")
    
    # éªŒè¯ï¼šåç½®è¶Šå¤§ï¼Œå¹³å‡æƒé‡åº”è¯¥è¶Šå¤§ï¼ˆæ›´åå‘è¶‹åŠ¿ï¼‰
    if results[0][1] < results[1][1] < results[2][1]:
        print("\nâœ… åç½®åˆå§‹åŒ–å½±å“æ­£ç¡®ï¼šåç½®è¶Šå¤§ï¼Œå¹³å‡æƒé‡è¶Šå¤§ï¼ˆæ›´åå‘è¶‹åŠ¿ï¼‰")
    else:
        print("\nâš ï¸  è­¦å‘Š: åç½®åˆå§‹åŒ–å½±å“ä¸ç¬¦åˆé¢„æœŸ")
    
    return True


def test_dual_reprogramming_adaptive_gate():
    """æµ‹è¯•3: DualReprogrammingLayer ä½¿ç”¨ adaptive_gate èåˆ"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•3: DualReprogrammingLayer ä½¿ç”¨ adaptive_gate èåˆ")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»º DualReprogrammingLayer
    layer = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='adaptive_gate',
        gate_bias_init=0.0
    )
    
    print(f"\nèåˆæ–¹æ³•: {layer.fusion_method}")
    print(f"fusion_gate ç±»å‹: {type(layer.fusion_gate).__name__}")
    
    if layer.fusion_gate is None:
        print("âŒ fusion_gate æœªåˆå§‹åŒ–")
        return False
    print("âœ… fusion_gate å·²æ­£ç¡®åˆå§‹åŒ–")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  - trend_embedding: {trend_embedding.shape}")
    print(f"  - detail_embedding: {detail_embedding.shape}")
    print(f"  - trend_prototypes: {trend_prototypes.shape}")
    print(f"  - detail_prototypes: {detail_prototypes.shape}")
    
    # å‰å‘ä¼ æ’­
    layer.eval()
    with torch.no_grad():
        output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"é¢„æœŸå½¢çŠ¶: ({batch_size}, {seq_len}, {d_llm})")
    
    if output.shape != (batch_size, seq_len, d_llm):
        print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®")
        return False
    print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
    
    # éªŒè¯è¾“å‡ºå€¼
    if torch.isnan(output).any():
        print("âŒ è¾“å‡ºåŒ…å«NaNå€¼")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— NaNï¼‰")
    
    if torch.isinf(output).any():
        print("âŒ è¾“å‡ºåŒ…å«Infå€¼")
        return False
    print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— Infï¼‰")
    
    return True


def test_fusion_methods_comparison():
    """æµ‹è¯•4: ä¸åŒèåˆæ–¹æ³•å¯¹æ¯”"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•4: ä¸åŒèåˆæ–¹æ³•å¯¹æ¯”")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»ºç›¸åŒçš„è¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model)
    detail_embedding = torch.randn(batch_size, seq_len, d_model)
    trend_prototypes = torch.randn(num_prototypes, d_llm)
    detail_prototypes = torch.randn(num_prototypes, d_llm)
    
    # æµ‹è¯•ä¸åŒçš„èåˆæ–¹æ³•
    fusion_methods = ['mean', 'weighted', 'adaptive_gate']
    results = {}
    
    for method in fusion_methods:
        layer = DualReprogrammingLayer(
            d_model=d_model,
            n_heads=n_heads,
            d_keys=d_model // n_heads,
            d_llm=d_llm,
            attention_dropout=0.1,
            fusion_method=method,
            gate_bias_init=0.0
        )
        layer.eval()
        
        with torch.no_grad():
            output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
        
        results[method] = {
            'output': output,
            'mean': output.mean().item(),
            'std': output.std().item(),
            'params': sum(p.numel() for p in layer.parameters())
        }
        
        print(f"\n{method} èåˆ:")
        print(f"  è¾“å‡ºç»Ÿè®¡: å‡å€¼={results[method]['mean']:.6f}, æ ‡å‡†å·®={results[method]['std']:.6f}")
        print(f"  å‚æ•°é‡: {results[method]['params']:,}")
    
    # éªŒè¯ä¸åŒæ–¹æ³•äº§ç”Ÿä¸åŒçš„è¾“å‡º
    mean_output = results['mean']['output']
    weighted_output = results['weighted']['output']
    adaptive_output = results['adaptive_gate']['output']
    
    if torch.allclose(mean_output, weighted_output, atol=1e-5):
        print("\nâš ï¸  è­¦å‘Š: mean å’Œ weighted è¾“å‡ºè¿‡äºæ¥è¿‘")
    else:
        print("\nâœ… mean å’Œ weighted è¾“å‡ºä¸åŒ")
    
    if torch.allclose(mean_output, adaptive_output, atol=1e-5):
        print("âš ï¸  è­¦å‘Š: mean å’Œ adaptive_gate è¾“å‡ºè¿‡äºæ¥è¿‘")
    else:
        print("âœ… mean å’Œ adaptive_gate è¾“å‡ºä¸åŒ")
    
    if torch.allclose(weighted_output, adaptive_output, atol=1e-5):
        print("âš ï¸  è­¦å‘Š: weighted å’Œ adaptive_gate è¾“å‡ºè¿‡äºæ¥è¿‘")
    else:
        print("âœ… weighted å’Œ adaptive_gate è¾“å‡ºä¸åŒ")
    
    # éªŒè¯å‚æ•°é‡
    print(f"\nå‚æ•°é‡å¯¹æ¯”:")
    print(f"  - mean: {results['mean']['params']:,} (æ— é¢å¤–å‚æ•°)")
    print(f"  - weighted: {results['weighted']['params']:,} (1ä¸ªå‚æ•°)")
    print(f"  - adaptive_gate: {results['adaptive_gate']['params']:,} (é—¨æ§ç½‘ç»œå‚æ•°)")
    
    if results['adaptive_gate']['params'] > results['weighted']['params']:
        print("âœ… adaptive_gate å‚æ•°é‡å¤§äº weightedï¼ˆç¬¦åˆé¢„æœŸï¼‰")
    else:
        print("âš ï¸  è­¦å‘Š: adaptive_gate å‚æ•°é‡å¼‚å¸¸")
    
    return True


def test_gradient_flow():
    """æµ‹è¯•5: æ¢¯åº¦æµæµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•5: æ¢¯åº¦æµæµ‹è¯•")
    print("=" * 70)
    
    d_model = 16
    d_llm = 768
    n_heads = 4
    batch_size = 2
    seq_len = 10
    num_prototypes = 100
    
    # åˆ›å»ºå±‚
    layer = DualReprogrammingLayer(
        d_model=d_model,
        n_heads=n_heads,
        d_keys=d_model // n_heads,
        d_llm=d_llm,
        attention_dropout=0.1,
        fusion_method='adaptive_gate',
        gate_bias_init=0.0
    )
    layer.train()
    
    # åˆ›å»ºè¾“å…¥
    trend_embedding = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
    detail_embedding = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
    trend_prototypes = torch.randn(num_prototypes, d_llm, requires_grad=False)
    detail_prototypes = torch.randn(num_prototypes, d_llm, requires_grad=False)
    
    # å‰å‘ä¼ æ’­
    output = layer(trend_embedding, detail_embedding, trend_prototypes, detail_prototypes)
    
    # åˆ›å»ºè™šæ‹ŸæŸå¤±
    target = torch.randn_like(output)
    loss = nn.MSELoss()(output, target)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    print("\n[æ£€æŸ¥1] è¾“å…¥æ¢¯åº¦...")
    if trend_embedding.grad is None or detail_embedding.grad is None:
        print("âŒ è¾“å…¥æ²¡æœ‰æ¢¯åº¦")
        return False
    print("âœ… è¾“å…¥æœ‰æ¢¯åº¦")
    
    print("\n[æ£€æŸ¥2] é—¨æ§ç½‘ç»œå‚æ•°æ¢¯åº¦...")
    gate_has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 
                       for p in layer.fusion_gate.parameters())
    if not gate_has_grad:
        print("âŒ é—¨æ§ç½‘ç»œæ²¡æœ‰æ¢¯åº¦")
        return False
    print("âœ… é—¨æ§ç½‘ç»œæœ‰æ¢¯åº¦")
    
    print("\n[æ£€æŸ¥3] é‡ç¼–ç¨‹å±‚å‚æ•°æ¢¯åº¦...")
    trend_has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 
                        for p in layer.trend_reprogramming.parameters())
    detail_has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 
                         for p in layer.detail_reprogramming.parameters())
    if not trend_has_grad or not detail_has_grad:
        print("âŒ é‡ç¼–ç¨‹å±‚æ²¡æœ‰æ¢¯åº¦")
        return False
    print("âœ… é‡ç¼–ç¨‹å±‚æœ‰æ¢¯åº¦")
    
    return True


def test_end_to_end():
    """æµ‹è¯•6: ç«¯åˆ°ç«¯æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•6: ç«¯åˆ°ç«¯æµ‹è¯•ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰")
    print("=" * 70)
    
    configs = TestConfig()
    
    try:
        model = Model(configs)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æ£€æŸ¥èåˆæ–¹æ³•
    if hasattr(model, 'reprogramming_layer') and model.reprogramming_layer is not None:
        if model.reprogramming_layer.fusion_method != 'adaptive_gate':
            print(f"âŒ èåˆæ–¹æ³•ä¸æ­£ç¡®: {model.reprogramming_layer.fusion_method} != adaptive_gate")
            return False
        print(f"âœ… èåˆæ–¹æ³•æ­£ç¡®: {model.reprogramming_layer.fusion_method}")
        
        if model.reprogramming_layer.fusion_gate is None:
            print("âŒ fusion_gate æœªåˆå§‹åŒ–")
            return False
        print("âœ… fusion_gate å·²åˆå§‹åŒ–")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    x_enc = torch.randn(batch_size, configs.seq_len, configs.enc_in)
    x_mark_enc = torch.zeros(batch_size, configs.seq_len, 4)
    x_dec = torch.randn(batch_size, configs.pred_len, configs.enc_in)
    x_mark_dec = torch.zeros(batch_size, configs.pred_len, 4)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  - x_enc: {x_enc.shape}")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    try:
        with torch.no_grad():
            output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"é¢„æœŸå½¢çŠ¶: ({batch_size}, {configs.pred_len}, {configs.enc_in})")
        
        if output.shape != (batch_size, configs.pred_len, configs.enc_in):
            print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…")
            return False
        print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
        
        # æ£€æŸ¥è¾“å‡ºå€¼
        if torch.isnan(output).any():
            print("âŒ è¾“å‡ºåŒ…å«NaNå€¼")
            return False
        print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— NaNï¼‰")
        
        if torch.isinf(output).any():
            print("âŒ è¾“å‡ºåŒ…å«Infå€¼")
            return False
        print("âœ… è¾“å‡ºå€¼åˆç†ï¼ˆæ— Infï¼‰")
        
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 70)
    print("åŠ¨æ€é—¨æ§èåˆå®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    
    tests = [
        ("AdaptiveFusionGate åŸºæœ¬åŠŸèƒ½", test_adaptive_fusion_gate_basic),
        ("é—¨æ§åç½®åˆå§‹åŒ–å½±å“", test_adaptive_fusion_gate_bias_init),
        ("DualReprogrammingLayer adaptive_gate èåˆ", test_dual_reprogramming_adaptive_gate),
        ("ä¸åŒèåˆæ–¹æ³•å¯¹æ¯”", test_fusion_methods_comparison),
        ("æ¢¯åº¦æµæµ‹è¯•", test_gradient_flow),
        ("ç«¯åˆ°ç«¯æµ‹è¯•", test_end_to_end),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\nâŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status}: {test_name}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŠ¨æ€é—¨æ§èåˆå®ç°æ­£ç¡®ï¼")
        return True
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)

