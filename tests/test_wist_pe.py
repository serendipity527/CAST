"""
WIST-PE (Wavelet-Informed Spatio-Temporal Patch Embedding) æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹:
1. æ¨¡å—å®ä¾‹åŒ–æµ‹è¯•
2. å‰å‘ä¼ æ’­å½¢çŠ¶æµ‹è¯•
3. å› æœæ€§éªŒè¯æµ‹è¯•
4. é—¨æ§æœºåˆ¶æµ‹è¯•
5. è½¯é˜ˆå€¼å»å™ªæµ‹è¯•
6. ä¸åŸç‰ˆ PatchEmbedding å¯¹æ¯”æµ‹è¯•

Author: CAST Project
Date: 2024
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import numpy as np

print("=" * 70)
print("WIST-PE å•å…ƒæµ‹è¯•")
print("=" * 70)

# è®¾å¤‡é€‰æ‹©
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"\næµ‹è¯•è®¾å¤‡: {device}")


def test_instantiation():
    """æµ‹è¯•1: æ¨¡å—å®ä¾‹åŒ–"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•1: æ¨¡å—å®ä¾‹åŒ–")
    print("=" * 70)
    
    from layers.Embed import WISTPatchEmbedding, PatchEmbedding, WaveletPatchEmbedding
    
    # æµ‹è¯• WISTPatchEmbedding å®ä¾‹åŒ–
    try:
        wist_pe = WISTPatchEmbedding(
            d_model=32,
            patch_len=16,
            stride=8,
            dropout=0.1,
            wavelet_type='db4',
            wavelet_level=1,
            hf_dropout=0.5,
            gate_bias_init=2.0,
            use_soft_threshold=True
        ).to(device)
        print("\nâœ… WISTPatchEmbedding å®ä¾‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"\nâŒ WISTPatchEmbedding å®ä¾‹åŒ–å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ä¸åŒå°æ³¢ç±»å‹
    for wavelet in ['db1', 'db2', 'db3', 'db4', 'db5', 'haar']:
        try:
            _ = WISTPatchEmbedding(
                d_model=32, patch_len=16, stride=8, dropout=0.1,
                wavelet_type=wavelet
            ).to(device)
            print(f"  âœ… å°æ³¢ç±»å‹ '{wavelet}' æ”¯æŒ")
        except Exception as e:
            print(f"  âŒ å°æ³¢ç±»å‹ '{wavelet}' å¤±è´¥: {e}")
    
    return True


def test_forward_shape():
    """æµ‹è¯•2: å‰å‘ä¼ æ’­å½¢çŠ¶"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•2: å‰å‘ä¼ æ’­å½¢çŠ¶")
    print("=" * 70)
    
    from layers.Embed import WISTPatchEmbedding, PatchEmbedding
    
    # æµ‹è¯•å‚æ•°
    B, N, T = 4, 7, 512  # batch, variables, time
    d_model = 32
    patch_len = 16
    stride = 8
    
    print(f"\nè¾“å…¥å‚æ•°: B={B}, N={N}, T={T}")
    print(f"æ¨¡å‹å‚æ•°: d_model={d_model}, patch_len={patch_len}, stride={stride}")
    
    # è®¡ç®—é¢„æœŸçš„ patch æ•°é‡
    num_patches = int((T - patch_len) / stride + 2)  # +2 æ˜¯å› ä¸º padding
    print(f"é¢„æœŸ num_patches: {num_patches}")
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(B, N, T, device=device)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # WIST-PE å‰å‘ä¼ æ’­
    wist_pe = WISTPatchEmbedding(
        d_model=d_model, patch_len=patch_len, stride=stride, dropout=0.1
    ).to(device)
    
    output, n_vars = wist_pe(x)
    print(f"WIST-PE è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¿”å›çš„ n_vars: {n_vars}")
    
    # éªŒè¯å½¢çŠ¶
    expected_shape = (B * N, num_patches, d_model)
    if output.shape == expected_shape:
        print(f"âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {output.shape} == {expected_shape}")
    else:
        print(f"âŒ è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape} != {expected_shape}")
        return False
    
    if n_vars == N:
        print(f"âœ… n_vars æ­£ç¡®: {n_vars} == {N}")
    else:
        print(f"âŒ n_vars é”™è¯¯: {n_vars} != {N}")
        return False
    
    # å¯¹æ¯”åŸç‰ˆ PatchEmbedding
    original_pe = PatchEmbedding(
        d_model=d_model, patch_len=patch_len, stride=stride, dropout=0.1
    ).to(device)
    
    # åŸç‰ˆéœ€è¦ (B, N, T) -> (B*N, T, 1) çš„è½¬æ¢
    x_for_original = x.reshape(B * N, T, 1).permute(0, 2, 1)  # (B*N, 1, T)
    output_orig, n_vars_orig = original_pe(x_for_original)
    print(f"\nåŸç‰ˆ PatchEmbedding è¾“å‡ºå½¢çŠ¶: {output_orig.shape}")
    
    return True


def test_causality():
    """æµ‹è¯•3: å› æœæ€§éªŒè¯ - ä¿®æ”¹æœªæ¥æ•°æ®ä¸åº”å½±å“è¿‡å»çš„è¾“å‡º"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•3: å› æœæ€§éªŒè¯")
    print("=" * 70)
    
    from layers.Embed import WISTPatchEmbedding
    
    # åˆ›å»ºæ¨¡å‹ (evalæ¨¡å¼å…³é—­dropout)
    wist_pe = WISTPatchEmbedding(
        d_model=32, patch_len=16, stride=8, dropout=0.0,
        hf_dropout=0.0  # å…³é—­é«˜é¢‘dropoutä»¥ä¾¿ç²¾ç¡®æµ‹è¯•
    ).to(device)
    wist_pe.eval()
    
    # åˆ›å»ºåŸå§‹è¾“å…¥
    B, N, T = 1, 1, 128
    x_orig = torch.randn(B, N, T, device=device)
    
    # å¤åˆ¶å¹¶ä¿®æ”¹"æœªæ¥"çš„æ•°æ®ç‚¹
    x_mod = x_orig.clone()
    target_time = 80  # ä¿®æ”¹ t=80 åŠä¹‹åçš„æ•°æ®
    x_mod[:, :, target_time:] += 100.0  # å¤§å¹…ä¿®æ”¹æœªæ¥æ•°æ®
    
    print(f"\næµ‹è¯•: ä¿®æ”¹ t>={target_time} çš„æ•°æ®(+100)ï¼Œæ£€æŸ¥ t<{target_time} å¯¹åº”çš„ patch è¾“å‡º")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output_orig, _ = wist_pe(x_orig)
        output_mod, _ = wist_pe(x_mod)
    
    # è®¡ç®—å“ªäº› patch å®Œå…¨åœ¨ target_time ä¹‹å‰
    patch_len = 16
    stride = 8
    # patch i è¦†ç›–çš„æ—¶é—´èŒƒå›´æ˜¯ [i*stride, i*stride + patch_len)
    # å¦‚æœ i*stride + patch_len <= target_timeï¼Œåˆ™è¯¥ patch å®Œå…¨åœ¨ä¿®æ”¹ç‚¹ä¹‹å‰
    safe_patches = target_time // stride - 1  # ä¿å®ˆä¼°è®¡
    
    print(f"å®‰å…¨ patch æ•°é‡ (å®Œå…¨åœ¨ä¿®æ”¹ç‚¹ä¹‹å‰): {safe_patches}")
    
    # æ£€æŸ¥å®‰å…¨ patch çš„è¾“å‡ºæ˜¯å¦ä¸€è‡´
    diff = (output_orig[:, :safe_patches, :] - output_mod[:, :safe_patches, :]).abs()
    max_diff = diff.max().item()
    mean_diff = diff.mean().item()
    
    print(f"\nå‰ {safe_patches} ä¸ª patch çš„å·®å¼‚:")
    print(f"  - æœ€å¤§å·®å¼‚: {max_diff:.10f}")
    print(f"  - å¹³å‡å·®å¼‚: {mean_diff:.10f}")
    
    if max_diff < 1e-5:
        print("âœ… å› æœæ€§éªŒè¯é€šè¿‡: ä¿®æ”¹æœªæ¥æ•°æ®ä¸å½±å“è¿‡å»çš„ patch è¾“å‡º")
        return True
    else:
        print("âŒ å› æœæ€§éªŒè¯å¤±è´¥: å­˜åœ¨ä¿¡æ¯æ³„éœ²")
        return False


def test_gate_mechanism():
    """æµ‹è¯•4: é—¨æ§æœºåˆ¶æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•4: é—¨æ§æœºåˆ¶æµ‹è¯•")
    print("=" * 70)
    
    from layers.Embed import WISTPatchEmbedding
    
    # æµ‹è¯•ä¸åŒçš„ gate_bias_init
    for bias_init in [0.0, 2.0, 4.0]:
        wist_pe = WISTPatchEmbedding(
            d_model=32, patch_len=16, stride=8, dropout=0.0,
            gate_bias_init=bias_init, hf_dropout=0.0
        ).to(device)
        
        # è·å–é—¨æ§å±‚çš„åç½®
        gate_bias = None
        for m in wist_pe.gate.modules():
            if isinstance(m, nn.Linear):
                gate_bias = m.bias.data.mean().item()
                break
        
        expected_ratio = torch.sigmoid(torch.tensor(bias_init)).item()
        print(f"\nbias_init={bias_init:.1f} -> sigmoid={expected_ratio:.2%} ä½é¢‘å…³æ³¨åº¦")
        print(f"  å®é™… gate bias: {gate_bias:.4f}")
        
        if abs(gate_bias - bias_init) < 1e-5:
            print(f"  âœ… é—¨æ§åç½®åˆå§‹åŒ–æ­£ç¡®")
        else:
            print(f"  âŒ é—¨æ§åç½®åˆå§‹åŒ–é”™è¯¯")
    
    return True


def test_soft_threshold():
    """æµ‹è¯•5: è½¯é˜ˆå€¼å»å™ªæµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•5: è½¯é˜ˆå€¼å»å™ªæµ‹è¯•")
    print("=" * 70)
    
    from layers.Embed import SoftThreshold
    
    # åˆ›å»ºè½¯é˜ˆå€¼æ¨¡å—
    num_features = 16
    init_tau = 0.5
    soft_thresh = SoftThreshold(num_features=num_features, init_tau=init_tau).to(device)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(4, 10, num_features, device=device)
    
    # åº”ç”¨è½¯é˜ˆå€¼
    y = soft_thresh(x)
    
    print(f"\nè¾“å…¥ç»Ÿè®¡: mean={x.mean().item():.4f}, std={x.std().item():.4f}")
    print(f"è¾“å‡ºç»Ÿè®¡: mean={y.mean().item():.4f}, std={y.std().item():.4f}")
    
    # éªŒè¯è½¯é˜ˆå€¼æ•ˆæœ: å°äºé˜ˆå€¼çš„å€¼åº”è¯¥å˜ä¸º0æˆ–æ¥è¿‘0
    tau = soft_thresh.tau.abs()
    small_values_mask = x.abs() < tau.mean()
    small_values_output = y[small_values_mask]
    
    # è½¯é˜ˆå€¼åº”è¯¥å°†å°äºtauçš„å€¼å‹ç¼©
    print(f"\né˜ˆå€¼ tau å‡å€¼: {tau.mean().item():.4f}")
    print(f"å°äºé˜ˆå€¼çš„è¾“å…¥æ•°é‡: {small_values_mask.sum().item()}")
    print(f"å¯¹åº”è¾“å‡ºçš„ç»å¯¹å€¼å‡å€¼: {small_values_output.abs().mean().item():.6f}")
    
    # éªŒè¯å¯å­¦ä¹ æ€§
    print(f"\ntau æ˜¯å¦å¯å­¦ä¹ : {soft_thresh.tau.requires_grad}")
    
    if soft_thresh.tau.requires_grad:
        print("âœ… è½¯é˜ˆå€¼å‚æ•°å¯å­¦ä¹ ")
        return True
    else:
        print("âŒ è½¯é˜ˆå€¼å‚æ•°ä¸å¯å­¦ä¹ ")
        return False


def test_gradient_flow():
    """æµ‹è¯•6: æ¢¯åº¦æµæµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•6: æ¢¯åº¦æµæµ‹è¯•")
    print("=" * 70)
    
    from layers.Embed import WISTPatchEmbedding
    
    # åˆ›å»ºæ¨¡å‹
    wist_pe = WISTPatchEmbedding(
        d_model=32, patch_len=16, stride=8, dropout=0.1
    ).to(device)
    wist_pe.train()
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(2, 3, 128, device=device, requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    output, _ = wist_pe(x)
    
    # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
    loss = output.mean()
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    print("\næ£€æŸ¥å„ç»„ä»¶æ¢¯åº¦:")
    
    grad_checks = {
        'low_freq_embedding': wist_pe.low_freq_embedding.weight.grad,
        'high_freq_embedding': wist_pe.high_freq_embedding.weight.grad,
        'soft_threshold.tau': wist_pe.soft_threshold.tau.grad if hasattr(wist_pe, 'soft_threshold') else None,
    }
    
    # æ£€æŸ¥é—¨æ§å±‚æ¢¯åº¦
    for name, module in wist_pe.gate.named_modules():
        if isinstance(module, nn.Linear):
            grad_checks['gate.weight'] = module.weight.grad
            grad_checks['gate.bias'] = module.bias.grad
    
    all_grads_ok = True
    for name, grad in grad_checks.items():
        if grad is not None:
            grad_norm = grad.norm().item()
            status = "âœ…" if grad_norm > 0 else "âš ï¸"
            print(f"  {status} {name}: grad_norm = {grad_norm:.6f}")
            if grad_norm == 0:
                all_grads_ok = False
        else:
            print(f"  âš ï¸ {name}: æ— æ¢¯åº¦")
    
    if all_grads_ok:
        print("\nâœ… æ¢¯åº¦æµæ­£å¸¸")
        return True
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ¢¯åº¦ä¸ºé›¶ï¼Œè¯·æ£€æŸ¥")
        return True  # ä¸ç®—å¤±è´¥ï¼Œå› ä¸ºå¯èƒ½æ˜¯æ­£å¸¸ç°è±¡


def test_different_seq_lengths():
    """æµ‹è¯•7: ä¸åŒåºåˆ—é•¿åº¦æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•7: ä¸åŒåºåˆ—é•¿åº¦æµ‹è¯•")
    print("=" * 70)
    
    from layers.Embed import WISTPatchEmbedding
    
    wist_pe = WISTPatchEmbedding(
        d_model=32, patch_len=16, stride=8, dropout=0.0
    ).to(device)
    wist_pe.eval()
    
    # æµ‹è¯•ä¸åŒçš„åºåˆ—é•¿åº¦
    test_lengths = [64, 96, 128, 256, 512]
    
    print("\nåºåˆ—é•¿åº¦æµ‹è¯•:")
    for T in test_lengths:
        try:
            x = torch.randn(2, 3, T, device=device)
            with torch.no_grad():
                output, n_vars = wist_pe(x)
            print(f"  âœ… T={T}: è¾“å‡ºå½¢çŠ¶ {output.shape}")
        except Exception as e:
            print(f"  âŒ T={T}: å¤±è´¥ - {e}")
            return False
    
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("å¼€å§‹è¿è¡Œæ‰€æœ‰ WIST-PE æµ‹è¯•")
    print("=" * 70)
    
    tests = [
        ("æ¨¡å—å®ä¾‹åŒ–", test_instantiation),
        ("å‰å‘ä¼ æ’­å½¢çŠ¶", test_forward_shape),
        ("å› æœæ€§éªŒè¯", test_causality),
        ("é—¨æ§æœºåˆ¶", test_gate_mechanism),
        ("è½¯é˜ˆå€¼å»å™ª", test_soft_threshold),
        ("æ¢¯åº¦æµ", test_gradient_flow),
        ("ä¸åŒåºåˆ—é•¿åº¦", test_different_seq_lengths),
    ]
    
    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{name}' å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    passed = sum(1 for _, r in results if r)
    total = len(results)
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {status}: {name}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼WIST-PE å®ç°æ­£ç¡®ï¼")
    else:
        print(f"\nâš ï¸ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
    
    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
