import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from torch.nn.utils import weight_norm
import math


class PositionalEmbedding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEmbedding, self).__init__()
        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model).float()
        pe.require_grad = False

        position = torch.arange(0, max_len).float().unsqueeze(1)
        div_term = (torch.arange(0, d_model, 2).float()
                    * -(math.log(10000.0) / d_model)).exp()

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return self.pe[:, :x.size(1)]


class TokenEmbedding(nn.Module):
    def __init__(self, c_in, d_model):
        super(TokenEmbedding, self).__init__()
        padding = 1 if torch.__version__ >= '1.5.0' else 2
        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,
                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(
                    m.weight, mode='fan_in', nonlinearity='leaky_relu')

    def forward(self, x):
        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        return x


class FixedEmbedding(nn.Module):
    def __init__(self, c_in, d_model):
        super(FixedEmbedding, self).__init__()

        w = torch.zeros(c_in, d_model).float()
        w.require_grad = False

        position = torch.arange(0, c_in).float().unsqueeze(1)
        div_term = (torch.arange(0, d_model, 2).float()
                    * -(math.log(10000.0) / d_model)).exp()

        w[:, 0::2] = torch.sin(position * div_term)
        w[:, 1::2] = torch.cos(position * div_term)

        self.emb = nn.Embedding(c_in, d_model)
        self.emb.weight = nn.Parameter(w, requires_grad=False)

    def forward(self, x):
        return self.emb(x).detach()


class TemporalEmbedding(nn.Module):
    def __init__(self, d_model, embed_type='fixed', freq='h'):
        super(TemporalEmbedding, self).__init__()

        minute_size = 4
        hour_size = 24
        weekday_size = 7
        day_size = 32
        month_size = 13

        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding
        if freq == 't':
            self.minute_embed = Embed(minute_size, d_model)
        self.hour_embed = Embed(hour_size, d_model)
        self.weekday_embed = Embed(weekday_size, d_model)
        self.day_embed = Embed(day_size, d_model)
        self.month_embed = Embed(month_size, d_model)

    def forward(self, x):
        x = x.long()
        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(
            self, 'minute_embed') else 0.
        hour_x = self.hour_embed(x[:, :, 3])
        weekday_x = self.weekday_embed(x[:, :, 2])
        day_x = self.day_embed(x[:, :, 1])
        month_x = self.month_embed(x[:, :, 0])

        return hour_x + weekday_x + day_x + month_x + minute_x


class TimeFeatureEmbedding(nn.Module):
    def __init__(self, d_model, embed_type='timeF', freq='h'):
        super(TimeFeatureEmbedding, self).__init__()

        freq_map = {'h': 4, 't': 5, 's': 6,
                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}
        d_inp = freq_map[freq]
        self.embed = nn.Linear(d_inp, d_model, bias=False)

    def forward(self, x):
        return self.embed(x)


class DataEmbedding(nn.Module):
    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):
        super(DataEmbedding, self).__init__()

        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)
        self.position_embedding = PositionalEmbedding(d_model=d_model)
        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,
                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(
            d_model=d_model, embed_type=embed_type, freq=freq)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x, x_mark):
        if x_mark is None:
            x = self.value_embedding(x) + self.position_embedding(x).to(x.device)
        else:
            x = self.value_embedding(
                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)
        return self.dropout(x)


class DataEmbedding_wo_pos(nn.Module):
    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):
        super(DataEmbedding_wo_pos, self).__init__()

        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)
        self.position_embedding = PositionalEmbedding(d_model=d_model)
        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,
                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(
            d_model=d_model, embed_type=embed_type, freq=freq)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x, x_mark):
        if x_mark is None:
            x = self.value_embedding(x)
        else:
            x = self.value_embedding(x) + self.temporal_embedding(x_mark)
        return self.dropout(x)


class ReplicationPad1d(nn.Module):
    def __init__(self, padding) -> None:
        super(ReplicationPad1d, self).__init__()
        self.padding = padding

    def forward(self, input: Tensor) -> Tensor:
        replicate_padding = input[:, :, -1].unsqueeze(-1).repeat(1, 1, self.padding[-1])
        output = torch.cat([input, replicate_padding], dim=-1)
        return output


class PatchEmbedding(nn.Module):
    def __init__(self, d_model, patch_len, stride, dropout, use_positional_encoding=False, pos_encoding_max_len=5000):
        super(PatchEmbedding, self).__init__()
        # Patching
        self.patch_len = patch_len
        self.stride = stride
        self.padding_patch_layer = ReplicationPad1d((0, stride))

        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space
        self.value_embedding = TokenEmbedding(patch_len, d_model)

        # Positional embedding (å¯é€‰)
        self.use_positional_encoding = use_positional_encoding
        if use_positional_encoding:
            self.position_embedding = PositionalEmbedding(d_model, max_len=pos_encoding_max_len)
            print(f"[PatchEmbedding] ä½ç½®ç¼–ç å·²å¯ç”¨ (max_len={pos_encoding_max_len})")
        else:
            self.position_embedding = None

        # Residual dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # do patching
        n_vars = x.shape[1]
        x = self.padding_patch_layer(x)
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))
        # Input encoding
        x = self.value_embedding(x)
        # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_positional_encoding:
            x = x + self.position_embedding(x).to(x.device)
        return self.dropout(x), n_vars


class SoftThreshold(nn.Module):
    """
    å¯å­¦ä¹ è½¯é˜ˆå€¼å»å™ªæ¨¡å—
    å…¬å¼: y = sign(x) * ReLU(|x| - tau)
    å½“ |x| < tau æ—¶ï¼Œè¾“å‡ºä¸º 0ï¼ˆè§†ä¸ºå™ªå£°ï¼‰
    å½“ |x| >= tau æ—¶ï¼Œè¾“å‡ºä¸º sign(x) * (|x| - tau)ï¼ˆä¿ç•™ä½†æ”¶ç¼©ï¼‰
    """
    def __init__(self, num_features, init_tau=0.1):
        super(SoftThreshold, self).__init__()
        # å¯å­¦ä¹ çš„é˜ˆå€¼å‚æ•°ï¼Œæ¯ä¸ªç‰¹å¾ç»´åº¦ä¸€ä¸ª
        self.tau = nn.Parameter(torch.ones(num_features) * init_tau)
    
    def forward(self, x):
        # x: (..., num_features)
        # tau éœ€è¦ broadcast åˆ° x çš„å½¢çŠ¶
        tau = torch.abs(self.tau)  # ç¡®ä¿é˜ˆå€¼ä¸ºæ­£
        return torch.sign(x) * torch.relu(torch.abs(x) - tau)


class FrequencyEmbedding(nn.Module):
    """
    å¯å­¦ä¹ çš„é¢‘æ®µåµŒå…¥ (Frequency Embedding)
    
    ä¸ºä¸åŒé¢‘æ®µçš„ Patch æ·»åŠ å¯å­¦ä¹ çš„é¢‘æ®µæ ‡è¯†å‘é‡ï¼Œå¸®åŠ© LLM åŒºåˆ†ï¼š
    - "è¿™ä¸ª Patch æ¥è‡ªä½é¢‘è¶‹åŠ¿" vs "è¿™ä¸ª Patch æ¥è‡ªé«˜é¢‘å™ªå£°"
    
    æ ¸å¿ƒä½œç”¨:
    1. èº«ä»½è¯†åˆ«ï¼šæ˜¾å¼å‘Šè¯‰ LLM æ¯ä¸ª Patch çš„é¢‘æ®µæ¥æº
    2. æ³¨æ„åŠ›å¼•å¯¼ï¼šè®© Self-Attention æ›´å¥½åœ°åˆ†é…æ³¨æ„åŠ›
       - è¶‹åŠ¿ Patch åº”è¯¥å¤šå…³æ³¨è¿‡å»çš„è¶‹åŠ¿ Patch
       - é«˜é¢‘ Patch åº”è¯¥å…³æ³¨å±€éƒ¨çš„é«˜é¢‘ Patch
    3. è¯­ä¹‰è§£è€¦ï¼šæ‰“ç ´é¢‘æ®µé—´çš„å¯¹ç§°æ€§ï¼Œé˜²æ­¢æ··æ·†
    
    Args:
        d_model: Embedding ç»´åº¦ (ä¸ Patch Embedding è¾“å‡ºç»´åº¦ä¸€è‡´)
        num_frequencies: é¢‘æ®µæ•°é‡ (ä½é¢‘ã€ä¸­é¢‘ã€é«˜é¢‘ç­‰)
        init_method: åˆå§‹åŒ–æ–¹æ³• ('random', 'orthogonal', 'scaled')
    
    Input:
        x: (B*N, num_patches, d_model) - Patch Embedding è¾“å‡º
        freq_idx: int - é¢‘æ®µç´¢å¼• (0=ä½é¢‘, 1=ä¸­é¢‘, 2=é«˜é¢‘, ...)
    
    Output:
        (B*N, num_patches, d_model) - åŠ ä¸Šé¢‘æ®µæ ‡è¯†åçš„è¡¨ç¤º
    """
    
    def __init__(self, d_model, num_frequencies=3, init_method='random'):
        super(FrequencyEmbedding, self).__init__()
        
        self.d_model = d_model
        self.num_frequencies = num_frequencies
        self.init_method = init_method
        
        # ä¸ºæ¯ä¸ªé¢‘æ®µåˆ›å»ºä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡
        self.freq_embeddings = nn.Parameter(
            torch.empty(num_frequencies, d_model)
        )
        
        # åˆå§‹åŒ–
        self._init_embeddings()
        
        print("=" * 70)
        print("[FrequencyEmbedding] å¯å­¦ä¹ çš„é¢‘æ®µåµŒå…¥å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ Embedding ç»´åº¦: {d_model}")
        print(f"  â”œâ”€ é¢‘æ®µæ•°é‡: {num_frequencies}")
        print(f"  â”œâ”€ åˆå§‹åŒ–æ–¹æ³•: {init_method}")
        print(f"  â”œâ”€ å‚æ•°é‡: {num_frequencies * d_model:,}")
        print(f"  â””â”€ ä½œç”¨: æ˜¾å¼æ ‡è®°é¢‘æ®µèº«ä»½ï¼Œå¼•å¯¼ LLM Self-Attention")
        print("=" * 70)
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–é¢‘æ®µ Embedding"""
        if self.init_method == 'random':
            # éšæœºåˆå§‹åŒ–ï¼ˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰
            nn.init.normal_(self.freq_embeddings, mean=0.0, std=0.02)
        
        elif self.init_method == 'orthogonal':
            # æ­£äº¤åˆå§‹åŒ–ï¼ˆè®©ä¸åŒé¢‘æ®µçš„å‘é‡å°½é‡æ­£äº¤ï¼‰
            nn.init.orthogonal_(self.freq_embeddings)
        
        elif self.init_method == 'scaled':
            # åˆ†å±‚åˆå§‹åŒ–ï¼ˆä½é¢‘ç”¨è¾ƒå¤§å€¼ï¼Œé«˜é¢‘ç”¨è¾ƒå°å€¼ï¼‰
            with torch.no_grad():
                for i in range(self.num_frequencies):
                    # ä½é¢‘(i=0)æƒé‡æœ€å¤§ï¼Œé«˜é¢‘(i=n-1)æƒé‡æœ€å°
                    scale = 1.0 / (i + 1)
                    nn.init.normal_(self.freq_embeddings[i], mean=0.0, std=0.02 * scale)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„åˆå§‹åŒ–æ–¹æ³•: {self.init_method}")
    
    def forward(self, x, freq_idx):
        """
        å‰å‘ä¼ æ’­ï¼šä¸º Patch Embedding åŠ ä¸Šé¢‘æ®µæ ‡è¯†
        
        Args:
            x: (B*N, num_patches, d_model) - Patch Embedding è¾“å‡º
            freq_idx: int - é¢‘æ®µç´¢å¼• (0=ä½é¢‘, 1=ä¸­é¢‘, 2=é«˜é¢‘, ...)
        
        Returns:
            (B*N, num_patches, d_model) - åŠ ä¸Šé¢‘æ®µæ ‡è¯†åçš„è¡¨ç¤º
        """
        # å¹¿æ’­åŠ æ³•ï¼šæ¯ä¸ª Patch éƒ½åŠ ä¸Šå¯¹åº”é¢‘æ®µçš„ Embedding
        # freq_embeddings[freq_idx]: (d_model,) -> broadcast to (B*N, num_patches, d_model)
        return x + self.freq_embeddings[freq_idx]
    
    def get_similarity_matrix(self):
        """
        è®¡ç®—ä¸åŒé¢‘æ®µ Embedding çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆç”¨äºå¯è§†åŒ–/è°ƒè¯•ï¼‰
        
        Returns:
            sim_matrix: (num_frequencies, num_frequencies) - ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        # å½’ä¸€åŒ–
        emb_norm = F.normalize(self.freq_embeddings, p=2, dim=-1)
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim_matrix = torch.mm(emb_norm, emb_norm.t())
        return sim_matrix


class FrequencyChannelAttention(nn.Module):
    """
    é¢‘ç‡é€šé“æ³¨æ„åŠ›æ¨¡å— (Frequency Channel Attention)
    
    å€Ÿé‰´ SE-Net (Squeeze-and-Excitation) çš„æ€æƒ³ï¼Œå®ç° Instance-wise çš„é¢‘ç‡æƒé‡åˆ†é…ã€‚
    
    æ ¸å¿ƒä¼˜åŠ¿:
    1. åŠ¨æ€è·¯ç”± (Dynamic Routing): æ¯ä¸ªæ ·æœ¬æ ¹æ®è‡ªèº«ç‰¹æ€§åŠ¨æ€åˆ†é…é¢‘ç‡æƒé‡
       - æ ·æœ¬ A å¯èƒ½æ˜¯ä½é¢‘ä¸»å¯¼ â†’ è‡ªåŠ¨åŠ å¤§ä½é¢‘æƒé‡
       - æ ·æœ¬ B å¯èƒ½æ˜¯é«˜é¢‘ä¸»å¯¼ â†’ è‡ªåŠ¨åŠ å¤§é«˜é¢‘æƒé‡
    2. è‡ªåŠ¨ç‰¹å¾é€‰æ‹©: å¦‚æœæŸå±‚åˆ†è§£å‡ºçš„å…¨æ˜¯å™ªå£°ï¼ŒAttention æƒé‡è‡ªç„¶è¶‹è¿‘äº 0
    3. æ›¿ä»£ç¡¬ç¼–ç çš„ gate_bias_initï¼Œå®ç°çœŸæ­£çš„è‡ªé€‚åº”
    
    æµç¨‹:
    è¾“å…¥: [e_band_0, e_band_1, ..., e_band_n]  å„é¢‘æ®µ embedding, å½¢çŠ¶ (B*N, P, d_model)
        â†’ Stack: (B*N, P, d_model, num_bands)
        â†’ Squeeze: å…¨å±€å¹³å‡æ± åŒ– â†’ (B*N, num_bands, d_model)
        â†’ Excitation: MLP â†’ (B*N, num_bands, 1)
        â†’ Softmax: å½’ä¸€åŒ–æƒé‡ â†’ (B*N, num_bands, 1)
        â†’ Scale: åŠ æƒæ±‚å’Œ â†’ (B*N, P, d_model)
    """
    
    def __init__(self, num_bands, d_model, reduction=4):
        """
        Args:
            num_bands: é¢‘æ®µæ•°é‡ (level + 1)
            d_model: embedding ç»´åº¦
            reduction: MLP ä¸­é—´å±‚çš„é™ç»´æ¯”ä¾‹
        """
        super(FrequencyChannelAttention, self).__init__()
        
        self.num_bands = num_bands
        self.d_model = d_model
        
        # Excitation ç½‘ç»œ: è½»é‡çº§ MLP
        # è¾“å…¥: (B*N, num_bands, d_model) -> è¾“å‡º: (B*N, num_bands, 1)
        hidden_dim = max(d_model // reduction, 8)  # ç¡®ä¿è‡³å°‘æœ‰ 8 ä¸ªéšè—å•å…ƒ
        
        self.excitation = nn.Sequential(
            nn.Linear(d_model, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        )
        
        # åˆå§‹åŒ–: è®©åˆå§‹æƒé‡ç›¸å¯¹å‡åŒ€ï¼Œä½†ç•¥å¾®åå‘ä½é¢‘
        # æœ€åä¸€ä¸ª Linear çš„ bias è®¾ç½®ä¸º [0.5, 0, 0, ...]
        # è¿™æ · softmax åä½é¢‘ (ç¬¬ 0 ä¸ª) ä¼šæœ‰ç•¥é«˜çš„åˆå§‹æƒé‡
        with torch.no_grad():
            # è·å–æœ€åä¸€ä¸ª Linear å±‚
            last_linear = self.excitation[-1]
            nn.init.zeros_(last_linear.weight)
            nn.init.zeros_(last_linear.bias)
    
    def forward(self, band_embeddings):
        """
        Args:
            band_embeddings: list of tensors, æ¯ä¸ªå½¢çŠ¶ (B*N, num_patches, d_model)
                             é¡ºåº: [e_cA, e_cD_n, e_cD_{n-1}, ..., e_cD_1]
        
        Returns:
            output: åŠ æƒèåˆåçš„ embedding, å½¢çŠ¶ (B*N, num_patches, d_model)
            attention_weights: æ³¨æ„åŠ›æƒé‡, å½¢çŠ¶ (B*N, num_bands), ç”¨äºå¯è§†åŒ–/è°ƒè¯•
        """
        # Stack: list of (B*N, P, d_model) -> (B*N, P, d_model, num_bands)
        stacked = torch.stack(band_embeddings, dim=-1)
        B_N, P, D, num_bands = stacked.shape
        
        # Squeeze: å…¨å±€å¹³å‡æ± åŒ– (åœ¨ Patch ç»´åº¦ä¸Š)
        # (B*N, P, d_model, num_bands) -> (B*N, d_model, num_bands) -> (B*N, num_bands, d_model)
        squeezed = stacked.mean(dim=1).permute(0, 2, 1)  # (B*N, num_bands, d_model)
        
        # Excitation: MLP è®¡ç®—æ¯ä¸ªé¢‘æ®µçš„é‡è¦æ€§åˆ†æ•°
        # (B*N, num_bands, d_model) -> (B*N, num_bands, 1)
        scores = self.excitation(squeezed)  # (B*N, num_bands, 1)
        
        # Softmax: å½’ä¸€åŒ–æƒé‡ï¼Œç¡®ä¿æ€»å’Œä¸º 1
        attention_weights = F.softmax(scores, dim=1)  # (B*N, num_bands, 1)
        
        # Scale: åŠ æƒæ±‚å’Œ
        # stacked: (B*N, P, d_model, num_bands)
        # attention_weights: (B*N, num_bands, 1) -> (B*N, 1, 1, num_bands)
        weights_expanded = attention_weights.permute(0, 2, 1).unsqueeze(1)  # (B*N, 1, 1, num_bands)
        
        # åŠ æƒæ±‚å’Œ: (B*N, P, d_model, num_bands) * (B*N, 1, 1, num_bands) -> sum -> (B*N, P, d_model)
        output = (stacked * weights_expanded).sum(dim=-1)
        
        # è¿”å›èåˆç»“æœå’Œæ³¨æ„åŠ›æƒé‡ (ç”¨äºè°ƒè¯•)
        return output, attention_weights.squeeze(-1)  # (B*N, P, d_model), (B*N, num_bands)


class FrequencyChannelAttentionV2(nn.Module):
    """
    é¢‘ç‡é€šé“æ³¨æ„åŠ›æ¨¡å— V2 (Frequency Channel Attention with Local Context)
    
    ç›¸æ¯” V1 çš„æ”¹è¿›:
    - ä½¿ç”¨ 1D å·ç§¯æ›¿ä»£ Global Average Pooling (GAP)
    - å®ç° Patch-wise çš„åŠ¨æ€é¢‘ç‡æƒé‡åˆ†é…
    - æ¯ä¸ªæ—¶é—´æ­¥çš„ Patch å¯ä»¥æ‹¥æœ‰ä¸åŒçš„é¢‘ç‡èåˆæƒé‡
    - æ›´å¥½åœ°å¤„ç†éå¹³ç¨³æ—¶é—´åºåˆ—ï¼ˆå¦‚çªå˜ã€è¶‹åŠ¿è½¬æŠ˜ç‚¹ï¼‰
    
    æ ¸å¿ƒä¼˜åŠ¿:
    1. æ—¶å˜åŠ¨æ€è·¯ç”± (Time-Varying Dynamic Routing): 
       - ç¬¬ 5 ä¸ª Patch å¯èƒ½æ˜¯çªå˜ç‚¹ â†’ è‡ªåŠ¨åŠ å¤§é«˜é¢‘æƒé‡
       - ç¬¬ 6 ä¸ª Patch å›å½’å¹³ç¨³ â†’ è‡ªåŠ¨åŠ å¤§ä½é¢‘æƒé‡
    2. å±€éƒ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥: 1D å·ç§¯èšåˆç›¸é‚» Patch çš„ä¿¡æ¯ï¼Œè€Œéå…¨å±€å¹³å‡
    3. ä¿ç•™æ—¶é—´ç»´åº¦: è¾“å‡ºæƒé‡å½¢çŠ¶ä¸º (B*N, P, num_bands)ï¼Œè€Œé (B*N, num_bands)
    
    æµç¨‹:
    è¾“å…¥: [e_band_0, e_band_1, ..., e_band_n]  å„é¢‘æ®µ embedding, å½¢çŠ¶ (B*N, P, d_model)
        â†’ Stack: (B*N, P, d_model, num_bands)
        â†’ Permute: (B*N, num_bands, d_model, P)
        â†’ 1D Conv (åœ¨ Patch ç»´åº¦ä¸Š): èšåˆå±€éƒ¨ä¸Šä¸‹æ–‡
        â†’ MLP: è®¡ç®—æ¯ä¸ª Patch çš„é¢‘ç‡é‡è¦æ€§åˆ†æ•°
        â†’ Softmax: å½’ä¸€åŒ–æƒé‡ â†’ (B*N, P, num_bands)
        â†’ Scale: é€ Patch åŠ æƒæ±‚å’Œ â†’ (B*N, P, d_model)
    """
    
    def __init__(self, num_bands, d_model, reduction=4, kernel_size=3):
        """
        Args:
            num_bands: é¢‘æ®µæ•°é‡ (level + 1)
            d_model: embedding ç»´åº¦
            reduction: MLP ä¸­é—´å±‚çš„é™ç»´æ¯”ä¾‹
            kernel_size: 1D å·ç§¯æ ¸å¤§å°ï¼Œæ§åˆ¶å±€éƒ¨ä¸Šä¸‹æ–‡èŒƒå›´
        """
        super(FrequencyChannelAttentionV2, self).__init__()
        
        self.num_bands = num_bands
        self.d_model = d_model
        self.kernel_size = kernel_size
        
        # éšè—å±‚ç»´åº¦
        hidden_dim = max(d_model // reduction, 8)
        
        # ========== å±€éƒ¨ä¸Šä¸‹æ–‡èšåˆ (æ›¿ä»£ GAP) ==========
        # ä½¿ç”¨ Depthwise 1D Conv åœ¨ Patch ç»´åº¦ä¸Šèšåˆå±€éƒ¨ä¿¡æ¯
        # è¾“å…¥: (B*N * num_bands, d_model, P)
        # è¾“å‡º: (B*N * num_bands, d_model, P)  -- ä¿ç•™æ—¶é—´ç»´åº¦
        self.local_context = nn.Sequential(
            nn.Conv1d(
                in_channels=d_model,
                out_channels=d_model,
                kernel_size=kernel_size,
                padding=kernel_size // 2,  # same padding
                groups=d_model,  # Depthwise: æ¯ä¸ªé€šé“ç‹¬ç«‹å·ç§¯ï¼Œå‚æ•°é‡å°
                bias=False
            ),
            nn.BatchNorm1d(d_model),
            nn.ReLU(inplace=True)
        )
        
        # ========== Excitation ç½‘ç»œ ==========
        # è¾“å…¥: (B*N, P, num_bands, d_model)
        # è¾“å‡º: (B*N, P, num_bands, 1)
        self.excitation = nn.Sequential(
            nn.Linear(d_model, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        )
        
        # åˆå§‹åŒ–: è®©åˆå§‹æƒé‡ç›¸å¯¹å‡åŒ€
        with torch.no_grad():
            last_linear = self.excitation[-1]
            nn.init.zeros_(last_linear.weight)
            nn.init.zeros_(last_linear.bias)
        
        # æ‰“å°é…ç½®
        self._print_config()
    
    def _print_config(self):
        """æ‰“å°æ¨¡å—é…ç½®"""
        print("=" * 70)
        print("[FrequencyChannelAttentionV2] Patch-wise é¢‘ç‡é€šé“æ³¨æ„åŠ›å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ é¢‘æ®µæ•°é‡: {self.num_bands}")
        print(f"  â”œâ”€ Embedding ç»´åº¦: {self.d_model}")
        print(f"  â”œâ”€ å·ç§¯æ ¸å¤§å°: {self.kernel_size} (å±€éƒ¨ä¸Šä¸‹æ–‡èŒƒå›´)")
        print(f"  â”œâ”€ èšåˆæ–¹å¼: Depthwise 1D Conv (æ›¿ä»£ GAP)")
        print(f"  â””â”€ è¾“å‡ºæƒé‡: Patch-wise (æ¯ä¸ªæ—¶é—´æ­¥ç‹¬ç«‹æƒé‡)")
        print("=" * 70)
    
    def forward(self, band_embeddings):
        """
        Args:
            band_embeddings: list of tensors, æ¯ä¸ªå½¢çŠ¶ (B*N, num_patches, d_model)
                             é¡ºåº: [e_cA, e_cD_n, e_cD_{n-1}, ..., e_cD_1]
        
        Returns:
            output: åŠ æƒèåˆåçš„ embedding, å½¢çŠ¶ (B*N, num_patches, d_model)
            attention_weights: æ³¨æ„åŠ›æƒé‡, å½¢çŠ¶ (B*N, num_patches, num_bands), ç”¨äºå¯è§†åŒ–/è°ƒè¯•
        """
        # Stack: list of (B*N, P, d_model) -> (B*N, P, d_model, num_bands)
        stacked = torch.stack(band_embeddings, dim=-1)
        B_N, P, D, num_bands = stacked.shape
        
        # ========== Step 1: å±€éƒ¨ä¸Šä¸‹æ–‡èšåˆ (æ›¿ä»£ GAP) ==========
        # å¯¹æ¯ä¸ªé¢‘æ®µç‹¬ç«‹åº”ç”¨ 1D Conv
        # stacked: (B*N, P, d_model, num_bands) -> (B*N, num_bands, d_model, P)
        x = stacked.permute(0, 3, 2, 1).contiguous()
        
        # Reshape ä»¥ä¾¿æ‰¹é‡å¤„ç†æ‰€æœ‰é¢‘æ®µ
        # (B*N, num_bands, d_model, P) -> (B*N * num_bands, d_model, P)
        x = x.view(B_N * num_bands, D, P)
        
        # 1D Conv: åœ¨ Patch ç»´åº¦ä¸Šèšåˆå±€éƒ¨ä¸Šä¸‹æ–‡
        # (B*N * num_bands, d_model, P) -> (B*N * num_bands, d_model, P)
        x = self.local_context(x)
        
        # Reshape å›æ¥
        # (B*N * num_bands, d_model, P) -> (B*N, num_bands, d_model, P)
        x = x.view(B_N, num_bands, D, P)
        
        # Permute: (B*N, num_bands, d_model, P) -> (B*N, P, num_bands, d_model)
        x = x.permute(0, 3, 1, 2).contiguous()
        
        # ========== Step 2: Excitation (è®¡ç®—æ¯ä¸ª Patch çš„é¢‘ç‡æƒé‡) ==========
        # (B*N, P, num_bands, d_model) -> (B*N, P, num_bands, 1)
        scores = self.excitation(x)
        
        # ========== Step 3: Softmax å½’ä¸€åŒ– ==========
        # åœ¨ num_bands ç»´åº¦ä¸Šå½’ä¸€åŒ–ï¼Œç¡®ä¿æ¯ä¸ª Patch çš„é¢‘ç‡æƒé‡å’Œä¸º 1
        # (B*N, P, num_bands, 1)
        attention_weights = F.softmax(scores, dim=2)
        
        # ========== Step 4: Scale (é€ Patch åŠ æƒæ±‚å’Œ) ==========
        # stacked: (B*N, P, d_model, num_bands)
        # attention_weights: (B*N, P, num_bands, 1) -> (B*N, P, 1, num_bands)
        weights_expanded = attention_weights.permute(0, 1, 3, 2)  # (B*N, P, 1, num_bands)
        
        # åŠ æƒæ±‚å’Œ: (B*N, P, d_model, num_bands) * (B*N, P, 1, num_bands) -> sum -> (B*N, P, d_model)
        output = (stacked * weights_expanded).sum(dim=-1)
        
        # è¿”å›èåˆç»“æœå’Œæ³¨æ„åŠ›æƒé‡ (ç”¨äºè°ƒè¯•)
        # attention_weights: (B*N, P, num_bands, 1) -> (B*N, P, num_bands)
        return output, attention_weights.squeeze(-1)


class FrequencyChannelAttentionV3(nn.Module):
    """
    é¢‘ç‡é€šé“æ³¨æ„åŠ›æ¨¡å— V3 (Global-Local Fusion / åŒæµæœºåˆ¶)
    
    æ ¸å¿ƒæ€æƒ³: Base + Residual (åŸºå‡† + æ®‹å·®)
    - Global Stream: GAP -> MLP -> å…¨å±€å…±äº«æƒé‡ (ç¨³å®šçš„å…ˆéªŒ)
    - Local Stream: 1D Conv -> MLP -> Patch-wise åŠ¨æ€æƒé‡ (å±€éƒ¨å¾®è°ƒ)
    - èåˆ: å¯å­¦ä¹ çš„åŠ æƒæ±‚å’Œï¼Œè®©æ¨¡å‹è‡ªåŠ¨å¹³è¡¡å…¨å±€ä¸å±€éƒ¨çš„é‡è¦æ€§
    
    ä¼˜åŠ¿:
    1. æŠ—è¿‡æ‹Ÿåˆ: Global åˆ†æ”¯é™åˆ¶æƒé‡è‡ªç”±åº¦ï¼Œé˜²æ­¢ Local åˆ†æ”¯å¯¹å™ªå£°è¿‡åº¦ååº”
    2. é²æ£’æ€§: åœ¨å¹³ç¨³æ®µé€€åŒ–ä¸º V1ï¼Œåœ¨çªå˜æ®µå‘æŒ¥ V2 çš„ä¼˜åŠ¿
    3. æ®‹å·®å­¦ä¹ : Local åªéœ€å­¦ä¹ å¯¹ Global çš„ä¿®æ­£ï¼Œå­¦ä¹ éš¾åº¦é™ä½
    
    æµç¨‹:
    è¾“å…¥: [e_band_0, e_band_1, ..., e_band_n]  å„é¢‘æ®µ embedding
        â†’ Global Stream: GAP -> MLP -> W_global (B*N, 1, num_bands)
        â†’ Local Stream: 1D Conv -> MLP -> W_local (B*N, P, num_bands)
        â†’ Fusion: Î± * W_global + (1-Î±) * W_local (Î± å¯å­¦ä¹ )
        â†’ Softmax -> Scale -> è¾“å‡º (B*N, P, d_model)
    """
    
    def __init__(self, num_bands, d_model, reduction=4, kernel_size=3):
        """
        Args:
            num_bands: é¢‘æ®µæ•°é‡ (level + 1)
            d_model: embedding ç»´åº¦
            reduction: MLP ä¸­é—´å±‚çš„é™ç»´æ¯”ä¾‹
            kernel_size: Local åˆ†æ”¯çš„ 1D å·ç§¯æ ¸å¤§å°
        """
        super(FrequencyChannelAttentionV3, self).__init__()
        
        self.num_bands = num_bands
        self.d_model = d_model
        self.kernel_size = kernel_size
        
        # éšè—å±‚ç»´åº¦
        hidden_dim = max(d_model // reduction, 8)
        
        # ========== Global Stream (å…¨å±€åˆ†æ”¯) ==========
        # GAP + MLP: æå–å…¨å±€é¢‘ç‡ç‰¹å¾
        self.global_excitation = nn.Sequential(
            nn.Linear(d_model, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        )
        
        # ========== Local Stream (å±€éƒ¨åˆ†æ”¯) ==========
        # Depthwise 1D Conv: èšåˆå±€éƒ¨ä¸Šä¸‹æ–‡
        self.local_context = nn.Sequential(
            nn.Conv1d(
                in_channels=d_model,
                out_channels=d_model,
                kernel_size=kernel_size,
                padding=kernel_size // 2,
                groups=d_model,
                bias=False
            ),
            nn.BatchNorm1d(d_model),
            nn.ReLU(inplace=True)
        )
        
        # Local MLP: è®¡ç®—å±€éƒ¨é¢‘ç‡æƒé‡
        self.local_excitation = nn.Sequential(
            nn.Linear(d_model, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        )
        
        # ========== Fusion (å¯å­¦ä¹ çš„èåˆæƒé‡) ==========
        # alpha: æ§åˆ¶ Global vs Local çš„å¹³è¡¡
        # åˆå§‹åŒ–ä¸º 0.5ï¼Œè®©æ¨¡å‹ä»å‡è¡¡çŠ¶æ€å¼€å§‹å­¦ä¹ 
        self.alpha = nn.Parameter(torch.tensor(0.5))
        
        # åˆå§‹åŒ–: è®©åˆå§‹æƒé‡ç›¸å¯¹å‡åŒ€
        self._init_weights()
        
        # æ‰“å°é…ç½®
        self._print_config()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        with torch.no_grad():
            # Global MLP åˆå§‹åŒ–
            nn.init.zeros_(self.global_excitation[-1].weight)
            nn.init.zeros_(self.global_excitation[-1].bias)
            # Local MLP åˆå§‹åŒ–
            nn.init.zeros_(self.local_excitation[-1].weight)
            nn.init.zeros_(self.local_excitation[-1].bias)
    
    def _print_config(self):
        """æ‰“å°æ¨¡å—é…ç½®"""
        print("=" * 70)
        print("[FrequencyChannelAttentionV3] Global-Local åŒæµèåˆæœºåˆ¶å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ é¢‘æ®µæ•°é‡: {self.num_bands}")
        print(f"  â”œâ”€ Embedding ç»´åº¦: {self.d_model}")
        print(f"  â”œâ”€ Local å·ç§¯æ ¸å¤§å°: {self.kernel_size}")
        print(f"  â”œâ”€ Global Stream: GAP + MLP (Instance-wise åŸºå‡†æƒé‡)")
        print(f"  â”œâ”€ Local Stream: 1D Conv + MLP (Patch-wise å¾®è°ƒæƒé‡)")
        print(f"  â”œâ”€ èåˆæ–¹å¼: Î± * W_global + (1-Î±) * W_local")
        print(f"  â””â”€ åˆå§‹ Î±: {self.alpha.item():.2f} (å¯å­¦ä¹ )")
        print("=" * 70)
    
    def forward(self, band_embeddings):
        """
        Args:
            band_embeddings: list of tensors, æ¯ä¸ªå½¢çŠ¶ (B*N, num_patches, d_model)
        
        Returns:
            output: åŠ æƒèåˆåçš„ embedding, å½¢çŠ¶ (B*N, num_patches, d_model)
            attention_weights: æ³¨æ„åŠ›æƒé‡, å½¢çŠ¶ (B*N, num_patches, num_bands)
            fusion_info: dict, åŒ…å« alpha, global_weights, local_weights ç”¨äºè°ƒè¯•
        """
        # Stack: list of (B*N, P, d_model) -> (B*N, P, d_model, num_bands)
        stacked = torch.stack(band_embeddings, dim=-1)
        B_N, P, D, num_bands = stacked.shape
        
        # ========== Global Stream ==========
        # GAP: (B*N, P, d_model, num_bands) -> (B*N, d_model, num_bands)
        global_feat = stacked.mean(dim=1)
        
        # Permute for MLP: (B*N, d_model, num_bands) -> (B*N, num_bands, d_model)
        global_feat = global_feat.permute(0, 2, 1)
        
        # Global MLP: (B*N, num_bands, d_model) -> (B*N, num_bands, 1)
        global_scores = self.global_excitation(global_feat)
        
        # Expand to patch dimension: (B*N, num_bands, 1) -> (B*N, P, num_bands, 1)
        global_scores = global_scores.unsqueeze(1).expand(-1, P, -1, -1)
        
        # ========== Local Stream ==========
        # Permute: (B*N, P, d_model, num_bands) -> (B*N, num_bands, d_model, P)
        x = stacked.permute(0, 3, 2, 1).contiguous()
        
        # Reshape: (B*N, num_bands, d_model, P) -> (B*N * num_bands, d_model, P)
        x = x.view(B_N * num_bands, D, P)
        
        # 1D Conv: (B*N * num_bands, d_model, P) -> (B*N * num_bands, d_model, P)
        x = self.local_context(x)
        
        # Reshape back: (B*N * num_bands, d_model, P) -> (B*N, num_bands, d_model, P)
        x = x.view(B_N, num_bands, D, P)
        
        # Permute: (B*N, num_bands, d_model, P) -> (B*N, P, num_bands, d_model)
        x = x.permute(0, 3, 1, 2).contiguous()
        
        # Local MLP: (B*N, P, num_bands, d_model) -> (B*N, P, num_bands, 1)
        local_scores = self.local_excitation(x)
        
        # ========== Fusion (å¯å­¦ä¹ åŠ æƒ) ==========
        # alpha é™åˆ¶åœ¨ [0, 1] èŒƒå›´å†…
        alpha = torch.sigmoid(self.alpha)
        
        # åŠ æƒèåˆ: Î± * global + (1-Î±) * local
        # (B*N, P, num_bands, 1)
        fused_scores = alpha * global_scores + (1 - alpha) * local_scores
        
        # ========== Softmax å½’ä¸€åŒ– ==========
        attention_weights = F.softmax(fused_scores, dim=2)
        
        # ========== Scale (é€ Patch åŠ æƒæ±‚å’Œ) ==========
        # stacked: (B*N, P, d_model, num_bands)
        # attention_weights: (B*N, P, num_bands, 1) -> (B*N, P, 1, num_bands)
        weights_expanded = attention_weights.permute(0, 1, 3, 2)
        
        # åŠ æƒæ±‚å’Œ: (B*N, P, d_model, num_bands) * (B*N, P, 1, num_bands) -> (B*N, P, d_model)
        output = (stacked * weights_expanded).sum(dim=-1)
        
        # æ„å»ºè°ƒè¯•ä¿¡æ¯
        fusion_info = {
            'alpha': alpha.item(),
            'global_weights': F.softmax(global_scores, dim=2).squeeze(-1),  # (B*N, P, num_bands)
            'local_weights': F.softmax(local_scores, dim=2).squeeze(-1),   # (B*N, P, num_bands)
        }
        
        return output, attention_weights.squeeze(-1), fusion_info


class WaveletPatchEmbedding(nn.Module):
    """
    å¤šåˆ†è¾¨ç‡ Patch Embeddingï¼šåŸºäº Haar å°æ³¢åˆ†è§£
    å°†æ¯ä¸ª Patch åˆ†è§£ä¸ºä½é¢‘ï¼ˆè¶‹åŠ¿ï¼‰å’Œé«˜é¢‘ï¼ˆç»†èŠ‚ï¼‰åˆ†é‡ï¼Œ
    åˆ†åˆ«æŠ•å½±åé€šè¿‡é—¨æ§æœºåˆ¶èåˆï¼Œä¿ç•™æ˜¾å¼çš„é¢‘åŸŸä¿¡æ¯ã€‚
    """

    def __init__(self, d_model, patch_len, stride, dropout, use_soft_threshold=False, 
                 use_positional_encoding=False, pos_encoding_max_len=5000):
        super(WaveletPatchEmbedding, self).__init__()
        # Patching å‚æ•°
        self.patch_len = patch_len
        self.stride = stride
        self.d_model = d_model
        self.padding_patch_layer = ReplicationPad1d((0, stride))

        # ç¡®ä¿ patch_len æ˜¯å¶æ•°ï¼ˆHaar å°æ³¢è¦æ±‚ï¼‰
        assert patch_len % 2 == 0, f"patch_len must be even for Haar DWT, got {patch_len}"

        # å°æ³¢åˆ†è§£åçš„é•¿åº¦
        self.half_len = patch_len // 2

        # åŒé€šé“ç‹¬ç«‹æŠ•å½±ï¼šä½é¢‘å’Œé«˜é¢‘å„è‡ªæœ‰ä¸“å±çš„ Embedding å±‚
        self.approx_embedding = TokenEmbedding(self.half_len, d_model)  # ä½é¢‘/è¶‹åŠ¿
        self.detail_embedding = TokenEmbedding(self.half_len, d_model)  # é«˜é¢‘/ç»†èŠ‚

        # é—¨æ§èåˆæœºåˆ¶ï¼šå­¦ä¹ å¦‚ä½•åŠ¨æ€åŠ æƒä½é¢‘å’Œé«˜é¢‘ç‰¹å¾
        self.gate = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.Sigmoid()
        )

        # åˆå§‹åŒ– Gate æƒé‡ï¼šåå‘ä½é¢‘ï¼ˆé˜²æ­¢é«˜é¢‘è¿‡æ‹Ÿåˆï¼‰
        # bias=2.0 -> Sigmoid(2.0) â‰ˆ 0.88
        # åˆå§‹èåˆ = 88% ä½é¢‘ (Trend) + 12% é«˜é¢‘ (Detail)
        for m in self.gate.modules():
            if isinstance(m, nn.Linear):
                nn.init.constant_(m.weight, 0)
                nn.init.constant_(m.bias, 2.0)

        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # ã€ä¼˜åŒ–1ã€‘é«˜é¢‘é€šé“ä¸“ç”¨ Dropoutï¼šé˜²æ­¢å¯¹å™ªå£°è¿‡æ‹Ÿåˆ
        # æ¯”å¸¸è§„ dropout æ›´å¼º (0.5)ï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¹ é«˜é¢‘çš„ç»Ÿè®¡åˆ†å¸ƒè€Œéå…·ä½“å™ªå£°
        self.detail_dropout = nn.Dropout(0.5)
        
        # ã€ä¼˜åŒ–2ã€‘å¯å­¦ä¹ è½¯é˜ˆå€¼å»å™ªï¼šæ™ºèƒ½è¿‡æ»¤é«˜é¢‘å™ªå£°
        self.use_soft_threshold = use_soft_threshold
        if use_soft_threshold:
            # å¯¹å°æ³¢åŸŸçš„é«˜é¢‘åˆ†é‡åº”ç”¨è½¯é˜ˆå€¼
            # init_tau=0.1 æ˜¯åˆå§‹é˜ˆå€¼ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨å­¦ä¹ æœ€ä½³å€¼
            self.soft_threshold = SoftThreshold(num_features=self.half_len, init_tau=0.1)
        
        # ä½ç½®ç¼–ç  (å¯é€‰)
        self.use_positional_encoding = use_positional_encoding
        self.pos_encoding_max_len = pos_encoding_max_len
        if use_positional_encoding:
            self.position_embedding = PositionalEmbedding(d_model, max_len=pos_encoding_max_len)
        else:
            self.position_embedding = None

        # æ‰“å°é…ç½®æ—¥å¿—
        self._print_config()

    def _print_config(self):
        """æ‰“å°å½“å‰æ¨¡å—çš„é…ç½®ä¿¡æ¯"""
        print("=" * 60)
        print("[WaveletPatchEmbedding] å°æ³¢å¤šåˆ†è¾¨ç‡ Patch Embedding å·²å¯ç”¨")
        print("=" * 60)
        print(f"  â”œâ”€ Patch é•¿åº¦: {self.patch_len}")
        print(f"  â”œâ”€ Stride: {self.stride}")
        print(f"  â”œâ”€ è¾“å‡ºç»´åº¦: {self.d_model}")
        print(f"  â”œâ”€ å°æ³¢åˆ†è§£: Haar DWT (å•çº§)")
        print(f"  â”œâ”€ ä½é¢‘åˆ†é‡é•¿åº¦: {self.half_len}")
        print(f"  â”œâ”€ é«˜é¢‘åˆ†é‡é•¿åº¦: {self.half_len}")
        print(f"  â”œâ”€ é—¨æ§åˆå§‹åŒ–: åå‘ä½é¢‘ (Trend ~88%, Detail ~12%)")
        print(f"  â”œâ”€ é«˜é¢‘ Dropout: p=0.5 (é˜²è¿‡æ‹Ÿåˆ)")
        if self.use_soft_threshold:
            print(f"  â”œâ”€ è½¯é˜ˆå€¼å»å™ª: âœ… å¯ç”¨ (å¯å­¦ä¹ é˜ˆå€¼)")
        else:
            print(f"  â”œâ”€ è½¯é˜ˆå€¼å»å™ª: âŒ å…³é—­")
        if self.use_positional_encoding:
            print(f"  â”œâ”€ ä½ç½®ç¼–ç : âœ… å¯ç”¨ (max_len={self.pos_encoding_max_len})")
        else:
            print(f"  â”œâ”€ ä½ç½®ç¼–ç : âŒ å…³é—­")
        print(f"  â””â”€ è¾“å‡º Dropout: p={self.dropout.p}")
        print("=" * 60)

    def haar_dwt_1d(self, x):
        """
        å¯¹æœ€åä¸€ä¸ªç»´åº¦æ‰§è¡Œå•çº§ Haar ç¦»æ•£å°æ³¢å˜æ¢ (DWT)
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (B, num_patches, patch_len)
        
        Returns:
            approx: è¿‘ä¼¼åˆ†é‡ï¼ˆä½é¢‘/è¶‹åŠ¿ï¼‰ï¼Œå½¢çŠ¶ (B, num_patches, patch_len//2)
            detail: ç»†èŠ‚åˆ†é‡ï¼ˆé«˜é¢‘/æ³¢åŠ¨ï¼‰ï¼Œå½¢çŠ¶ (B, num_patches, patch_len//2)
        """
        # Haar å°æ³¢ï¼šç›¸é‚»ç‚¹æ±‚å¹³å‡ â†’ ä½é¢‘è¿‘ä¼¼
        approx = (x[..., 0::2] + x[..., 1::2]) / math.sqrt(2)
        # Haar å°æ³¢ï¼šç›¸é‚»ç‚¹æ±‚å·® â†’ é«˜é¢‘ç»†èŠ‚
        detail = (x[..., 0::2] - x[..., 1::2]) / math.sqrt(2)
        return approx, detail

    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (B, N, T)ï¼Œå…¶ä¸­ N æ˜¯å˜é‡æ•°ï¼ŒT æ˜¯åºåˆ—é•¿åº¦
        
        Returns:
            output: èåˆåçš„ Patch Embeddingï¼Œå½¢çŠ¶ (B*N, num_patches, d_model)
            n_vars: å˜é‡æ•° N
        """
        # è®°å½•å˜é‡æ•°
        n_vars = x.shape[1]

        # Step 1: Padding å¹¶åˆ‡åˆ†ä¸º Patches
        x = self.padding_patch_layer(x)
        # unfold: (B, N, num_patches, patch_len)
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        # reshape: (B*N, num_patches, patch_len)
        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))

        # Step 2: Haar å°æ³¢åˆ†è§£
        # approx, detail: å„ä¸º (B*N, num_patches, patch_len//2)
        approx, detail = self.haar_dwt_1d(x)
        
        # ã€ä¼˜åŒ–2ã€‘å¯¹é«˜é¢‘åˆ†é‡åº”ç”¨å¯å­¦ä¹ è½¯é˜ˆå€¼å»å™ª
        if self.use_soft_threshold:
            detail = self.soft_threshold(detail)

        # Step 3: åŒé€šé“ç‹¬ç«‹æŠ•å½±
        # TokenEmbedding è¾“å…¥ (B, L, C)ï¼Œè¾“å‡º (B, L, d_model)
        # è¿™é‡Œ C = patch_len//2ï¼ŒL = num_patches
        e_approx = self.approx_embedding(approx)  # (B*N, num_patches, d_model)
        e_detail = self.detail_embedding(detail)  # (B*N, num_patches, d_model)
        
        # ã€ä¼˜åŒ–1ã€‘å¯¹é«˜é¢‘åˆ†é‡æ–½åŠ å¼º Dropoutï¼ŒæŠ‘åˆ¶å™ªå£°è¿‡æ‹Ÿåˆ
        e_detail = self.detail_dropout(e_detail)

        # Step 4: é—¨æ§èåˆ
        # æ‹¼æ¥ä½é¢‘å’Œé«˜é¢‘ embedding
        combined = torch.cat([e_approx, e_detail], dim=-1)  # (B*N, num_patches, d_model*2)
        # è®¡ç®—é—¨æ§æƒé‡ (0~1)ï¼Œå†³å®šä½é¢‘å’Œé«˜é¢‘çš„æ··åˆæ¯”ä¾‹
        gate_weight = self.gate(combined)  # (B*N, num_patches, d_model)
        # åŠ æƒèåˆï¼šgate * ä½é¢‘ + (1-gate) * é«˜é¢‘
        output = gate_weight * e_approx + (1 - gate_weight) * e_detail
        
        # Step 5: æ·»åŠ ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_positional_encoding:
            output = output + self.position_embedding(output).to(output.device)

        return self.dropout(output), n_vars


class DataEmbedding_wo_time(nn.Module):
    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):
        super(DataEmbedding_wo_time, self).__init__()

        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)
        self.position_embedding = PositionalEmbedding(d_model=d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        x = self.value_embedding(x) + self.position_embedding(x)
        return self.dropout(x)


class CausalConv1d(nn.Module):
    """
    å› æœå·ç§¯å±‚ï¼šåªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼Œä¸çœ‹æœªæ¥
    é€šè¿‡å·¦ä¾§å¡«å……å®ç°å› æœæ€§ï¼Œæ¢å¤ Patch é—´çš„å±€éƒ¨è¿é€šæ€§
    """
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(CausalConv1d, self).__init__()
        self.kernel_size = kernel_size
        self.padding = kernel_size - 1  # å·¦ä¾§å¡«å……é‡
        
        # æ ‡å‡† Conv1dï¼Œä¸ä½¿ç”¨å†…ç½® padding
        self.conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            padding=0,  # æˆ‘ä»¬æ‰‹åŠ¨åšå› æœå¡«å……
            bias=False
        )
        
        # Kaiming åˆå§‹åŒ–
        nn.init.kaiming_normal_(self.conv.weight, mode='fan_in', nonlinearity='leaky_relu')
    
    def forward(self, x):
        """
        Args:
            x: (B, num_patches, patch_len) - Patch åºåˆ—
        Returns:
            out: (B, num_patches, out_channels)
        """
        # x: (B, L, C) -> (B, C, L) for Conv1d
        x = x.transpose(1, 2)
        
        # å› æœå¡«å……ï¼šåªåœ¨å·¦ä¾§å¡«å……ï¼Œä¸å¡«å……å³ä¾§
        # è¿™æ ·å·ç§¯æ ¸åªèƒ½çœ‹åˆ°å½“å‰å’Œè¿‡å»çš„ Patch
        x = F.pad(x, (self.padding, 0))  # (left_pad, right_pad)
        
        # ç¡®ä¿è¾“å…¥ç±»å‹ä¸æƒé‡ç±»å‹åŒ¹é…ï¼ˆå¤„ç†æ··åˆç²¾åº¦è®­ç»ƒï¼‰
        if x.dtype != self.conv.weight.dtype:
            x = x.to(self.conv.weight.dtype)
        
        # å·ç§¯
        x = self.conv(x)
        
        # (B, C, L) -> (B, L, C)
        x = x.transpose(1, 2)
        return x


class WISTPatchEmbedding(nn.Module):
    """
    WIST-PE: Wavelet-Informed Spatio-Temporal Patch Embedding
    
    æ ¸å¿ƒåˆ›æ–°ç‚¹:
    1. å…¨å±€å› æœå°æ³¢åˆ†è§£ (Global Causal DWT): åœ¨ Patching ä¹‹å‰å…ˆåšå…¨å±€ db4 åˆ†è§£
    2. åŒé€šé“å·®å¼‚åŒ–å¤„ç†: ä½é¢‘ç›´æ¥æŠ•å½±ï¼Œé«˜é¢‘ç»è¿‡è½¯é˜ˆå€¼å»å™ª+å¼ºDropout
    3. é—¨æ§èåˆ (Gated Fusion): åç½®åˆå§‹åŒ–ä½¿æ¨¡å‹åˆæœŸå…³æ³¨ä½é¢‘è¶‹åŠ¿ (88%/12%)
    4. ä¸¥æ ¼å› æœæ€§: ä½¿ç”¨ CausalSWTï¼Œä»…å·¦ä¾§å¡«å……ï¼Œé˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²
    5. å› æœå·ç§¯æŠ•å½± (å¯é€‰): æ¢å¤ Patch é—´çš„å±€éƒ¨è¿é€šæ€§ï¼ŒåŒæ—¶ä¿æŒå› æœæ€§
    6. åˆ†å±‚é‡‘å­—å¡”èåˆ (Pyramid Fusion): æ”¯æŒå¤šçº§å°æ³¢åˆ†è§£ï¼Œä»é«˜é¢‘åˆ°ä½é¢‘é€çº§èåˆ
    
    æµç¨‹ (level=1, åŒé€šé“æ¨¡å¼):
    è¾“å…¥ (B, N, T) 
        â†’ å…¨å±€å› æœå°æ³¢åˆ†è§£ â†’ [ä½é¢‘ Trend, é«˜é¢‘ Detail]
        â†’ åˆ†åˆ«åˆ‡åˆ† Patch
        â†’ å·®å¼‚åŒ–æŠ•å½± (ä½é¢‘ç›´æŠ•, é«˜é¢‘å»å™ª+æŠ•å½±+Dropout)
        â†’ é—¨æ§èåˆ
        â†’ è¾“å‡º (B*N, num_patches, d_model)
    
    æµç¨‹ (level>=2, é‡‘å­—å¡”èåˆæ¨¡å¼):
    è¾“å…¥ (B, N, T)
        â†’ å…¨å±€å› æœå°æ³¢åˆ†è§£ â†’ [cA_n, cD_n, cD_{n-1}, ..., cD_1]
        â†’ åˆ†åˆ«åˆ‡åˆ† Patch å¹¶æŠ•å½±
        â†’ åˆ†å±‚é‡‘å­—å¡”èåˆ: cD_1 + cD_2 â†’ D_fused â†’ D_fused + cA â†’ æœ€ç»ˆè¾“å‡º
        â†’ è¾“å‡º (B*N, num_patches, d_model)
    """
    
    def __init__(self, d_model, patch_len, stride, dropout,
                 wavelet_type='db4', wavelet_level=1,
                 hf_dropout=0.5, gate_bias_init=2.0,
                 use_soft_threshold=True, use_causal_conv=True,
                 pyramid_fusion=True, mf_dropout=0.3,
                 use_freq_attention=False, freq_attention_version=1,
                 freq_attn_kernel_size=3,
                 use_freq_embedding=False, freq_embed_init_method='random',
                 use_positional_encoding=False, pos_encoding_max_len=5000,
                 use_hf_freq_attention=True,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨é¢‘ç‡æ³¨æ„åŠ›è¿›è¡Œé«˜é¢‘èåˆï¼ˆç”¨äºforward_separatedï¼‰
                 configs=None):
        super(WISTPatchEmbedding, self).__init__()
        
        # åŸºç¡€å‚æ•°
        self.d_model = d_model
        self.patch_len = patch_len
        self.stride = stride
        self.wavelet_type = wavelet_type
        self.wavelet_level = wavelet_level
        self.use_causal_conv = use_causal_conv
        self.pyramid_fusion = pyramid_fusion and (wavelet_level >= 2)  # åªæœ‰ level>=2 æ‰å¯ç”¨é‡‘å­—å¡”èåˆ
        self.use_freq_attention = use_freq_attention  # æ˜¯å¦ä½¿ç”¨é¢‘ç‡é€šé“æ³¨æ„åŠ›æ›¿ä»£é—¨æ§èåˆ
        self.freq_attention_version = freq_attention_version  # 1=GAPç‰ˆæœ¬, 2=1D Convç‰ˆæœ¬ (Patch-wise)
        self.freq_attn_kernel_size = freq_attn_kernel_size  # V2ç‰ˆæœ¬çš„å·ç§¯æ ¸å¤§å°
        
        # ğŸ†• Frequency Embedding æ”¯æŒ
        self.use_freq_embedding = use_freq_embedding
        self.freq_embed_init_method = freq_embed_init_method
        
        # ğŸ†• é«˜é¢‘èåˆé¢‘ç‡æ³¨æ„åŠ›æ”¯æŒï¼ˆç”¨äºforward_separatedï¼‰
        self.use_hf_freq_attention = use_hf_freq_attention
        
        # å¯¼å…¥å› æœå°æ³¢å˜æ¢æ¨¡å—
        from layers.CausalWavelet import CausalSWT
        self.swt = CausalSWT(wavelet=wavelet_type, level=wavelet_level)
        
        # Patching å±‚
        self.padding_patch_layer = ReplicationPad1d((0, stride))
        
        # ========== é¢‘æ®µæŠ•å½±å±‚ ==========
        # é¢‘æ®µæ•°é‡: level + 1 (1ä¸ªä½é¢‘ cA + levelä¸ªé«˜é¢‘ cD)
        self.num_bands = wavelet_level + 1
        
        # ğŸ†• é¢‘æ®µ Embedding å±‚ï¼ˆå¯é€‰ï¼‰
        if self.use_freq_embedding:
            self.freq_embedding = FrequencyEmbedding(
                d_model=d_model,
                num_frequencies=self.num_bands,
                init_method=self.freq_embed_init_method
            )
        else:
            self.freq_embedding = None
        
        if self.pyramid_fusion:
            # é‡‘å­—å¡”èåˆæ¨¡å¼: ä¸ºæ¯ä¸ªé¢‘æ®µåˆ›å»ºç‹¬ç«‹çš„æŠ•å½±å±‚
            # band_embeddings[0] = cA (æœ€ä½é¢‘/è¶‹åŠ¿)
            # band_embeddings[1] = cD_n (æœ€é«˜å±‚ç»†èŠ‚ï¼Œç›¸å¯¹è¾ƒä½é¢‘)
            # band_embeddings[2] = cD_{n-1}
            # ...
            # band_embeddings[n] = cD_1 (æœ€é«˜é¢‘ç»†èŠ‚)
            self.band_embeddings = nn.ModuleList()
            for i in range(self.num_bands):
                if use_causal_conv:
                    self.band_embeddings.append(CausalConv1d(patch_len, d_model, kernel_size=3))
                else:
                    proj = nn.Linear(patch_len, d_model)
                    nn.init.kaiming_normal_(proj.weight, mode='fan_in', nonlinearity='leaky_relu')
                    self.band_embeddings.append(proj)
            
            # æ¯ä¸ªé«˜é¢‘é¢‘æ®µçš„ Dropout (ä» cD_n åˆ° cD_1ï¼ŒDropout é€æ¸å¢å¼º)
            # cD_n (ä¸­é¢‘): mf_dropout, cD_1 (æœ€é«˜é¢‘): hf_dropout
            self.band_dropouts = nn.ModuleList()
            self.band_dropouts.append(nn.Identity())  # cA ä¸åš Dropout
            for i in range(1, self.num_bands):
                # çº¿æ€§æ’å€¼: ä» mf_dropout åˆ° hf_dropout
                if wavelet_level > 1:
                    ratio = (i - 1) / (wavelet_level - 1)  # 0 åˆ° 1
                    drop_rate = mf_dropout + ratio * (hf_dropout - mf_dropout)
                else:
                    drop_rate = hf_dropout
                self.band_dropouts.append(nn.Dropout(drop_rate))
            
            # æ¯ä¸ªé«˜é¢‘é¢‘æ®µçš„è½¯é˜ˆå€¼å»å™ª (å¯é€‰)
            self.use_soft_threshold = use_soft_threshold
            if use_soft_threshold:
                self.band_thresholds = nn.ModuleList()
                self.band_thresholds.append(nn.Identity())  # cA ä¸åšå»å™ª
                for i in range(1, self.num_bands):
                    # é«˜é¢‘é¢‘æ®µçš„åˆå§‹é˜ˆå€¼æ›´å¤§
                    if wavelet_level > 1:
                        ratio = (i - 1) / (wavelet_level - 1)
                        init_tau = 0.05 + ratio * 0.1  # ä» 0.05 åˆ° 0.15
                    else:
                        init_tau = 0.1
                    self.band_thresholds.append(SoftThreshold(num_features=patch_len, init_tau=init_tau))
            
            # ========== èåˆæœºåˆ¶é€‰æ‹© ==========
            if use_freq_attention:
                # ä½¿ç”¨é¢‘ç‡é€šé“æ³¨æ„åŠ›æ›¿ä»£ç¡¬ç¼–ç çš„é—¨æ§èåˆ
                if freq_attention_version == 3:
                    # V3: Global-Local åŒæµèåˆæœºåˆ¶
                    self.freq_attention = FrequencyChannelAttentionV3(
                        num_bands=self.num_bands,
                        d_model=d_model,
                        reduction=4,
                        kernel_size=freq_attn_kernel_size
                    )
                elif freq_attention_version == 2:
                    # V2: ä½¿ç”¨ 1D Conv æ›¿ä»£ GAPï¼Œå®ç° Patch-wise åŠ¨æ€è·¯ç”±
                    self.freq_attention = FrequencyChannelAttentionV2(
                        num_bands=self.num_bands,
                        d_model=d_model,
                        reduction=4,
                        kernel_size=freq_attn_kernel_size
                    )
                else:
                    # V1: ä½¿ç”¨ GAPï¼Œå®ç° Instance-wise åŠ¨æ€è·¯ç”±
                    self.freq_attention = FrequencyChannelAttention(
                        num_bands=self.num_bands,
                        d_model=d_model,
                        reduction=4
                    )
                self.gate_layers = None  # ä¸éœ€è¦é—¨æ§å±‚
            else:
                # é‡‘å­—å¡”èåˆé—¨æ§: ä»é«˜é¢‘åˆ°ä½é¢‘é€çº§èåˆ
                # éœ€è¦ (num_bands - 1) ä¸ªé—¨æ§å±‚
                # gate_layers[0]: cD_1 + cD_2 çš„èåˆ (å¦‚æœ level >= 2)
                # gate_layers[1]: (cD_1+cD_2) + cD_3 çš„èåˆ (å¦‚æœ level >= 3)
                # ...
                # gate_layers[-1]: æ‰€æœ‰ç»†èŠ‚ + cA çš„æœ€ç»ˆèåˆ
                self.gate_layers = nn.ModuleList()
                for i in range(self.num_bands - 1):
                    gate = nn.Sequential(
                        nn.Linear(d_model * 2, d_model),
                        nn.Sigmoid()
                    )
                    # åˆå§‹åŒ–é—¨æ§åç½®
                    # æœ€åä¸€ä¸ªé—¨æ§ (èåˆ cA) åå‘ä½é¢‘
                    # å…¶ä»–é—¨æ§ (èåˆç»†èŠ‚) ç›¸å¯¹å¹³è¡¡
                    if i == self.num_bands - 2:  # æœ€åä¸€ä¸ªé—¨æ§ï¼Œèåˆ cA
                        bias_init = gate_bias_init
                    else:  # ç»†èŠ‚ä¹‹é—´çš„èåˆï¼Œç›¸å¯¹å¹³è¡¡
                        bias_init = 0.5  # sigmoid(0.5) â‰ˆ 0.62
                    for m in gate.modules():
                        if isinstance(m, nn.Linear):
                            nn.init.constant_(m.weight, 0)
                            nn.init.constant_(m.bias, bias_init)
                    self.gate_layers.append(gate)
                self.freq_attention = None  # ä¸ä½¿ç”¨æ³¨æ„åŠ›
            
            # ========== é«˜é¢‘èåˆä¸“ç”¨é¢‘ç‡æ³¨æ„åŠ› (ç”¨äºforward_separated) ==========
            # é«˜é¢‘é¢‘æ®µæ•°é‡ = num_bands - 1 (ä¸åŒ…æ‹¬ä½é¢‘cA)
            num_high_freq_bands = self.num_bands - 1
            if num_high_freq_bands > 1 and use_hf_freq_attention:
                # ä½¿ç”¨é¢‘ç‡æ³¨æ„åŠ›V1ç‰ˆæœ¬è¿›è¡Œé«˜é¢‘å†…éƒ¨èåˆ
                self.hf_freq_attention = FrequencyChannelAttention(
                    num_bands=num_high_freq_bands,
                    d_model=d_model,
                    reduction=4
                )
            else:
                # ä¸ä½¿ç”¨é¢‘ç‡æ³¨æ„åŠ›ï¼Œå°†ä½¿ç”¨é—¨æ§èåˆï¼ˆåœ¨forward_separatedä¸­å¤„ç†ï¼‰
                self.hf_freq_attention = None
        else:
            # åŸå§‹åŒé€šé“æ¨¡å¼ (level=1 æˆ–ç¦ç”¨é‡‘å­—å¡”èåˆ)
            if use_causal_conv:
                self.low_freq_embedding = CausalConv1d(patch_len, d_model, kernel_size=3)
                self.high_freq_embedding = CausalConv1d(patch_len, d_model, kernel_size=3)
            else:
                self.low_freq_embedding = nn.Linear(patch_len, d_model)
                self.high_freq_embedding = nn.Linear(patch_len, d_model)
                nn.init.kaiming_normal_(self.low_freq_embedding.weight, mode='fan_in', nonlinearity='leaky_relu')
                nn.init.kaiming_normal_(self.high_freq_embedding.weight, mode='fan_in', nonlinearity='leaky_relu')
            
            self.hf_dropout = nn.Dropout(hf_dropout)
            
            self.use_soft_threshold = use_soft_threshold
            if use_soft_threshold:
                self.soft_threshold = SoftThreshold(num_features=patch_len, init_tau=0.1)
            
            # ========== èåˆæœºåˆ¶é€‰æ‹© ==========
            if use_freq_attention:
                # ä½¿ç”¨é¢‘ç‡é€šé“æ³¨æ„åŠ›æ›¿ä»£é—¨æ§èåˆ
                if freq_attention_version == 3:
                    # V3: Global-Local åŒæµèåˆæœºåˆ¶
                    self.freq_attention = FrequencyChannelAttentionV3(
                        num_bands=2,  # åŒé€šé“: ä½é¢‘ + é«˜é¢‘
                        d_model=d_model,
                        reduction=4,
                        kernel_size=freq_attn_kernel_size
                    )
                elif freq_attention_version == 2:
                    # V2: ä½¿ç”¨ 1D Conv æ›¿ä»£ GAPï¼Œå®ç° Patch-wise åŠ¨æ€è·¯ç”±
                    self.freq_attention = FrequencyChannelAttentionV2(
                        num_bands=2,  # åŒé€šé“: ä½é¢‘ + é«˜é¢‘
                        d_model=d_model,
                        reduction=4,
                        kernel_size=freq_attn_kernel_size
                    )
                else:
                    # V1: ä½¿ç”¨ GAPï¼Œå®ç° Instance-wise åŠ¨æ€è·¯ç”±
                    self.freq_attention = FrequencyChannelAttention(
                        num_bands=2,  # åŒé€šé“: ä½é¢‘ + é«˜é¢‘
                        d_model=d_model,
                        reduction=4
                    )
                self.gate = None
            else:
                self.gate = nn.Sequential(
                    nn.Linear(d_model * 2, d_model),
                    nn.Sigmoid()
                )
                for m in self.gate.modules():
                    if isinstance(m, nn.Linear):
                        nn.init.constant_(m.weight, 0)
                        nn.init.constant_(m.bias, gate_bias_init)
                self.freq_attention = None
            
            # åŒé€šé“æ¨¡å¼ä¸‹ï¼Œé«˜é¢‘åªæœ‰ä¸€ä¸ªé¢‘æ®µï¼Œä¸éœ€è¦èåˆæ³¨æ„åŠ›
            # ä½†ä¸ºäº†ä¿æŒå±æ€§ä¸€è‡´æ€§ï¼Œä»ç„¶è®¾ç½®ä¸ºNone
            self.hf_freq_attention = None
        
        # ä¿å­˜å‚æ•°ç”¨äºæ‰“å°
        self.gate_bias_init = gate_bias_init
        self.hf_dropout_rate = hf_dropout
        self.mf_dropout_rate = mf_dropout
        
        # ä½ç½®ç¼–ç  (å¯é€‰)
        self.use_positional_encoding = use_positional_encoding
        self.pos_encoding_max_len = pos_encoding_max_len
        if use_positional_encoding:
            self.position_embedding = PositionalEmbedding(d_model, max_len=pos_encoding_max_len)
        else:
            self.position_embedding = None
        
        # è¾“å‡º Dropout
        self.dropout = nn.Dropout(dropout)
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        self._print_config()
    
    def _print_config(self):
        """æ‰“å°æ¨¡å—é…ç½®"""
        print("=" * 70)
        print("[WIST-PE] Wavelet-Informed Spatio-Temporal Patch Embedding å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ å°æ³¢åŸºç±»å‹: {self.wavelet_type}")
        print(f"  â”œâ”€ åˆ†è§£å±‚æ•°: {self.wavelet_level}")
        print(f"  â”œâ”€ é¢‘æ®µæ•°é‡: {self.num_bands} (1ä¸ªä½é¢‘ + {self.wavelet_level}ä¸ªé«˜é¢‘)")
        print(f"  â”œâ”€ Patch é•¿åº¦: {self.patch_len}")
        print(f"  â”œâ”€ Stride: {self.stride}")
        print(f"  â”œâ”€ è¾“å‡ºç»´åº¦: {self.d_model}")
        
        # ğŸ†• é¢‘æ®µ Embedding ä¿¡æ¯
        if self.use_freq_embedding:
            print(f"  â”œâ”€ é¢‘æ®µ Embedding: âœ… å¯ç”¨ ({self.freq_embed_init_method} åˆå§‹åŒ–)")
            print(f"  â”‚   â””â”€ ä½œç”¨: æ˜¾å¼æ ‡è®°é¢‘æ®µèº«ä»½ï¼Œå¼•å¯¼ LLM Self-Attention")
        else:
            print(f"  â”œâ”€ é¢‘æ®µ Embedding: âŒ æœªå¯ç”¨")
        
        if self.use_causal_conv:
            print(f"  â”œâ”€ æŠ•å½±æ–¹å¼: âœ… å› æœå·ç§¯ (CausalConv1d, kernel=3)")
        else:
            print(f"  â”œâ”€ æŠ•å½±æ–¹å¼: Linear (æ—  Patch é—´äº¤äº’)")
        
        if self.pyramid_fusion:
            print(f"  â”œâ”€ èåˆæ¨¡å¼: âœ… åˆ†å±‚é‡‘å­—å¡”èåˆ (Pyramid Fusion)")
            print(f"  â”‚   â”œâ”€ ä¸­é¢‘ Dropout: p={self.mf_dropout_rate}")
            print(f"  â”‚   â”œâ”€ é«˜é¢‘ Dropout: p={self.hf_dropout_rate}")
            print(f"  â”‚   â””â”€ èåˆé¡ºåº: cD_1 â†’ cD_2 â†’ ... â†’ cD_n â†’ cA")
            
            # é«˜é¢‘èåˆæœºåˆ¶ä¿¡æ¯ï¼ˆç”¨äºforward_separatedï¼‰
            if hasattr(self, 'use_hf_freq_attention') and hasattr(self, 'hf_freq_attention'):
                if self.use_hf_freq_attention and self.hf_freq_attention is not None:
                    print(f"  â”œâ”€ é«˜é¢‘èåˆæœºåˆ¶: âœ… é¢‘ç‡æ³¨æ„åŠ›V1 (ç”¨äºforward_separated/CWPR)")
                    print(f"  â”‚   â””â”€ ä»…èåˆé«˜é¢‘é¢‘æ®µ [cD_n, ..., cD_1]ï¼Œä½é¢‘cAå•ç‹¬è¾“å‡º")
                else:
                    print(f"  â”œâ”€ é«˜é¢‘èåˆæœºåˆ¶: é—¨æ§èåˆ (ç”¨äºforward_separated/CWPR)")
                    print(f"  â”‚   â””â”€ ä»…èåˆé«˜é¢‘é¢‘æ®µ [cD_n, ..., cD_1]ï¼Œä½é¢‘cAå•ç‹¬è¾“å‡º")
        else:
            print(f"  â”œâ”€ èåˆæ¨¡å¼: åŒé€šé“èåˆ (Dual-Channel)")
            print(f"  â”œâ”€ é«˜é¢‘ Dropout: p={self.hf_dropout_rate}")
        
        # å…¨é¢‘æ®µèåˆæœºåˆ¶ï¼ˆç”¨äºforwardæ–¹æ³•ï¼Œå½“ä¸ä½¿ç”¨CWPRæ—¶ï¼‰
        if self.use_freq_attention:
            if self.freq_attention_version == 3:
                print(f"  â”œâ”€ å…¨é¢‘æ®µèåˆæœºåˆ¶: âœ… é¢‘ç‡é€šé“æ³¨æ„åŠ› V3 (Global-Local åŒæµèåˆ)")
                print(f"  â”‚   â”œâ”€ Global Stream: GAP + MLP (åŸºå‡†æƒé‡)")
                print(f"  â”‚   â”œâ”€ Local Stream: 1D Conv + MLP (å¾®è°ƒæƒé‡)")
                print(f"  â”‚   â””â”€ å·ç§¯æ ¸å¤§å°: {self.freq_attn_kernel_size}")
            elif self.freq_attention_version == 2:
                print(f"  â”œâ”€ å…¨é¢‘æ®µèåˆæœºåˆ¶: âœ… é¢‘ç‡é€šé“æ³¨æ„åŠ› V2 (1D Conv, Patch-wise åŠ¨æ€è·¯ç”±)")
                print(f"  â”‚   â””â”€ å·ç§¯æ ¸å¤§å°: {self.freq_attn_kernel_size}")
            else:
                print(f"  â”œâ”€ å…¨é¢‘æ®µèåˆæœºåˆ¶: âœ… é¢‘ç‡é€šé“æ³¨æ„åŠ› V1 (GAP, Instance-wise åŠ¨æ€è·¯ç”±)")
        else:
            print(f"  â”œâ”€ å…¨é¢‘æ®µèåˆæœºåˆ¶: é—¨æ§èåˆ (Gate Fusion)")
            print(f"  â”œâ”€ é—¨æ§åˆå§‹åŒ–: bias={self.gate_bias_init:.1f} (ä½é¢‘â‰ˆ{100*torch.sigmoid(torch.tensor(self.gate_bias_init)).item():.0f}%)")
        
        if self.use_soft_threshold:
            print(f"  â”œâ”€ è½¯é˜ˆå€¼å»å™ª: âœ… å¯ç”¨ (å¯å­¦ä¹ é˜ˆå€¼)")
        else:
            print(f"  â”œâ”€ è½¯é˜ˆå€¼å»å™ª: âŒ å…³é—­")
        
        if self.use_positional_encoding:
            print(f"  â”œâ”€ ä½ç½®ç¼–ç : âœ… å¯ç”¨ (max_len={self.pos_encoding_max_len})")
        else:
            print(f"  â”œâ”€ ä½ç½®ç¼–ç : âŒ å…³é—­")
        
        fusion_type = 'æ³¨æ„åŠ›' if self.use_freq_attention else ('é‡‘å­—å¡”' if self.pyramid_fusion else 'é—¨æ§')
        freq_emb_str = ' + é¢‘æ®µEmbedding' if self.use_freq_embedding else ''
        pos_emb_str = ' + ä½ç½®ç¼–ç ' if self.use_positional_encoding else ''
        print(f"  â””â”€ ç‰¹æ€§: å…¨å±€å› æœå°æ³¢åˆ†è§£ + å·®å¼‚åŒ–å¤„ç† + {fusion_type}èåˆ{freq_emb_str}{pos_emb_str}")
        print("=" * 70)
    
    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (B, N, T)ï¼Œå…¶ä¸­ N æ˜¯å˜é‡æ•°ï¼ŒT æ˜¯åºåˆ—é•¿åº¦
        
        Returns:
            output: èåˆåçš„ Patch Embeddingï¼Œå½¢çŠ¶ (B*N, num_patches, d_model)
            n_vars: å˜é‡æ•° N
        """
        B, N, T = x.shape
        n_vars = N
        
        # ========== Step 1: å…¨å±€å› æœå°æ³¢åˆ†è§£ ==========
        # x: (B, N, T) -> swt è¾“å‡º: (B, N, T, level+1)
        # è¾“å‡ºé¡ºåº: [cA_n, cD_n, cD_{n-1}, ..., cD_1]
        coeffs = self.swt(x)
        
        if self.pyramid_fusion:
            # ========== é‡‘å­—å¡”èåˆæ¨¡å¼ ==========
            return self._forward_pyramid(coeffs, B, N, n_vars)
        else:
            # ========== åŸå§‹åŒé€šé“æ¨¡å¼ ==========
            return self._forward_dual_channel(coeffs, B, N, n_vars)
    
    def _forward_dual_channel(self, coeffs, B, N, n_vars):
        """åŸå§‹åŒé€šé“èåˆæ¨¡å¼ (level=1)"""
        # æå–ä½é¢‘å’Œé«˜é¢‘åˆ†é‡
        low_freq = coeffs[:, :, :, 0]   # cA: (B, N, T) ä½é¢‘/è¶‹åŠ¿
        high_freq = coeffs[:, :, :, 1]  # cD: (B, N, T) é«˜é¢‘/ç»†èŠ‚
        
        # å¯¹ä½é¢‘åˆ†é‡ Patching
        low_freq = self.padding_patch_layer(low_freq)
        low_patches = low_freq.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        low_patches = low_patches.reshape(B * N, -1, self.patch_len)
        
        # å¯¹é«˜é¢‘åˆ†é‡ Patching
        high_freq = self.padding_patch_layer(high_freq)
        high_patches = high_freq.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        high_patches = high_patches.reshape(B * N, -1, self.patch_len)
        
        # ä½é¢‘è·¯å¾„: ç›´æ¥æŠ•å½±
        # ç¡®ä¿è¾“å…¥ç±»å‹ä¸æƒé‡ç±»å‹åŒ¹é…ï¼ˆå¤„ç†æ··åˆç²¾åº¦è®­ç»ƒï¼‰
        if hasattr(self.low_freq_embedding, 'weight') and low_patches.dtype != self.low_freq_embedding.weight.dtype:
            low_patches = low_patches.to(self.low_freq_embedding.weight.dtype)
        elif hasattr(self.low_freq_embedding, 'conv') and hasattr(self.low_freq_embedding.conv, 'weight') and low_patches.dtype != self.low_freq_embedding.conv.weight.dtype:
            low_patches = low_patches.to(self.low_freq_embedding.conv.weight.dtype)
        e_low = self.low_freq_embedding(low_patches)
        
        # é«˜é¢‘è·¯å¾„: è½¯é˜ˆå€¼å»å™ª â†’ æŠ•å½± â†’ Dropout
        if self.use_soft_threshold:
            high_patches = self.soft_threshold(high_patches)
        # ç¡®ä¿è¾“å…¥ç±»å‹ä¸æƒé‡ç±»å‹åŒ¹é…ï¼ˆå¤„ç†æ··åˆç²¾åº¦è®­ç»ƒï¼‰
        if hasattr(self.high_freq_embedding, 'weight') and high_patches.dtype != self.high_freq_embedding.weight.dtype:
            high_patches = high_patches.to(self.high_freq_embedding.weight.dtype)
        elif hasattr(self.high_freq_embedding, 'conv') and hasattr(self.high_freq_embedding.conv, 'weight') and high_patches.dtype != self.high_freq_embedding.conv.weight.dtype:
            high_patches = high_patches.to(self.high_freq_embedding.conv.weight.dtype)
        e_high = self.high_freq_embedding(high_patches)
        e_high = self.hf_dropout(e_high)
        
        # ğŸ†• åŠ é¢‘æ®µ Embeddingï¼ˆåœ¨èåˆä¹‹å‰ï¼‰
        if self.use_freq_embedding:
            e_low = self.freq_embedding(e_low, freq_idx=0)   # ä½é¢‘
            e_high = self.freq_embedding(e_high, freq_idx=1)  # é«˜é¢‘
        
        # ========== èåˆæœºåˆ¶ ==========
        if self.use_freq_attention:
            # ä½¿ç”¨é¢‘ç‡é€šé“æ³¨æ„åŠ› (Instance-wise åŠ¨æ€è·¯ç”±)
            output, _ = self.freq_attention([e_low, e_high])
        else:
            # é—¨æ§èåˆ
            combined = torch.cat([e_low, e_high], dim=-1)
            gate_weight = self.gate(combined)
            output = gate_weight * e_low + (1 - gate_weight) * e_high
        
        # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_positional_encoding:
            output = output + self.position_embedding(output).to(output.device)
        
        return self.dropout(output), n_vars
    
    def _forward_pyramid(self, coeffs, B, N, n_vars):
        """
        åˆ†å±‚é‡‘å­—å¡”èåˆæ¨¡å¼ (level >= 2)
        
        èåˆé¡ºåº (ä»¥ level=2 ä¸ºä¾‹):
        coeffs é¡ºåº: [cA_2, cD_2, cD_1]
        
        Step 1: å¯¹æ¯ä¸ªé¢‘æ®µè¿›è¡Œ Patching å’ŒæŠ•å½±
        Step 2: ä»æœ€é«˜é¢‘å¼€å§‹é€çº§èåˆ
            - e_D1 (æœ€é«˜é¢‘) + e_D2 (ä¸­é¢‘) â†’ e_detail_fused
            - e_detail_fused + e_A (ä½é¢‘) â†’ e_final
        
        èåˆé¡ºåº (ä»¥ level=3 ä¸ºä¾‹):
        coeffs é¡ºåº: [cA_3, cD_3, cD_2, cD_1]
        
        Step 2:
            - e_D1 + e_D2 â†’ e_fused_12
            - e_fused_12 + e_D3 â†’ e_detail_fused
            - e_detail_fused + e_A â†’ e_final
        """
        # ========== Step 1: å¯¹æ¯ä¸ªé¢‘æ®µè¿›è¡Œ Patching å’ŒæŠ•å½± ==========
        band_embeddings = []
        
        for i in range(self.num_bands):
            # æå–ç¬¬ i ä¸ªé¢‘æ®µ
            band = coeffs[:, :, :, i]  # (B, N, T)
            
            # Patching
            band = self.padding_patch_layer(band)
            patches = band.unfold(dimension=-1, size=self.patch_len, step=self.stride)
            patches = patches.reshape(B * N, -1, self.patch_len)
            
            # å¯¹é«˜é¢‘é¢‘æ®µåº”ç”¨è½¯é˜ˆå€¼å»å™ª (i > 0 è¡¨ç¤ºé«˜é¢‘)
            if i > 0 and self.use_soft_threshold:
                patches = self.band_thresholds[i](patches)
            
            # ç¡®ä¿è¾“å…¥ç±»å‹ä¸æƒé‡ç±»å‹åŒ¹é…ï¼ˆå¤„ç†æ··åˆç²¾åº¦è®­ç»ƒï¼‰
            embedding_layer = self.band_embeddings[i]
            if hasattr(embedding_layer, 'weight') and patches.dtype != embedding_layer.weight.dtype:
                patches = patches.to(embedding_layer.weight.dtype)
            elif hasattr(embedding_layer, 'conv') and hasattr(embedding_layer.conv, 'weight') and patches.dtype != embedding_layer.conv.weight.dtype:
                patches = patches.to(embedding_layer.conv.weight.dtype)
            
            # æŠ•å½±
            e_band = embedding_layer(patches)  # (B*N, num_patches, d_model)
            
            # ğŸ†• åŠ é¢‘æ®µ Embeddingï¼ˆåœ¨ Dropout ä¹‹å‰ï¼‰
            if self.use_freq_embedding:
                e_band = self.freq_embedding(e_band, freq_idx=i)
            
            # å¯¹é«˜é¢‘é¢‘æ®µåº”ç”¨ Dropout
            e_band = self.band_dropouts[i](e_band)
            
            band_embeddings.append(e_band)
        
        # band_embeddings é¡ºåº: [e_cA, e_cD_n, e_cD_{n-1}, ..., e_cD_1]
        
        # ========== Step 2: èåˆæœºåˆ¶ ==========
        if self.use_freq_attention:
            # ä½¿ç”¨é¢‘ç‡é€šé“æ³¨æ„åŠ› (Instance-wise åŠ¨æ€è·¯ç”±)
            # ç›´æ¥å°†æ‰€æœ‰é¢‘æ®µä¼ å…¥æ³¨æ„åŠ›æ¨¡å—ï¼Œè®©å®ƒè‡ªåŠ¨å­¦ä¹ æƒé‡
            e_fused, _ = self.freq_attention(band_embeddings)
        else:
            # åŸå§‹é—¨æ§èåˆ: ä»æœ€é«˜é¢‘ (cD_1) å¼€å§‹ï¼Œé€çº§å‘ä½é¢‘èåˆ
            # æœ€é«˜é¢‘åœ¨ band_embeddings çš„æœ€åä¸€ä¸ªä½ç½®
            
            # åˆå§‹åŒ–: ä»æœ€é«˜é¢‘å¼€å§‹
            e_fused = band_embeddings[-1]  # e_cD_1 (æœ€é«˜é¢‘)
            
            # é€çº§èåˆ: cD_1 â†’ cD_2 â†’ ... â†’ cD_n â†’ cA
            # èåˆé¡ºåº: band_embeddings[-2], band_embeddings[-3], ..., band_embeddings[0]
            for i in range(self.num_bands - 2, -1, -1):
                e_next = band_embeddings[i]  # ä¸‹ä¸€ä¸ªè¦èåˆçš„é¢‘æ®µ (æ›´ä½é¢‘)
                
                # é—¨æ§ç´¢å¼•: ä» 0 å¼€å§‹
                gate_idx = (self.num_bands - 2) - i
                
                # é—¨æ§èåˆ
                combined = torch.cat([e_fused, e_next], dim=-1)
                gate_weight = self.gate_layers[gate_idx](combined)
                
                # èåˆ: gate * å½“å‰èåˆç»“æœ + (1-gate) * ä¸‹ä¸€ä¸ªé¢‘æ®µ
                # æ³¨æ„: å¯¹äºæœ€åä¸€ä¸ªé—¨æ§ (èåˆ cA)ï¼Œgate åå‘ä½é¢‘ï¼Œæ‰€ä»¥ (1-gate) ä¼šæ›´å¤§
                e_fused = gate_weight * e_fused + (1 - gate_weight) * e_next
        
        # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_positional_encoding:
            e_fused = e_fused + self.position_embedding(e_fused).to(e_fused.device)
        
        return self.dropout(e_fused), n_vars
    
    def forward_separated(self, x):
        """
        ä¸ºCWPRå±‚æä¾›åˆ†ç¦»çš„ç‰¹å¾è¾“å‡º
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (B, N, T)
        
        Returns:
            e_cA: ä½é¢‘è¶‹åŠ¿ç‰¹å¾ï¼Œå½¢çŠ¶ (B*N, num_patches, d_model)
            e_detail: é«˜é¢‘ç»†èŠ‚ç‰¹å¾ï¼ˆèåˆåçš„ï¼‰ï¼Œå½¢çŠ¶ (B*N, num_patches, d_model)
            n_vars: å˜é‡æ•° N
        """
        B, N, T = x.shape
        n_vars = N
        
        # ========== Step 1: å…¨å±€å› æœå°æ³¢åˆ†è§£ ==========
        coeffs = self.swt(x)  # (B, N, T, level+1)
        
        if self.pyramid_fusion:
            # ========== é‡‘å­—å¡”èåˆæ¨¡å¼ ==========
            return self._forward_pyramid_separated(coeffs, B, N, n_vars)
        else:
            # ========== åŸå§‹åŒé€šé“æ¨¡å¼ ==========
            return self._forward_dual_channel_separated(coeffs, B, N, n_vars)
    
    def _forward_dual_channel_separated(self, coeffs, B, N, n_vars):
        """åŒé€šé“æ¨¡å¼çš„åˆ†ç¦»è¾“å‡º (level=1)"""
        # æå–ä½é¢‘å’Œé«˜é¢‘åˆ†é‡
        low_freq = coeffs[:, :, :, 0]   # cA: (B, N, T) ä½é¢‘/è¶‹åŠ¿
        high_freq = coeffs[:, :, :, 1]  # cD: (B, N, T) é«˜é¢‘/ç»†èŠ‚
        
        # å¯¹ä½é¢‘åˆ†é‡ Patching
        low_freq = self.padding_patch_layer(low_freq)
        low_patches = low_freq.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        low_patches = low_patches.reshape(B * N, -1, self.patch_len)
        
        # å¯¹é«˜é¢‘åˆ†é‡ Patching
        high_freq = self.padding_patch_layer(high_freq)
        high_patches = high_freq.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        high_patches = high_patches.reshape(B * N, -1, self.patch_len)
        
        # ä½é¢‘è·¯å¾„: ç›´æ¥æŠ•å½±
        if hasattr(self.low_freq_embedding, 'weight') and low_patches.dtype != self.low_freq_embedding.weight.dtype:
            low_patches = low_patches.to(self.low_freq_embedding.weight.dtype)
        elif hasattr(self.low_freq_embedding, 'conv') and hasattr(self.low_freq_embedding.conv, 'weight') and low_patches.dtype != self.low_freq_embedding.conv.weight.dtype:
            low_patches = low_patches.to(self.low_freq_embedding.conv.weight.dtype)
        e_cA = self.low_freq_embedding(low_patches)
        
        # é«˜é¢‘è·¯å¾„: è½¯é˜ˆå€¼å»å™ª â†’ æŠ•å½± â†’ Dropout
        if self.use_soft_threshold:
            high_patches = self.soft_threshold(high_patches)
        if hasattr(self.high_freq_embedding, 'weight') and high_patches.dtype != self.high_freq_embedding.weight.dtype:
            high_patches = high_patches.to(self.high_freq_embedding.weight.dtype)
        elif hasattr(self.high_freq_embedding, 'conv') and hasattr(self.high_freq_embedding.conv, 'weight') and high_patches.dtype != self.high_freq_embedding.conv.weight.dtype:
            high_patches = high_patches.to(self.high_freq_embedding.conv.weight.dtype)
        e_detail = self.high_freq_embedding(high_patches)
        e_detail = self.hf_dropout(e_detail)
        
        # æ·»åŠ é¢‘æ®µ Embeddingï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_freq_embedding:
            e_cA = self.freq_embedding(e_cA, freq_idx=0)
            e_detail = self.freq_embedding(e_detail, freq_idx=1)
        
        # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_positional_encoding:
            e_cA = e_cA + self.position_embedding(e_cA).to(e_cA.device)
            e_detail = e_detail + self.position_embedding(e_detail).to(e_detail.device)
        
        # åº”ç”¨ Dropout
        e_cA = self.dropout(e_cA)
        e_detail = self.dropout(e_detail)
        
        return e_cA, e_detail, n_vars
    
    def _forward_pyramid_separated(self, coeffs, B, N, n_vars):
        """é‡‘å­—å¡”èåˆæ¨¡å¼çš„åˆ†ç¦»è¾“å‡º (level >= 2)"""
        # ========== Step 1: å¯¹æ¯ä¸ªé¢‘æ®µè¿›è¡Œ Patching å’ŒæŠ•å½± ==========
        band_embeddings = []
        
        for i in range(self.num_bands):
            # æå–ç¬¬ i ä¸ªé¢‘æ®µ
            band = coeffs[:, :, :, i]  # (B, N, T)
            
            # Patching
            band = self.padding_patch_layer(band)
            patches = band.unfold(dimension=-1, size=self.patch_len, step=self.stride)
            patches = patches.reshape(B * N, -1, self.patch_len)
            
            # å¯¹é«˜é¢‘é¢‘æ®µåº”ç”¨è½¯é˜ˆå€¼å»å™ª (i > 0 è¡¨ç¤ºé«˜é¢‘)
            if i > 0 and self.use_soft_threshold:
                patches = self.band_thresholds[i](patches)
            
            # ç¡®ä¿è¾“å…¥ç±»å‹ä¸æƒé‡ç±»å‹åŒ¹é…
            embedding_layer = self.band_embeddings[i]
            if hasattr(embedding_layer, 'weight') and patches.dtype != embedding_layer.weight.dtype:
                patches = patches.to(embedding_layer.weight.dtype)
            elif hasattr(embedding_layer, 'conv') and hasattr(embedding_layer.conv, 'weight') and patches.dtype != embedding_layer.conv.weight.dtype:
                patches = patches.to(embedding_layer.conv.weight.dtype)
            
            # æŠ•å½±
            e_band = embedding_layer(patches)  # (B*N, num_patches, d_model)
            
            # åŠ é¢‘æ®µ Embeddingï¼ˆåœ¨ Dropout ä¹‹å‰ï¼‰
            if self.use_freq_embedding:
                e_band = self.freq_embedding(e_band, freq_idx=i)
            
            # å¯¹é«˜é¢‘é¢‘æ®µåº”ç”¨ Dropout
            e_band = self.band_dropouts[i](e_band)
            
            band_embeddings.append(e_band)
        
        # band_embeddings é¡ºåº: [e_cA, e_cD_n, e_cD_{n-1}, ..., e_cD_1]
        # e_cA æ˜¯ band_embeddings[0]ï¼ˆä½é¢‘ï¼‰
        e_cA = band_embeddings[0]
        
        # ========== Step 2: é«˜é¢‘èåˆ ==========
        # æå–é«˜é¢‘éƒ¨åˆ†å¹¶èåˆ: e_cD_n, e_cD_{n-1}, ..., cD_1
        high_freq_bands = band_embeddings[1:]  # æ‰€æœ‰é«˜é¢‘é¢‘æ®µ
        
        if len(high_freq_bands) == 1:
            # åªæœ‰ä¸€ä¸ªé«˜é¢‘é¢‘æ®µï¼Œç›´æ¥ä½¿ç”¨
            e_detail = high_freq_bands[0]
        elif self.hf_freq_attention is not None:
            # ä½¿ç”¨é¢‘ç‡æ³¨æ„åŠ›V1ç‰ˆæœ¬è¿›è¡Œé«˜é¢‘èåˆ
            e_detail, _ = self.hf_freq_attention(high_freq_bands)
        else:
            # ä½¿ç”¨é—¨æ§èåˆè¿›è¡Œé«˜é¢‘èåˆï¼ˆå½“use_hf_freq_attention=Falseæ—¶ï¼‰
            # ä»æœ€é«˜é¢‘ (cD_1) å¼€å§‹ï¼Œé€çº§å‘ä¸­é¢‘èåˆ
            e_detail = band_embeddings[-1]  # e_cD_1 (æœ€é«˜é¢‘)
            for i in range(self.num_bands - 2, 0, -1):  # ä»å€’æ•°ç¬¬äºŒä¸ªåˆ°ç¬¬äºŒä¸ªï¼ˆä¸åŒ…æ‹¬ç¬¬ä¸€ä¸ªcAï¼‰
                e_next = band_embeddings[i]
                # é—¨æ§ç´¢å¼•ï¼šè·³è¿‡æœ€åä¸€ä¸ªé—¨æ§ï¼ˆå› ä¸ºæœ€åä¸€ä¸ªé—¨æ§æ˜¯èåˆcAçš„ï¼‰
                gate_idx = (self.num_bands - 2) - i
                combined = torch.cat([e_detail, e_next], dim=-1)
                gate_weight = self.gate_layers[gate_idx](combined)
                e_detail = gate_weight * e_detail + (1 - gate_weight) * e_next
        
        # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_positional_encoding:
            e_cA = e_cA + self.position_embedding(e_cA).to(e_cA.device)
            e_detail = e_detail + self.position_embedding(e_detail).to(e_detail.device)
        
        # åº”ç”¨ Dropout
        e_cA = self.dropout(e_cA)
        e_detail = self.dropout(e_detail)
        
        return e_cA, e_detail, n_vars