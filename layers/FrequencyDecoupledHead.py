"""
é¢‘ç‡è§£è€¦è¾“å‡ºå¤´ (Frequency Decoupled Head)

åŒ…å«å¤šç§ä» LLM éšçŠ¶æ€æ˜ å°„åˆ°æ—¶åŸŸé¢„æµ‹çš„è¾“å‡ºå¤´è®¾è®¡ï¼š
1. Tri-Band Decoupled Head (V2.0): ä¸‰é¢‘å¸¦è§£è€¦ + è½¯é˜ˆå€¼ + æ·±åº¦ç›‘ç£
2. Dual-Scale Residual Head (New): åŒå°ºåº¦æ®‹å·® (Global Trend + Local Detail)

Author: CAST Project
Date: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class SoftThreshold(nn.Module):
    """
    å¯å­¦ä¹ è½¯é˜ˆå€¼æ¨¡å—
    
    åœ¨éšç©ºé—´ä¸­åº”ç”¨è½¯é˜ˆå€¼å»å™ªï¼Œæ»¤é™¤å°å¹…åº¦çš„å™ªå£°æ¿€æ´»ã€‚
    å…¬å¼: y = sign(x) * ReLU(|x| - Ï„)
    
    Args:
        num_features: ç‰¹å¾ç»´åº¦
        init_tau: åˆå§‹é˜ˆå€¼ (é»˜è®¤ 0.1)
    """
    def __init__(self, num_features, init_tau=0.1):
        super(SoftThreshold, self).__init__()
        self.tau = nn.Parameter(torch.ones(num_features) * init_tau)
    
    def forward(self, x):
        tau = torch.abs(self.tau)  # ç¡®ä¿é˜ˆå€¼ä¸ºæ­£
        return torch.sign(x) * F.relu(torch.abs(x) - tau)
    
    def extra_repr(self):
        return f'num_features={self.tau.shape[0]}, init_tau={self.tau.mean().item():.4f}'


class TriBandDecoupledHead(nn.Module):
    """
    ä¸‰é¢‘å¸¦è§£è€¦è¾“å‡ºå¤´ (Tri-Band Decoupled Head)
    
    å°† LLM çš„éšçŠ¶æ€è§£è€¦ä¸ºä¸‰ä¸ªé¢‘ç‡åˆ†é‡çš„æ—¶åŸŸé¢„æµ‹ï¼Œç„¶åç›¸åŠ å¾—åˆ°æœ€ç»ˆé¢„æµ‹ã€‚
    
    æ¶æ„:
        LLM Output (B*N, nf)
            â”‚
            â”œâ”€â”€â–º Head_Trend (Linear) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Pred_Trend
            â”‚
            â”œâ”€â”€â–º Head_Mid (Linear + Dropout) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Pred_Mid
            â”‚
            â””â”€â”€â–º Head_Detail (Linear + SoftThreshold + Dropout) â”€â”€â–º Pred_Detail
            â”‚
            â–¼
        Final = Pred_Trend + Pred_Mid + Pred_Detail
    
    Args:
        n_vars: å˜é‡æ•°é‡
        nf: è¾“å…¥ç‰¹å¾ç»´åº¦ (d_ff * patch_nums)
        target_window: é¢„æµ‹çª—å£é•¿åº¦ (pred_len)
        head_dropout: è¾“å‡º Dropout æ¯”ä¾‹
        mid_dropout: ä¸­é¢‘å¤´ Dropout æ¯”ä¾‹ (é»˜è®¤ 0.2)
        high_dropout: é«˜é¢‘å¤´ Dropout æ¯”ä¾‹ (é»˜è®¤ 0.5)
        use_soft_threshold: æ˜¯å¦åœ¨é«˜é¢‘å¤´ä½¿ç”¨è½¯é˜ˆå€¼ (é»˜è®¤ True)
        soft_threshold_init: è½¯é˜ˆå€¼åˆå§‹å€¼ (é»˜è®¤ 0.1)
        use_conv: æ˜¯å¦ä½¿ç”¨ Conv1d æ›¿ä»£ Linear (å¢åŠ ä½ç½®æ„ŸçŸ¥ï¼Œé»˜è®¤ False)
    """
    
    def __init__(self, n_vars, nf, target_window, head_dropout=0.1,
                 mid_dropout=0.2, high_dropout=0.5,
                 use_soft_threshold=True, soft_threshold_init=0.1,
                 use_conv=False):
        super(TriBandDecoupledHead, self).__init__()
        
        self.n_vars = n_vars
        self.nf = nf
        self.target_window = target_window
        self.use_soft_threshold = use_soft_threshold
        self.use_conv = use_conv
        
        # å±•å¹³å±‚
        self.flatten = nn.Flatten(start_dim=-2)
        
        # ========== Head 1: ä½é¢‘/è¶‹åŠ¿å¤´ (æ— æ­£åˆ™) ==========
        if use_conv:
            # Conv1d æä¾›ä½ç½®æ„ŸçŸ¥èƒ½åŠ›
            self.head_trend = nn.Conv1d(nf, target_window, kernel_size=1)
        else:
            self.head_trend = nn.Linear(nf, target_window)
        
        # ========== Head 2: ä¸­é¢‘å¤´ (è½»å¾® Dropout) ==========
        if use_conv:
            self.head_mid_proj = nn.Conv1d(nf, target_window, kernel_size=1)
        else:
            self.head_mid_proj = nn.Linear(nf, target_window)
        self.head_mid_dropout = nn.Dropout(mid_dropout)
        
        # ========== Head 3: é«˜é¢‘/ç»†èŠ‚å¤´ (å¼ºæ­£åˆ™) ==========
        # è®¾è®¡ï¼šå…ˆæŠ•å½±åˆ°éšå±‚ â†’ è½¯é˜ˆå€¼å»å™ª â†’ é‡æ„åˆ°æ—¶åŸŸ
        hidden_dim = max(nf // 2, target_window)  # éšå±‚ç»´åº¦
        
        if use_conv:
            self.head_detail_to_latent = nn.Conv1d(nf, hidden_dim, kernel_size=1)
            self.head_detail_to_time = nn.Conv1d(hidden_dim, target_window, kernel_size=1)
        else:
            self.head_detail_to_latent = nn.Linear(nf, hidden_dim)
            self.head_detail_to_time = nn.Linear(hidden_dim, target_window)
        
        if use_soft_threshold:
            self.soft_threshold = SoftThreshold(hidden_dim, init_tau=soft_threshold_init)
        
        self.head_detail_dropout = nn.Dropout(high_dropout)
        
        # è¾“å‡º Dropout
        self.output_dropout = nn.Dropout(head_dropout)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
        # æ‰“å°é…ç½®
        self._print_config()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def _print_config(self):
        """æ‰“å°æ¨¡å—é…ç½®"""
        print("=" * 70)
        print("[TriBandDecoupledHead] ä¸‰é¢‘å¸¦è§£è€¦è¾“å‡ºå¤´å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ è¾“å…¥ç‰¹å¾ç»´åº¦: {self.nf}")
        print(f"  â”œâ”€ é¢„æµ‹çª—å£é•¿åº¦: {self.target_window}")
        print(f"  â”œâ”€ å˜é‡æ•°é‡: {self.n_vars}")
        print(f"  â”œâ”€ æŠ•å½±ç±»å‹: {'Conv1d' if self.use_conv else 'Linear'}")
        print(f"  â”œâ”€ Head 1 (Trend): æ— æ­£åˆ™åŒ–")
        print(f"  â”œâ”€ Head 2 (Mid): Dropout={self.head_mid_dropout.p}")
        print(f"  â”œâ”€ Head 3 (Detail):")
        print(f"  â”‚   â”œâ”€ SoftThreshold: {'âœ… å¯ç”¨' if self.use_soft_threshold else 'âŒ å…³é—­'}")
        print(f"  â”‚   â””â”€ Dropout={self.head_detail_dropout.p}")
        print(f"  â””â”€ è¾“å‡º Dropout: {self.output_dropout.p}")
        print("=" * 70)
    
    def forward(self, x, return_components=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: LLM è¾“å‡ºï¼Œå½¢çŠ¶ (B, n_vars, d_ff, patch_nums) æˆ– (B, n_vars, nf)
            return_components: æ˜¯å¦è¿”å›ä¸‰ä¸ªé¢‘ç‡åˆ†é‡ (ç”¨äºæ·±åº¦ç›‘ç£)
        
        Returns:
            final_pred: æœ€ç»ˆé¢„æµ‹ï¼Œå½¢çŠ¶ (B, target_window, n_vars)
            components: (å¯é€‰) å­—å…¸ï¼ŒåŒ…å« pred_trend, pred_mid, pred_detail
        """
        # è¾“å…¥å½¢çŠ¶å¤„ç†
        if x.dim() == 4:
            # (B, n_vars, d_ff, patch_nums) -> (B, n_vars, nf)
            B, N, D, P = x.shape
            x = self.flatten(x)  # (B, n_vars, d_ff * patch_nums)
        else:
            B, N, _ = x.shape
        
        # ========== Head 1: ä½é¢‘è¶‹åŠ¿é¢„æµ‹ ==========
        if self.use_conv:
            pred_trend = self.head_trend(x.transpose(-1, -2)).transpose(-1, -2)
        else:
            pred_trend = self.head_trend(x)  # (B, n_vars, target_window)
        
        # ========== Head 2: ä¸­é¢‘é¢„æµ‹ ==========
        if self.use_conv:
            pred_mid = self.head_mid_proj(x.transpose(-1, -2)).transpose(-1, -2)
        else:
            pred_mid = self.head_mid_proj(x)
        pred_mid = self.head_mid_dropout(pred_mid)
        
        # ========== Head 3: é«˜é¢‘ç»†èŠ‚é¢„æµ‹ ==========
        # Step 1: æŠ•å½±åˆ°éšå±‚
        if self.use_conv:
            h_detail = self.head_detail_to_latent(x.transpose(-1, -2)).transpose(-1, -2)
        else:
            h_detail = self.head_detail_to_latent(x)
        
        # Step 2: éšå±‚è½¯é˜ˆå€¼å»å™ª
        if self.use_soft_threshold:
            h_detail = self.soft_threshold(h_detail)
        
        # Step 3: Dropout
        h_detail = self.head_detail_dropout(h_detail)
        
        # Step 4: é‡æ„åˆ°æ—¶åŸŸ
        if self.use_conv:
            pred_detail = self.head_detail_to_time(h_detail.transpose(-1, -2)).transpose(-1, -2)
        else:
            pred_detail = self.head_detail_to_time(h_detail)
        
        # ========== æ—¶åŸŸç›´æ¥ç›¸åŠ é‡æ„ ==========
        final_pred = pred_trend + pred_mid + pred_detail
        
        # è¾“å‡º Dropout
        final_pred = self.output_dropout(final_pred)
        
        # è°ƒæ•´è¾“å‡ºå½¢çŠ¶: (B, n_vars, target_window) -> (B, target_window, n_vars)
        final_pred = final_pred.permute(0, 2, 1).contiguous()
        
        if return_components:
            components = {
                'pred_trend': pred_trend.permute(0, 2, 1).contiguous(),
                'pred_mid': pred_mid.permute(0, 2, 1).contiguous(),
                'pred_detail': pred_detail.permute(0, 2, 1).contiguous(),
            }
            return final_pred, components
        
        return final_pred


class DualScaleResidualHead(nn.Module):
    """
    åŒå°ºåº¦æ®‹å·®å¤´ (Dual-Scale Residual Head) - ç®€åŒ–ç‰ˆ
    
    è®¾è®¡ç†å¿µï¼šæ˜¾å¼åˆ†ç¦»æ•´ä½“è¶‹åŠ¿ä¸å±€éƒ¨ç»†èŠ‚ï¼Œåˆ©ç”¨æ®‹å·®å­¦ä¹ åŠ é€Ÿæ”¶æ•›ã€‚
    
    æ¶æ„ï¼š
        LLM Output (B, N, d_ff, patch_nums)
           â”‚
           â”œâ”€â”€â–º Branch A (Trend): GAP -> Linear(d_ff, T) â”€â”€â”€â”€â”€â”€â”€â–º Pred_Trend (æ•´ä½“æ°´ä½)
           â”‚
           â””â”€â”€â–º Branch B (Detail): Flatten -> Linear(nf, T) â”€â”€â”€â”€â–º Pred_Detail (å±€éƒ¨æ³¢åŠ¨)
           â”‚
           â–¼
        Final = Pred_Trend + Pred_Detail
    
    Args:
        n_vars: å˜é‡æ•°é‡
        d_ff: FFN ç»´åº¦ (ç‰¹å¾é€šé“æ•°)
        patch_nums: Patch æ•°é‡
        target_window: é¢„æµ‹çª—å£é•¿åº¦ (pred_len)
        head_dropout: è¾“å‡º Dropout æ¯”ä¾‹
    """
    def __init__(self, n_vars, d_ff, patch_nums, target_window, head_dropout=0.1):
        super().__init__()
        self.n_vars = n_vars
        self.d_ff = d_ff
        self.patch_nums = patch_nums
        self.target_window = target_window
        self.nf = d_ff * patch_nums
        
        # Branch A: Global Trend (GAP + Small Linear)
        # è¾“å…¥: (B*N, d_ff) <- GAP over patch_nums
        self.head_trend = nn.Linear(d_ff, target_window)
        
        # Branch B: Local Detail (Flatten + Large Linear)
        # è¾“å…¥: (B*N, nf)
        self.flatten = nn.Flatten(start_dim=-2)
        self.head_detail = nn.Linear(self.nf, target_window)
        
        self.dropout = nn.Dropout(head_dropout)
        
        self._init_weights()
        self._print_config()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def _print_config(self):
        print("=" * 70)
        print("[DualScaleResidualHead] åŒå°ºåº¦æ®‹å·®å¤´å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ è¾“å…¥ç»´åº¦: (B, {self.n_vars}, {self.d_ff}, {self.patch_nums})")
        print(f"  â”œâ”€ é¢„æµ‹çª—å£: {self.target_window}")
        print(f"  â”œâ”€ Branch A (Trend): GAP -> Linear({self.d_ff} -> {self.target_window})")
        print(f"  â”œâ”€ Branch B (Detail): Flatten -> Linear({self.nf} -> {self.target_window})")
        print(f"  â””â”€ Dropout: {self.dropout.p}")
        print("=" * 70)

    def forward(self, x):
        """
        Args:
            x: (B, n_vars, d_ff, patch_nums)
        Returns:
            final_pred: (B, target_window, n_vars)
        """
        B, N, D, P = x.shape
        # ç¡®ä¿è¾“å…¥æ˜¯é¢„æœŸçš„å½¢çŠ¶
        # å¦‚æœè¾“å…¥ç»´åº¦ä¸å¯¹ï¼Œå°è¯•ä¿®æ­£ (å…¼å®¹ Flatten åçš„è¾“å…¥)
        if x.dim() == 3: # (B, N, nf)
            if x.shape[-1] == self.nf:
                 # è¿™ç§æƒ…å†µä¸‹æ— æ³•è¿›è¡Œ Trend åˆ†æ”¯çš„ GAP è®¡ç®—ï¼Œå› ä¸ºç©ºé—´ä¿¡æ¯å·²ä¸¢å¤±
                 # æ‰€ä»¥å¦‚æœç”¨äº†è¿™ä¸ªHeadï¼Œå¿…é¡»è¾“å…¥ (B, N, D, P)
                 # ä½œä¸ºä¸€ä¸ªå…¼å®¹æ€§å›é€€ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯• reshape å›å»ï¼Œä½†è¿™ä¾èµ– d_ff å’Œ patch_nums çš„æ­£ç¡®æ€§
                 try:
                     x = x.view(B, N, self.d_ff, self.patch_nums)
                 except:
                     raise ValueError(f"[DualScaleResidualHead] è¾“å…¥å½¢çŠ¶é”™è¯¯: {x.shape}, æœŸæœ› (B, N, {self.d_ff}, {self.patch_nums})")
        
        # å˜æ¢ä¸º (B*N, D, P) ä»¥ä¾¿æ‰¹é‡å¤„ç†
        x = x.view(B * N, D, P)
        
        # Branch A: Trend
        # Global Average Pooling over Patch Dimension
        x_trend = x.mean(dim=-1) # (B*N, D)
        pred_trend = self.head_trend(x_trend) # (B*N, T)
        
        # Branch B: Detail
        x_detail = x.view(B * N, -1) # Flatten -> (B*N, D*P)
        pred_detail = self.head_detail(x_detail) # (B*N, T)
        
        # Fusion
        final_pred = pred_trend + pred_detail
        
        # Dropout
        final_pred = self.dropout(final_pred)
        
        # Reshape: (B*N, T) -> (B, N, T) -> (B, T, N)
        final_pred = final_pred.view(B, N, self.target_window).permute(0, 2, 1).contiguous()
        
        return final_pred


class DeepSupervisionLoss(nn.Module):
    """
    æ·±åº¦ç›‘ç£æŸå¤±æ¨¡å—
    
    ä½¿ç”¨æ ‡å‡† SWT (éå› æœï¼Œæ— æŸ) åˆ†è§£ Ground Truthï¼Œ
    ä½œä¸ºä¸‰ä¸ªé¢‘ç‡å¤´çš„è¾…åŠ©ç›‘ç£ç›®æ ‡ã€‚
    
    Total Loss = Main Loss + Î± Ã— (Loss_Trend + Loss_Mid + Loss_Detail)
    
    Args:
        wavelet: å°æ³¢ç±»å‹ (é»˜è®¤ 'db4')
        level: åˆ†è§£å±‚æ•° (é»˜è®¤ 2ï¼Œäº§ç”Ÿ 3 ä¸ªé¢‘å¸¦)
        alpha: è¾…åŠ©æŸå¤±æƒé‡ (é»˜è®¤ 0.3)
        use_causal_swt: æ˜¯å¦ä½¿ç”¨å› æœ SWT åˆ†è§£ GT (Plan Bï¼Œé»˜è®¤ False)
    """
    
    def __init__(self, wavelet='db4', level=2, alpha=0.3, use_causal_swt=False):
        super(DeepSupervisionLoss, self).__init__()
        
        self.wavelet = wavelet
        self.level = level
        self.alpha = alpha
        self.use_causal_swt = use_causal_swt
        self.num_bands = level + 1
        
        # å°è¯•å¯¼å…¥å°æ³¢æ¨¡å—
        self._init_swt()
        
        print("=" * 70)
        print("[DeepSupervisionLoss] æ·±åº¦ç›‘ç£æŸå¤±å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ å°æ³¢ç±»å‹: {wavelet}")
        print(f"  â”œâ”€ åˆ†è§£å±‚æ•°: {level}")
        print(f"  â”œâ”€ é¢‘å¸¦æ•°é‡: {self.num_bands}")
        print(f"  â”œâ”€ è¾…åŠ©æŸå¤±æƒé‡ Î±: {alpha}")
        print(f"  â”œâ”€ SWT ç±»å‹: {'Causal (Plan B)' if use_causal_swt else 'Standard (Plan A)'}")
        print(f"  â””â”€ Loss å…¬å¼: Main + Î± Ã— (Aux_Trend + Aux_Mid + Aux_Detail)")
        print("=" * 70)
    
    def _init_swt(self):
        """åˆå§‹åŒ– SWT æ¨¡å—"""
        if self.use_causal_swt:
            # Plan B: ä½¿ç”¨å› æœ SWT
            try:
                from layers.CausalWavelet import CausalSWT
            except ImportError:
                from .CausalWavelet import CausalSWT
            self.swt = CausalSWT(wavelet=self.wavelet, level=self.level)
            self.swt_type = 'causal'
        else:
            # Plan A: ä½¿ç”¨æ ‡å‡† SWT (å°è¯• ptwtï¼Œå¦åˆ™å›é€€åˆ°å› æœç‰ˆæœ¬)
            try:
                import ptwt
                import pywt
                self.swt_type = 'standard'
                self._ptwt = ptwt
                self._pywt = pywt
            except ImportError:
                print("[Warning] ptwt/pywt æœªå®‰è£…ï¼Œå›é€€åˆ° CausalSWT")
                try:
                    from layers.CausalWavelet import CausalSWT
                except ImportError:
                    from .CausalWavelet import CausalSWT
                self.swt = CausalSWT(wavelet=self.wavelet, level=self.level)
                self.swt_type = 'causal'
    
    def _standard_swt(self, x):
        """
        æ ‡å‡† SWT åˆ†è§£ (éå› æœï¼Œä½¿ç”¨ ptwt)
        
        Args:
            x: (B, N, T) è¾“å…¥ä¿¡å·
        
        Returns:
            coeffs: (B, N, T, num_bands) å°æ³¢ç³»æ•°
        """
        B, N, T = x.shape
        device = x.device
        dtype = x.dtype
        
        # ptwt.swt éœ€è¦ (B, T) æˆ– (B, C, T) è¾“å…¥
        # æˆ‘ä»¬é€å˜é‡å¤„ç†
        x_flat = x.reshape(B * N, T)
        
        # è½¬ä¸º float32 (ptwt å¯èƒ½ä¸æ”¯æŒ bfloat16)
        x_float = x_flat.float()
        
        # SWT åˆ†è§£
        coeffs_list = self._ptwt.swt(x_float, self._pywt.Wavelet(self.wavelet), level=self.level)
        
        # coeffs_list æ˜¯ [(cA, cD), (cA, cD), ...] æˆ– [cD1, cD2, ..., cA]
        # ptwt.swt è¿”å›æ ¼å¼: list of (cA, cD) tuples, ä» level 1 åˆ° level n
        # æˆ‘ä»¬éœ€è¦é‡ç»„ä¸º [cA_n, cD_n, cD_{n-1}, ..., cD_1]
        
        all_bands = []
        
        # æå–æœ€ç»ˆçš„è¿‘ä¼¼ç³»æ•° cA
        cA = coeffs_list[-1][0]  # æœ€åä¸€å±‚çš„ cA
        all_bands.append(cA)
        
        # æå–ç»†èŠ‚ç³»æ•° cD (ä»é«˜å±‚åˆ°ä½å±‚)
        for i in range(self.level - 1, -1, -1):
            cD = coeffs_list[i][1]
            all_bands.append(cD)
        
        # Stack: (B*N, T, num_bands)
        coeffs = torch.stack(all_bands, dim=-1)
        
        # Reshape: (B*N, T, num_bands) -> (B, N, T, num_bands)
        coeffs = coeffs.reshape(B, N, T, self.num_bands)
        
        # è½¬å›åŸå§‹ dtype
        coeffs = coeffs.to(dtype)
        
        return coeffs
    
    def _decompose_target(self, target):
        """
        åˆ†è§£ç›®æ ‡åºåˆ—
        
        Args:
            target: (B, T, N) Ground Truth
        
        Returns:
            target_bands: dict, åŒ…å« 'trend', 'mid', 'detail'
        """
        # è°ƒæ•´å½¢çŠ¶: (B, T, N) -> (B, N, T)
        target = target.permute(0, 2, 1).contiguous()
        B, N, T = target.shape
        
        if self.swt_type == 'standard':
            coeffs = self._standard_swt(target)
        else:
            coeffs = self.swt(target)
        
        # coeffs: (B, N, T, num_bands)
        # é¡ºåº: [cA_n (trend), cD_n (mid), cD_1 (detail)]
        
        # å¯¹äº level=2: [cA2, cD2, cD1]
        target_trend = coeffs[:, :, :, 0]    # cA: ä½é¢‘è¶‹åŠ¿
        target_mid = coeffs[:, :, :, 1]      # cD2: ä¸­é¢‘
        target_detail = coeffs[:, :, :, -1]  # cD1: é«˜é¢‘ç»†èŠ‚
        
        # è½¬å› (B, T, N) æ ¼å¼
        target_bands = {
            'trend': target_trend.permute(0, 2, 1).contiguous(),
            'mid': target_mid.permute(0, 2, 1).contiguous(),
            'detail': target_detail.permute(0, 2, 1).contiguous(),
        }
        
        return target_bands
    
    def forward(self, pred, target, components=None, main_loss=None):
        """
        è®¡ç®—æ·±åº¦ç›‘ç£æŸå¤±
        
        Args:
            pred: (B, T, N) æ¨¡å‹é¢„æµ‹
            target: (B, T, N) Ground Truth
            components: dict, åŒ…å« 'pred_trend', 'pred_mid', 'pred_detail'
            main_loss: é¢„è®¡ç®—çš„ä¸»æŸå¤± (å¯é€‰)
        
        Returns:
            total_loss: æ€»æŸå¤±
            loss_dict: åŒ…å«å„é¡¹æŸå¤±çš„å­—å…¸
        """
        # è®¡ç®—ä¸»æŸå¤±
        if main_loss is None:
            main_loss = F.mse_loss(pred, target)
        
        loss_dict = {'main_loss': main_loss.item()}
        
        # å¦‚æœæ²¡æœ‰æä¾›åˆ†é‡ï¼Œåªè¿”å›ä¸»æŸå¤±
        if components is None:
            return main_loss, loss_dict
        
        # åˆ†è§£ç›®æ ‡
        target_bands = self._decompose_target(target)
        
        # è®¡ç®—è¾…åŠ©æŸå¤±
        loss_trend = F.mse_loss(components['pred_trend'], target_bands['trend'])
        loss_mid = F.mse_loss(components['pred_mid'], target_bands['mid'])
        loss_detail = F.mse_loss(components['pred_detail'], target_bands['detail'])
        
        # æ€»è¾…åŠ©æŸå¤±
        aux_loss = loss_trend + loss_mid + loss_detail
        
        # æ€»æŸå¤±
        total_loss = main_loss + self.alpha * aux_loss
        
        # è®°å½•å„é¡¹æŸå¤±
        loss_dict.update({
            'loss_trend': loss_trend.item(),
            'loss_mid': loss_mid.item(),
            'loss_detail': loss_detail.item(),
            'aux_loss': aux_loss.item(),
            'total_loss': total_loss.item(),
            'alpha': self.alpha,
        })
        
        return total_loss, loss_dict


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    import sys
    import os
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)
    
    print("=" * 70)
    print("FrequencyDecoupledHead æ¨¡å—æµ‹è¯•")
    print("=" * 70)
    
    # è®¾å¤‡é€‰æ‹©
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"\nè®¾å¤‡: {device}")
    
    # æµ‹è¯•å‚æ•°
    B = 4           # Batch size
    N = 7           # å˜é‡æ•°
    d_ff = 32       # FFN ç»´åº¦
    patch_nums = 10 # Patch æ•°é‡
    pred_len = 96   # é¢„æµ‹é•¿åº¦
    
    nf = d_ff * patch_nums  # ç‰¹å¾ç»´åº¦
    
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  - Batch: {B}, Variables: {N}")
    print(f"  - d_ff: {d_ff}, patch_nums: {patch_nums}")
    print(f"  - nf (d_ff * patch_nums): {nf}")
    print(f"  - pred_len: {pred_len}")
    
    # ========== æµ‹è¯• 1: TriBandDecoupledHead ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 1: TriBandDecoupledHead")
    print("=" * 70)
    
    head = TriBandDecoupledHead(
        n_vars=N,
        nf=nf,
        target_window=pred_len,
        head_dropout=0.1,
        mid_dropout=0.2,
        high_dropout=0.5,
        use_soft_threshold=True,
        soft_threshold_init=0.1,
        use_conv=False
    ).to(device)
    
    # æ¨¡æ‹Ÿ LLM è¾“å‡º
    x = torch.randn(B, N, d_ff, patch_nums, device=device)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­ (ä¸è¿”å›åˆ†é‡)
    output = head(x, return_components=False)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert output.shape == (B, pred_len, N), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    print("âœ… TriBandDecoupledHead å‰å‘ä¼ æ’­é€šè¿‡")

    # ========== æµ‹è¯• 2: DualScaleResidualHead ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: DualScaleResidualHead")
    print("=" * 70)
    
    ds_head = DualScaleResidualHead(
        n_vars=N,
        d_ff=d_ff,
        patch_nums=patch_nums,
        target_window=pred_len,
        head_dropout=0.1
    ).to(device)
    
    # æ¨¡æ‹Ÿ LLM è¾“å‡º
    x = torch.randn(B, N, d_ff, patch_nums, device=device)
    
    # å‰å‘ä¼ æ’­
    output = ds_head(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert output.shape == (B, pred_len, N), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    print("âœ… DualScaleResidualHead å‰å‘ä¼ æ’­é€šè¿‡")
    
    # æ£€æŸ¥ä¸¤ä¸ªåˆ†æ”¯çš„æ¢¯åº¦
    ds_head.train()
    x.requires_grad = True
    output = ds_head(x)
    loss = output.mean()
    loss.backward()
    
    print("\næ¢¯åº¦æ£€æŸ¥:")
    if ds_head.head_trend.weight.grad is not None:
         print(f"  - Trend Head Grad: {ds_head.head_trend.weight.grad.norm().item():.6f}")
    if ds_head.head_detail.weight.grad is not None:
         print(f"  - Detail Head Grad: {ds_head.head_detail.weight.grad.norm().item():.6f}")
    
    print("âœ… æ¢¯åº¦ä¼ æ’­æ­£ç¡®")

    # ========== æµ‹è¯•å®Œæˆ ==========
    print("\n" + "=" * 70)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 70)
