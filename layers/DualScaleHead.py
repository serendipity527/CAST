"""
åŒå°ºåº¦æ®‹å·®å¤´ (Dual-Scale Residual Head)

å®ç°ä¸€ä¸ªæ¯” FlattenHead æ›´æœ‰æ•ˆçš„ç®€åŒ–è¾“å‡ºå¤´ï¼š
1. å…¨å±€è¶‹åŠ¿å¤´ï¼šä½¿ç”¨ Global Average Pooling æå–æ•´ä½“è¯­ä¹‰
2. å±€éƒ¨ç»†èŠ‚å¤´ï¼šä½¿ç”¨ Flatten æ“ä½œä¿ç•™æ—¶åºç»†èŠ‚
3. æ®‹å·®èåˆï¼šä¸¤åˆ†æ”¯ç›¸åŠ å¾—åˆ°æœ€ç»ˆé¢„æµ‹

æ ¸å¿ƒç†å¿µï¼š
- æ˜¾å¼åˆ†ç¦»æ•´ä½“è¶‹åŠ¿ä¸å±€éƒ¨ç»†èŠ‚
- åˆ©ç”¨æ®‹å·®å­¦ä¹ è®©æ¨¡å‹æ›´å®¹æ˜“æ”¶æ•›
- è®¡ç®—é‡å‡ ä¹æ— å¢åŠ ï¼Œä½†æ¢¯åº¦ä¼ æ’­æ›´é«˜æ•ˆ

Author: CAST Project
Date: 2024-12-17
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class DualScaleResidualHead(nn.Module):
    """
    åŒå°ºåº¦æ®‹å·®è¾“å‡ºå¤´
    
    æ¶æ„è®¾è®¡:
        LLM Output (B, n_vars, d_ff, patch_nums)
            â”‚
            â”œâ”€â”€â–º Global Average Pooling â”€â”€â–º Linear â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Trend_Pred
            â”‚    (patch_nums ç»´åº¦æ±‚å‡å€¼)     (d_ff -> pred_len)
            â”‚
            â””â”€â”€â–º Flatten â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Linear â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Detail_Pred
                 (ä¿æŒåŸæœ‰æ“ä½œ)              (d_ff*patch_nums -> pred_len)
            â”‚
            â–¼
        Final_Pred = Trend_Pred + Detail_Pred
    
    ç›¸æ¯” FlattenHead çš„ä¼˜åŠ¿:
    1. æ¢¯åº¦é«˜é€Ÿå…¬è·¯ï¼šGAP åˆ†æ”¯å‚æ•°å°‘ï¼Œå¿«é€Ÿæ”¶æ•›åˆ°è¶‹åŠ¿
    2. æ®‹å·®å­¦ä¹ ï¼šDetail åˆ†æ”¯åªéœ€å­¦ä¹ æ³¢åŠ¨éƒ¨åˆ†ï¼Œé™ä½éš¾åº¦
    3. é›¶é¢å¤–ä»£ä»·ï¼šè®¡ç®—é‡å‡ ä¹ä¸å¢åŠ 
    
    Args:
        n_vars: å˜é‡æ•°é‡
        d_ff: FFN ç»´åº¦ 
        patch_nums: Patch æ•°é‡
        target_window: é¢„æµ‹çª—å£é•¿åº¦
        head_dropout: è¾“å‡º Dropout ç‡ (é»˜è®¤ 0.1)
        detail_dropout: å±€éƒ¨ç»†èŠ‚åˆ†æ”¯çš„ Dropout ç‡ (é»˜è®¤ 0.0ï¼Œå¯åç»­è°ƒæ•´)
    """
    
    def __init__(self, n_vars, d_ff, patch_nums, target_window, 
                 head_dropout=0.1, detail_dropout=0.0):
        super(DualScaleResidualHead, self).__init__()
        
        self.n_vars = n_vars
        self.d_ff = d_ff
        self.patch_nums = patch_nums
        self.target_window = target_window
        
        # ========== åˆ†æ”¯ A: å…¨å±€è¶‹åŠ¿å¤´ ==========
        # Global Average Pooling ä¸éœ€è¦å‚æ•°
        self.trend_head = nn.Linear(d_ff, target_window)
        
        # ========== åˆ†æ”¯ B: å±€éƒ¨ç»†èŠ‚å¤´ ==========
        self.flatten = nn.Flatten(start_dim=-2)  # å±•å¹³ (d_ff, patch_nums)
        self.detail_head = nn.Linear(d_ff * patch_nums, target_window)
        
        # ========== æ­£åˆ™åŒ– ==========
        self.detail_dropout = nn.Dropout(detail_dropout) if detail_dropout > 0 else nn.Identity()
        self.output_dropout = nn.Dropout(head_dropout)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
        # æ‰“å°é…ç½®
        self._print_config()
    
    def _init_weights(self):
        """Xavier åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def _print_config(self):
        """æ‰“å°æ¨¡å—é…ç½®"""
        trend_params = sum(p.numel() for p in self.trend_head.parameters())
        detail_params = sum(p.numel() for p in self.detail_head.parameters())
        total_params = trend_params + detail_params
        
        print("=" * 70)
        print("[DualScaleResidualHead] åŒå°ºåº¦æ®‹å·®è¾“å‡ºå¤´å·²å¯ç”¨")
        print("=" * 70)
        print(f"  â”œâ”€ è¾“å…¥å½¢çŠ¶: (B, {self.n_vars}, {self.d_ff}, {self.patch_nums})")
        print(f"  â”œâ”€ é¢„æµ‹çª—å£: {self.target_window}")
        print(f"  â”œâ”€ è¶‹åŠ¿å¤´å‚æ•°: {trend_params:,}")
        print(f"  â”œâ”€ ç»†èŠ‚å¤´å‚æ•°: {detail_params:,}")
        print(f"  â”œâ”€ æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  â”œâ”€ ç»†èŠ‚ Dropout: {self.detail_dropout.p if hasattr(self.detail_dropout, 'p') else 0}")
        print(f"  â””â”€ è¾“å‡º Dropout: {self.output_dropout.p}")
        print("=" * 70)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: LLM è¾“å‡ºï¼Œå½¢çŠ¶ (B, n_vars, d_ff, patch_nums)
        
        Returns:
            output: æœ€ç»ˆé¢„æµ‹ï¼Œå½¢çŠ¶ (B, target_window, n_vars)
        """
        B, N, D, P = x.shape
        
        # ========== åˆ†æ”¯ A: å…¨å±€è¶‹åŠ¿é¢„æµ‹ ==========
        # Global Average Pooling: (B, n_vars, d_ff, patch_nums) -> (B, n_vars, d_ff)
        trend_features = x.mean(dim=-1)  # å¯¹ patch_nums ç»´åº¦æ±‚å¹³å‡
        
        # è¶‹åŠ¿é¢„æµ‹: (B, n_vars, d_ff) -> (B, n_vars, target_window)
        trend_pred = self.trend_head(trend_features)
        
        # ========== åˆ†æ”¯ B: å±€éƒ¨ç»†èŠ‚é¢„æµ‹ ==========
        # Flatten: (B, n_vars, d_ff, patch_nums) -> (B, n_vars, d_ff * patch_nums)
        detail_features = self.flatten(x)
        
        # æ·»åŠ æ­£åˆ™åŒ–
        detail_features = self.detail_dropout(detail_features)
        
        # ç»†èŠ‚é¢„æµ‹: (B, n_vars, d_ff * patch_nums) -> (B, n_vars, target_window)
        detail_pred = self.detail_head(detail_features)
        
        # ========== æ®‹å·®èåˆ ==========
        # ä¸¤åˆ†æ”¯ç›¸åŠ : (B, n_vars, target_window)
        final_pred = trend_pred + detail_pred
        
        # è¾“å‡º Dropout
        final_pred = self.output_dropout(final_pred)
        
        # è½¬æ¢ä¸ºæ ‡å‡†è¾“å‡ºæ ¼å¼: (B, n_vars, target_window) -> (B, target_window, n_vars)
        final_pred = final_pred.permute(0, 2, 1).contiguous()
        
        return final_pred
    
    def get_components(self, x):
        """
        è·å–ä¸¤ä¸ªåˆ†æ”¯çš„ç‹¬ç«‹é¢„æµ‹ (ç”¨äºåˆ†æå’Œè°ƒè¯•)
        
        Args:
            x: LLM è¾“å‡ºï¼Œå½¢çŠ¶ (B, n_vars, d_ff, patch_nums)
        
        Returns:
            components: å­—å…¸ï¼ŒåŒ…å« trend_pred, detail_pred, final_pred
        """
        B, N, D, P = x.shape
        
        # è¶‹åŠ¿åˆ†æ”¯
        trend_features = x.mean(dim=-1)
        trend_pred = self.trend_head(trend_features)
        
        # ç»†èŠ‚åˆ†æ”¯
        detail_features = self.flatten(x)
        detail_features = self.detail_dropout(detail_features)
        detail_pred = self.detail_head(detail_features)
        
        # æœ€ç»ˆé¢„æµ‹
        final_pred = trend_pred + detail_pred
        final_pred = self.output_dropout(final_pred)
        
        # è½¬æ¢æ ¼å¼
        components = {
            'trend_pred': trend_pred.permute(0, 2, 1).contiguous(),
            'detail_pred': detail_pred.permute(0, 2, 1).contiguous(),
            'final_pred': final_pred.permute(0, 2, 1).contiguous(),
        }
        
        return components


class FlattenHead(nn.Module):
    """
    åŸç‰ˆ FlattenHead (ç”¨äºå¯¹æ¯”æµ‹è¯•)
    """
    
    def __init__(self, n_vars, d_ff, patch_nums, target_window, head_dropout=0.1):
        super(FlattenHead, self).__init__()
        
        self.n_vars = n_vars
        self.d_ff = d_ff
        self.patch_nums = patch_nums
        self.target_window = target_window
        
        self.flatten = nn.Flatten(start_dim=-2)
        self.projection = nn.Linear(d_ff * patch_nums, target_window)
        self.dropout = nn.Dropout(head_dropout)
        
        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.projection.weight)
        if self.projection.bias is not None:
            nn.init.zeros_(self.projection.bias)
        
        print(f"[FlattenHead] å‚æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, x):
        # (B, n_vars, d_ff, patch_nums) -> (B, n_vars, d_ff * patch_nums)
        x = self.flatten(x)
        
        # (B, n_vars, d_ff * patch_nums) -> (B, n_vars, target_window)
        x = self.projection(x)
        x = self.dropout(x)
        
        # (B, n_vars, target_window) -> (B, target_window, n_vars)
        x = x.permute(0, 2, 1).contiguous()
        
        return x


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("DualScaleResidualHead æµ‹è¯•")
    print("=" * 70)
    
    # è®¾å¤‡é€‰æ‹©
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"\nè®¾å¤‡: {device}")
    
    # æµ‹è¯•å‚æ•°
    B = 8           # Batch size
    N = 7           # å˜é‡æ•° (ETTh1)
    d_ff = 32       # FFN ç»´åº¦
    patch_nums = 10 # Patch æ•°é‡
    pred_len = 96   # é¢„æµ‹é•¿åº¦
    
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  - Batch: {B}, Variables: {N}")
    print(f"  - d_ff: {d_ff}, patch_nums: {patch_nums}")
    print(f"  - pred_len: {pred_len}")
    
    # ========== æµ‹è¯• 1: åŸºæœ¬åŠŸèƒ½å¯¹æ¯” ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 1: åŸºæœ¬åŠŸèƒ½å¯¹æ¯”")
    print("=" * 70)
    
    # åˆ›å»ºä¸¤ä¸ª Head
    dual_head = DualScaleResidualHead(
        n_vars=N, d_ff=d_ff, patch_nums=patch_nums, 
        target_window=pred_len, head_dropout=0.1, detail_dropout=0.0
    ).to(device)
    
    flatten_head = FlattenHead(
        n_vars=N, d_ff=d_ff, patch_nums=patch_nums,
        target_window=pred_len, head_dropout=0.1
    ).to(device)
    
    # æ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(B, N, d_ff, patch_nums, device=device)
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    dual_output = dual_head(x)
    flatten_output = flatten_head(x)
    
    print(f"DualScale è¾“å‡ºå½¢çŠ¶: {dual_output.shape}")
    print(f"FlattenHead è¾“å‡ºå½¢çŠ¶: {flatten_output.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (B, pred_len, N)
    assert dual_output.shape == expected_shape, f"DualScale è¾“å‡ºå½¢çŠ¶é”™è¯¯: {dual_output.shape}"
    assert flatten_output.shape == expected_shape, f"FlattenHead è¾“å‡ºå½¢çŠ¶é”™è¯¯: {flatten_output.shape}"
    print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
    
    # ========== æµ‹è¯• 2: åˆ†é‡åˆ†æ ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: åˆ†é‡åˆ†æ")
    print("=" * 70)
    
    dual_head.eval()
    with torch.no_grad():
        components = dual_head.get_components(x)
    
    print(f"è¶‹åŠ¿åˆ†é‡å½¢çŠ¶: {components['trend_pred'].shape}")
    print(f"ç»†èŠ‚åˆ†é‡å½¢çŠ¶: {components['detail_pred'].shape}")
    print(f"æœ€ç»ˆé¢„æµ‹å½¢çŠ¶: {components['final_pred'].shape}")
    
    # éªŒè¯åˆ†é‡ç›¸åŠ ç­‰äºæœ€ç»ˆè¾“å‡º (eval æ¨¡å¼ä¸‹åº”è¯¥å®Œå…¨ä¸€è‡´)
    reconstructed = components['trend_pred'] + components['detail_pred']
    diff = (components['final_pred'] - reconstructed).abs().max().item()
    print(f"\nåˆ†é‡ç›¸åŠ  vs æœ€ç»ˆè¾“å‡º å·®å¼‚: {diff:.10f}")
    assert diff < 1e-5, "åˆ†é‡é‡æ„ä¸ä¸€è‡´!"
    print("âœ… åˆ†é‡é‡æ„æ­£ç¡®")
    
    # åˆ†æåˆ†é‡çš„ç»Ÿè®¡ç‰¹æ€§
    trend_std = components['trend_pred'].std().item()
    detail_std = components['detail_pred'].std().item()
    print(f"\nè¶‹åŠ¿åˆ†é‡æ ‡å‡†å·®: {trend_std:.6f}")
    print(f"ç»†èŠ‚åˆ†é‡æ ‡å‡†å·®: {detail_std:.6f}")
    print("âœ… åˆ†é‡åˆ†æå®Œæˆ")
    
    # ========== æµ‹è¯• 3: å‚æ•°é‡å¯¹æ¯” ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: å‚æ•°é‡å¯¹æ¯”")
    print("=" * 70)
    
    dual_params = sum(p.numel() for p in dual_head.parameters())
    flatten_params = sum(p.numel() for p in flatten_head.parameters())
    
    print(f"DualScale å‚æ•°é‡: {dual_params:,}")
    print(f"FlattenHead å‚æ•°é‡: {flatten_params:,}")
    print(f"å‚æ•°å¢åŠ æ¯”ä¾‹: {(dual_params / flatten_params - 1) * 100:.2f}%")
    
    # ç†è®ºä¸Šï¼ŒDualScale çš„å‚æ•°åº”è¯¥ç•¥å¤šäº FlattenHead
    # å› ä¸ºå®ƒæœ‰ä¸¤ä¸ª Linear å±‚ï¼Œè€Œ FlattenHead åªæœ‰ä¸€ä¸ª
    expected_dual_params = (d_ff * pred_len + pred_len) + (d_ff * patch_nums * pred_len + pred_len)
    expected_flatten_params = d_ff * patch_nums * pred_len + pred_len
    
    print(f"\nç†è®º DualScale å‚æ•°: {expected_dual_params * N:,}")
    print(f"ç†è®º FlattenHead å‚æ•°: {expected_flatten_params * N:,}")
    print("âœ… å‚æ•°é‡ç»Ÿè®¡æ­£ç¡®")
    
    # ========== æµ‹è¯• 4: æ¢¯åº¦ä¼ æ’­ ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 4: æ¢¯åº¦ä¼ æ’­")
    print("=" * 70)
    
    dual_head.train()
    flatten_head.train()
    
    # éœ€è¦æ¢¯åº¦çš„è¾“å…¥
    x_dual = torch.randn(B, N, d_ff, patch_nums, device=device, requires_grad=True)
    x_flatten = x_dual.clone().detach().requires_grad_(True)
    
    # æ¨¡æ‹Ÿç›®æ ‡
    target = torch.randn(B, pred_len, N, device=device)
    
    # å‰å‘ + åå‘ä¼ æ’­
    dual_loss = F.mse_loss(dual_head(x_dual), target)
    flatten_loss = F.mse_loss(flatten_head(x_flatten), target)
    
    dual_loss.backward()
    flatten_loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    dual_grad_norm = x_dual.grad.norm().item()
    flatten_grad_norm = x_flatten.grad.norm().item()
    
    print(f"DualScale è¾“å…¥æ¢¯åº¦èŒƒæ•°: {dual_grad_norm:.6f}")
    print(f"FlattenHead è¾“å…¥æ¢¯åº¦èŒƒæ•°: {flatten_grad_norm:.6f}")
    
    assert dual_grad_norm > 0, "DualScale æ¢¯åº¦ä¸ºé›¶"
    assert flatten_grad_norm > 0, "FlattenHead æ¢¯åº¦ä¸ºé›¶"
    print("âœ… æ¢¯åº¦ä¼ æ’­æ­£ç¡®")
    
    # ========== æµ‹è¯• 5: æ”¶æ•›æ€§èƒ½æ¨¡æ‹Ÿ ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 5: æ”¶æ•›æ€§èƒ½æ¨¡æ‹Ÿ")
    print("=" * 70)
    
    # æ¨¡æ‹Ÿç®€å•çš„æ”¶æ•›æµ‹è¯•
    dual_head.train()
    flatten_head.train()
    
    # ä¼˜åŒ–å™¨
    dual_optim = torch.optim.Adam(dual_head.parameters(), lr=0.001)
    flatten_optim = torch.optim.Adam(flatten_head.parameters(), lr=0.001)
    
    # æ¨¡æ‹Ÿæ•°æ® (è®©è¶‹åŠ¿æ›´æ˜æ˜¾)
    torch.manual_seed(42)
    n_samples = 100
    
    dual_losses = []
    flatten_losses = []
    
    for step in range(n_samples):
        # ç”Ÿæˆæœ‰è¶‹åŠ¿çš„æ•°æ®
        x_batch = torch.randn(4, N, d_ff, patch_nums, device=device)
        trend = torch.linspace(-1, 1, pred_len, device=device).unsqueeze(0).unsqueeze(-1).repeat(4, 1, N)
        noise = 0.1 * torch.randn(4, pred_len, N, device=device)
        y_batch = trend + noise
        
        # DualScale è®­ç»ƒ
        dual_optim.zero_grad()
        dual_pred = dual_head(x_batch)
        dual_loss = F.mse_loss(dual_pred, y_batch)
        dual_loss.backward()
        dual_optim.step()
        dual_losses.append(dual_loss.item())
        
        # FlattenHead è®­ç»ƒ
        flatten_optim.zero_grad()
        flatten_pred = flatten_head(x_batch)
        flatten_loss = F.mse_loss(flatten_pred, y_batch)
        flatten_loss.backward()
        flatten_optim.step()
        flatten_losses.append(flatten_loss.item())
    
    # æ¯”è¾ƒæœ€ç»ˆæŸå¤±
    dual_final_loss = sum(dual_losses[-10:]) / 10
    flatten_final_loss = sum(flatten_losses[-10:]) / 10
    
    print(f"DualScale å¹³å‡æŸå¤± (æœ€å10æ­¥): {dual_final_loss:.6f}")
    print(f"FlattenHead å¹³å‡æŸå¤± (æœ€å10æ­¥): {flatten_final_loss:.6f}")
    print(f"ç›¸å¯¹æ”¹è¿›: {(flatten_final_loss - dual_final_loss) / flatten_final_loss * 100:.2f}%")
    print("âœ… æ”¶æ•›æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    # ========== æµ‹è¯• 6: è¾¹ç•Œæƒ…å†µ ==========
    print("\n" + "=" * 70)
    print("æµ‹è¯• 6: è¾¹ç•Œæƒ…å†µ")
    print("=" * 70)
    
    # æµ‹è¯•ä¸åŒçš„è¾“å…¥å°ºå¯¸
    test_configs = [
        (1, 1, 8, 5, 24),   # æœ€å°é…ç½®
        (2, 3, 16, 8, 48),  # ä¸­ç­‰é…ç½®
        (4, 12, 64, 20, 192) # å¤§é…ç½®
    ]
    
    for i, (b, n, d, p, pred) in enumerate(test_configs):
        print(f"\né…ç½® {i+1}: B={b}, N={n}, d_ff={d}, patch_nums={p}, pred_len={pred}")
        
        test_head = DualScaleResidualHead(n, d, p, pred).to(device)
        test_input = torch.randn(b, n, d, p, device=device)
        test_output = test_head(test_input)
        
        expected_shape = (b, pred, n)
        assert test_output.shape == expected_shape, f"é…ç½® {i+1} è¾“å‡ºå½¢çŠ¶é”™è¯¯"
        print(f"  âœ… è¾“å‡ºå½¢çŠ¶: {test_output.shape}")
    
    print("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")
    
    # ========== æµ‹è¯•å®Œæˆ ==========
    print("\n" + "=" * 70)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 70)
    print("\næ€»ç»“:")
    print(f"  - DualScaleResidualHead å®ç°æ­£ç¡®")
    print(f"  - å‚æ•°é‡é€‚ä¸­ï¼Œæ¯” FlattenHead ç•¥å¤šä½†å¯æ§")
    print(f"  - æ¢¯åº¦ä¼ æ’­æ­£å¸¸ï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ")
    print(f"  - åˆ†é‡åˆ†æåŠŸèƒ½å®Œå–„ï¼Œä¾¿äºè°ƒè¯•")
    print(f"  - åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½")
    print("=" * 70)
