




根据我对代码的分析，以下是 TimeLLM + WIST-PE 的完整架构图：

```mermaid
flowchart TB
    subgraph Input["输入层"]
        X["时间序列 x_enc<br/>(B, T, N)"]
    end

    subgraph Normalize["归一化"]
        NORM["Normalize<br/>(StandardNorm.py)"]
    end

    subgraph WIST_PE["WIST-PE 模块 (Embed.py)"]
        direction TB
        
        subgraph CausalSWT["因果小波分解 (CausalWavelet.py)"]
            SWT["CausalSWT<br/>wavelet=db4, level=2"]
            SWT --> COEFFS["小波系数<br/>[cA, cD_n, ..., cD_1]"]
        end
        
        subgraph Patching["Patch 切分"]
            PAD["ReplicationPad1d"]
            UNFOLD["unfold<br/>(patch_len, stride)"]
        end
        
        subgraph BandProcessing["频段处理"]
            direction LR
            subgraph LowFreq["低频通道 (cA)"]
                LF_PROJ["CausalConv1d<br/>投影"]
            end
            subgraph MidFreq["中频通道 (cD_n)"]
                MF_THRESH["SoftThreshold<br/>去噪"]
                MF_PROJ["CausalConv1d<br/>投影"]
                MF_DROP["Dropout<br/>(mf_dropout)"]
            end
            subgraph HighFreq["高频通道 (cD_1)"]
                HF_THRESH["SoftThreshold<br/>去噪"]
                HF_PROJ["CausalConv1d<br/>投影"]
                HF_DROP["Dropout<br/>(hf_dropout)"]
            end
        end
        
        subgraph PyramidFusion["金字塔融合"]
            GATE1["Gate Layer 1<br/>cD_1 + cD_2"]
            GATE2["Gate Layer 2<br/>D_fused + cA"]
            FUSED["融合输出<br/>(B*N, num_patches, d_model)"]
        end
    end

    subgraph LLM_Reprogram["LLM 重编程层"]
        direction TB
        
        subgraph PromptGen["Prompt 生成"]
            STATS["统计特征提取<br/>(min, max, median, trend, lags)"]
            TOKENIZE["Tokenizer<br/>(GPT2/LLAMA/BERT)"]
            PROMPT_EMB["Prompt Embeddings"]
        end
        
        subgraph Reprogram["ReprogrammingLayer"]
            WORD_EMB["Word Embeddings<br/>(vocab_size, d_llm)"]
            MAPPING["Mapping Layer<br/>(vocab → num_tokens)"]
            SOURCE_EMB["Source Embeddings"]
            
            Q_PROJ["Query Projection"]
            K_PROJ["Key Projection"]
            V_PROJ["Value Projection"]
            ATTN["Cross-Attention<br/>Reprogramming"]
            OUT_PROJ["Output Projection"]
        end
    end

    subgraph LLM["预训练 LLM (冻结)"]
        GPT2["GPT2Model / LlamaModel / BertModel<br/>(llm_layers 层)"]
    end

    subgraph OutputHead["输出头"]
        FLATTEN["FlattenHead"]
        LINEAR["Linear<br/>(head_nf → pred_len)"]
        DENORM["Denormalize"]
    end

    subgraph Output["输出"]
        PRED["预测结果<br/>(B, pred_len, N)"]
    end

    %% 数据流连接
    X --> NORM
    NORM --> SWT
    COEFFS --> PAD --> UNFOLD
    
    UNFOLD --> LF_PROJ
    UNFOLD --> MF_THRESH --> MF_PROJ --> MF_DROP
    UNFOLD --> HF_THRESH --> HF_PROJ --> HF_DROP
    
    HF_DROP --> GATE1
    MF_DROP --> GATE1
    GATE1 --> GATE2
    LF_PROJ --> GATE2
    GATE2 --> FUSED
    
    NORM --> STATS --> TOKENIZE --> PROMPT_EMB
    
    WORD_EMB --> MAPPING --> SOURCE_EMB
    FUSED --> Q_PROJ
    SOURCE_EMB --> K_PROJ
    SOURCE_EMB --> V_PROJ
    Q_PROJ --> ATTN
    K_PROJ --> ATTN
    V_PROJ --> ATTN
    ATTN --> OUT_PROJ
    
    PROMPT_EMB --> GPT2
    OUT_PROJ --> GPT2
    
    GPT2 --> FLATTEN --> LINEAR --> DENORM --> PRED

    %% 样式
    classDef wavelet fill:#e1f5fe,stroke:#01579b
    classDef llm fill:#fff3e0,stroke:#e65100
    classDef gate fill:#f3e5f5,stroke:#7b1fa2
    classDef output fill:#e8f5e9,stroke:#2e7d32
    
    class SWT,COEFFS wavelet
    class GPT2 llm
    class GATE1,GATE2 gate
    class PRED output
```

---

## 架构说明

### 1. **WIST-PE 模块** ([layers/Embed.py](cci:7://file:///home/dmx_MT/LZF/project/CAST/layers/Embed.py:0:0-0:0))
核心创新模块，包含：
- **CausalSWT** ([layers/CausalWavelet.py](cci:7://file:///home/dmx_MT/LZF/project/CAST/layers/CausalWavelet.py:0:0-0:0)): 因果平稳小波变换，支持 db1~db5 小波基
- **多频段处理**: 每个频段独立投影 + 去噪 + Dropout
- **金字塔融合**: 从高频到低频逐级门控融合

### 2. **ReprogrammingLayer** ([models/TimeLLM.py](cci:7://file:///home/dmx_MT/LZF/project/CAST/models/TimeLLM.py:0:0-0:0))
将时序 Patch Embedding 重编程为 LLM 可理解的表示：
- Query: 来自 WIST-PE 的 Patch Embedding
- Key/Value: 来自 LLM 词表的 Source Embedding
- 通过 Cross-Attention 实现时序→语言的跨模态对齐

### 3. **预训练 LLM** (冻结)
支持 GPT2 / LLAMA / BERT，参数冻结，仅作为特征提取器

### 4. **FlattenHead** ([models/TimeLLM.py](cci:7://file:///home/dmx_MT/LZF/project/CAST/models/TimeLLM.py:0:0-0:0))
将 LLM 输出展平并投影到预测长度