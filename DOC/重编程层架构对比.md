# TimeLLM 重编程层架构对比

## 1. 原版 TimeLLM 重编程层架构

```mermaid
graph TB
    subgraph "输入层"
        A[Word Embeddings<br/>vocab_size × d_llm<br/>例如: 50257 × 4096]
    end
    
    subgraph "映射层 (Mapping Layer)"
        B[Permute<br/>转置: 50257 × 4096 → 4096 × 50257]
        C[Linear Layer<br/>Linear(50257, 1000)<br/>参数量: ~50M]
        D[Permute<br/>转置: 4096 × 1000 → 1000 × 4096]
        E[Source Embeddings<br/>1000 × d_llm<br/>原型库]
    end
    
    subgraph "Patch Embedding"
        F[时序特征<br/>B × L × d_model]
    end
    
    subgraph "重编程层 (ReprogrammingLayer)"
        G[Query Projection<br/>Linear(d_model → d_keys × n_heads)]
        H[Key Projection<br/>Linear(d_llm → d_keys × n_heads)]
        I[Value Projection<br/>Linear(d_llm → d_keys × n_heads)]
        J[Cross-Attention<br/>Q × K^T → Softmax → × V]
        K[Output Projection<br/>Linear(d_keys × n_heads → d_llm)]
        L[语义空间表示<br/>B × L × d_llm]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    
    F --> G
    E --> H
    E --> I
    
    G --> J
    H --> J
    I --> J
    J --> K
    K --> L
    
    style A fill:#e1f5ff
    style E fill:#fff4e1
    style L fill:#e8f5e9
    style C fill:#ffebee
```

### 原版架构特点：
- **映射层**: `Linear(vocab_size, 1000)` - 从整个词表（50257个词）映射到1000个原型
- **参数量**: ~50M（50257 × 1000）
- **输入空间**: 整个词表，信息丰富但包含噪声
- **映射方式**: 线性变换（旋转+缩放）
- **原型库**: 单一共享原型库（1000个）

---

## 2. 新实现：MLP映射层 + 分离原型架构

```mermaid
graph TB
    subgraph "种子词筛选层"
        A1[Word Embeddings<br/>vocab_size × d_llm<br/>例如: 50257 × 4096]
        A2[语义筛选<br/>select_seed_words]
        A3[Trend Seed Embeddings<br/>1000 × d_llm<br/>Buffer, 不更新]
        A4[Detail Seed Embeddings<br/>1000 × d_llm<br/>Buffer, 不更新]
    end
    
    subgraph "MLP映射层 (策略一)"
        B1[Trend: Permute<br/>转置: 1000 × 4096 → 4096 × 1000]
        B2[Trend: MLP<br/>Linear(1000 → 4096)<br/>GELU + Dropout<br/>Linear(4096 → 1000)]
        B3[Trend: Permute<br/>转置: 4096 × 1000 → 1000 × 4096]
        B4[Trend Prototypes<br/>1000 × d_llm]
        
        C1[Detail: Permute<br/>转置: 1000 × 4096 → 4096 × 1000]
        C2[Detail: MLP<br/>Linear(1000 → 4096)<br/>GELU + Dropout<br/>Linear(4096 → 1000)]
        C3[Detail: Permute<br/>转置: 4096 × 1000 → 1000 × 4096]
        C4[Detail Prototypes<br/>1000 × d_llm]
    end
    
    subgraph "WIST Patch Embedding"
        D1[时序输入<br/>B × T × N]
        D2[WIST分解]
        D3[Trend Features e_cA<br/>B × L × d_model]
        D4[Detail Features e_detail<br/>B × L × d_model]
    end
    
    subgraph "双流重编程层 (DualReprogrammingLayer)"
        E1[Trend Reprogramming<br/>Cross-Attention<br/>e_cA × Trend Prototypes]
        E2[Detail Reprogramming<br/>Cross-Attention<br/>e_detail × Detail Prototypes]
        E3[Trend Semantics<br/>B × L × d_llm]
        E4[Detail Semantics<br/>B × L × d_llm]
        E5[融合策略<br/>Weighted Fusion<br/>weight × Trend + (1-weight) × Detail]
        E6[最终语义表示<br/>B × L × d_llm]
    end
    
    A1 --> A2
    A2 --> A3
    A2 --> A4
    
    A3 --> B1
    B1 --> B2
    B2 --> B3
    B3 --> B4
    
    A4 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4
    
    D1 --> D2
    D2 --> D3
    D2 --> D4
    
    D3 --> E1
    B4 --> E1
    E1 --> E3
    
    D4 --> E2
    C4 --> E2
    E2 --> E4
    
    E3 --> E5
    E4 --> E5
    E5 --> E6
    
    style A3 fill:#e1f5ff
    style A4 fill:#e1f5ff
    style B4 fill:#fff4e1
    style C4 fill:#fff4e1
    style E6 fill:#e8f5e9
    style B2 fill:#ffebee
    style C2 fill:#ffebee
```

### 新架构特点：
- **种子词筛选**: 从50257个词中筛选出1000个趋势词 + 1000个细节词（语义相关）
- **MLP映射层**: `MLP(1000 → 4096 → 1000)` - 非线性映射，参数量 ~8.2M
- **输入空间**: 精选的种子词（1000个），信息密度高但可能遗漏
- **映射方式**: 非线性变换（升维 → GELU激活 → 降维）
- **原型库**: 分离的双原型库（趋势1000个 + 细节1000个）
- **重编程层**: 双流架构，分别处理趋势和细节特征

---

## 3. 关键差异对比

| 特性 | 原版 TimeLLM | 新实现 (MLP) |
|------|-------------|-------------|
| **输入词表** | 整个词表 (50257个) | 精选种子词 (1000+1000个) |
| **映射层类型** | Linear(50257, 1000) | MLP(1000 → 4096 → 1000) |
| **参数量** | ~50M | ~8.2M × 2 = ~16.4M |
| **映射方式** | 线性变换 | 非线性变换 (GELU) |
| **原型库数量** | 1个 (共享) | 2个 (趋势+细节) |
| **信息空间** | 大但含噪声 | 小但精选 |
| **表达能力** | 高（过参数化） | 中等（需MLP增强） |
| **语义先验** | 无 | 有（种子词筛选） |

---

## 4. 数据流对比

### 原版数据流：
```
Word Embeddings (50257, 4096)
  → Permute (4096, 50257)
  → Linear(50257, 1000) [线性映射]
  → Permute (1000, 4096)
  → Source Embeddings (1000, 4096)
  → ReprogrammingLayer
  → 语义表示
```

### 新实现数据流：
```
Word Embeddings (50257, 4096)
  → 语义筛选 → Trend Seeds (1000, 4096) + Detail Seeds (1000, 4096)
  → Permute (4096, 1000)
  → MLP(1000 → 4096 → 1000) [非线性映射]
  → Permute (1000, 4096)
  → Trend Prototypes (1000, 4096) + Detail Prototypes (1000, 4096)
  → DualReprogrammingLayer (双流)
  → 融合 → 语义表示
```

---

## 5. MLP映射层详细结构

```mermaid
graph LR
    subgraph "MLP映射层内部结构"
        A[输入<br/>d_llm × num_seeds<br/>例如: 4096 × 1000]
        B[Linear Layer 1<br/>Linear(1000, 4096)<br/>参数量: 4.1M]
        C[GELU激活<br/>非线性变换]
        D[Dropout<br/>p=0.1]
        E[Linear Layer 2<br/>Linear(4096, 1000)<br/>参数量: 4.1M]
        F[输出<br/>d_llm × num_prototypes<br/>例如: 4096 × 1000]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    
    style B fill:#ffebee
    style C fill:#fff4e1
    style D fill:#e1f5ff
    style E fill:#ffebee
```

### MLP优势：
1. **非线性映射**: GELU激活允许学习复杂的语义空间变换
2. **信息展开**: 4096维中间层提供更大的表示容量
3. **特征交互**: 高维空间中种子词特征可以充分交互
4. **正则化**: Dropout防止过拟合

---

## 6. 重编程层内部机制

```mermaid
graph TB
    subgraph "ReprogrammingLayer (原版)"
        A1[Target Embedding<br/>B × L × d_model<br/>时序特征]
        A2[Source Embedding<br/>S × d_llm<br/>原型库]
        A3[Query: Linear(d_model → d_k×H)]
        A4[Key: Linear(d_llm → d_k×H)]
        A5[Value: Linear(d_llm → d_k×H)]
        A6[Attention Scores<br/>Q × K^T / √d_k]
        A7[Softmax + Dropout]
        A8[Weighted Sum<br/>Attention × V]
        A9[Output: Linear(d_k×H → d_llm)]
        A10[语义表示<br/>B × L × d_llm]
    end
    
    A1 --> A3
    A2 --> A4
    A2 --> A5
    A3 --> A6
    A4 --> A6
    A6 --> A7
    A7 --> A8
    A5 --> A8
    A8 --> A9
    A9 --> A10
    
    style A10 fill:#e8f5e9
```

---

## 总结

**原版TimeLLM**的优势在于过参数化带来的强大表达能力，能够从整个词表中自由组合原型。

**新实现**通过MLP非线性映射和语义筛选，在保留语义先验的同时，增强了从文本语义空间到时序语义空间的映射能力。虽然参数量减少了，但通过非线性变换和双流架构，仍然保持了较强的表达能力。

