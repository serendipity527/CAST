nohup: 忽略输入
/home/dmx_MT/.conda/envs/timellm/lib/python3.11/site-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[CausalSWT] 创建因果平稳小波变换
  - 小波类型: haar
  - 滤波器长度: 2
  - 分解层数: 2
  - 输出频段数: 3
  - 特性: 严格因果（仅使用过去数据）
======================================================================
[WIST-PE] Wavelet-Informed Spatio-Temporal Patch Embedding 已启用
======================================================================
  ├─ 小波基类型: haar
  ├─ 分解层数: 2
  ├─ 频段数量: 3 (1个低频 + 2个高频)
  ├─ Patch 长度: 16
  ├─ Stride: 8
  ├─ 输出维度: 64
  ├─ 频段 Embedding: ❌ 未启用
  ├─ 投影方式: ✅ 因果卷积 (CausalConv1d, kernel=3)
  ├─ 融合模式: ✅ 分层金字塔融合 (Pyramid Fusion)
  │   ├─ 中频 Dropout: p=0.2
  │   ├─ 高频 Dropout: p=0.2
  │   └─ 融合顺序: cD_1 → cD_2 → ... → cD_n → cA
  ├─ 高频融合机制: ✅ 频率注意力V1 (用于forward_separated/CWPR)
  │   └─ 仅融合高频频段 [cD_n, ..., cD_1]，低频cA单独输出
  ├─ 全频段融合机制: 门控融合 (Gate Fusion)
  ├─ 门控初始化: bias=2.0 (低频≈88%)
  ├─ 软阈值去噪: ✅ 启用 (可学习阈值)
  ├─ 位置编码: ❌ 关闭
  └─ 特性: 全局因果小波分解 + 差异化处理 + 金字塔融合
======================================================================
[TimeLLM] 使用 WISTPatchEmbedding (WIST-PE 全局因果小波方案)
[TimeLLM] ✅ 启用分离原型模式: 500 趋势 + 500 细节
[TimeLLM] 使用 DualReprogrammingLayer (分离原型: 500+500, 融合方法=weighted)
[TimeLLM] 使用 FlattenHead (原版输出头)
[TimeLLM] 使用原版Prompt（无小波特征）
[2025-12-21 23:32:43,635] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-21 23:32:43,993] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2025-12-21 23:32:43,994] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-12-21 23:32:43,994] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-12-21 23:32:44,765] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.23.21.220, master_port=29500
[2025-12-21 23:32:44,765] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-12-21 23:32:47,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-21 23:32:47,768] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-21 23:32:47,768] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-21 23:32:47,770] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2025-12-21 23:32:47,770] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2025-12-21 23:32:47,770] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-12-21 23:32:47,770] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-12-21 23:32:47,770] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-12-21 23:32:47,770] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-12-21 23:32:47,770] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-12-21 23:32:48,799] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2025-12-21 23:32:48,802] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.56 GB         CA 0.58 GB         Max_CA 1 GB 
[2025-12-21 23:32:48,803] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 100.61 GB, percent = 20.0%
[2025-12-21 23:32:48,999] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2025-12-21 23:32:49,000] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.66 GB         CA 0.78 GB         Max_CA 1 GB 
[2025-12-21 23:32:49,000] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 100.64 GB, percent = 20.0%
[2025-12-21 23:32:49,000] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2025-12-21 23:32:49,181] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2025-12-21 23:32:49,183] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.78 GB         Max_CA 1 GB 
[2025-12-21 23:32:49,183] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 100.61 GB, percent = 20.0%
[2025-12-21 23:32:49,185] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2025-12-21 23:32:49,185] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-12-21 23:32:49,185] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-21 23:32:49,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]
[2025-12-21 23:32:49,186] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-12-21 23:32:49,186] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-21 23:32:49,186] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f548074a590>
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-12-21 23:32:49,187] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-21 23:32:49,188] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   train_batch_size ............. 24
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   world_size ................... 1
[2025-12-21 23:32:49,189] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2025-12-21 23:32:49,190] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-12-21 23:32:49,190] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2025-12-21 23:32:49,190] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-21 23:32:49,190] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2025-12-21 23:32:49,190] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 24, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}

1it [00:01,  1.92s/it]
	iters: 100, epoch: 1 | loss: 0.5429873
	speed: 0.1857s/iter; left time: 6504.8311s
	iters: 200, epoch: 1 | loss: 0.8860074
	speed: 0.1077s/iter; left time: 3760.4588s
	iters: 300, epoch: 1 | loss: 0.6441253
	speed: 0.1043s/iter; left time: 3631.1520s
	iters: 400, epoch: 1 | loss: 0.7115851
	speed: 0.1042s/iter; left time: 3619.6947s
	iters: 500, epoch: 1 | loss: 0.4853480
	speed: 0.1015s/iter; left time: 3515.2619s
	iters: 600, epoch: 1 | loss: 0.6368732
	speed: 0.1072s/iter; left time: 3701.1383s
	iters: 700, epoch: 1 | loss: 0.5736238
	speed: 0.1096s/iter; left time: 3772.0094s
	iters: 800, epoch: 1 | loss: 0.4431008
	speed: 0.1083s/iter; left time: 3718.1975s
	iters: 900, epoch: 1 | loss: 0.4342571
	speed: 0.1060s/iter; left time: 3627.2192s
	iters: 1000, epoch: 1 | loss: 0.5260684
	speed: 0.1053s/iter; left time: 3593.5070s
	iters: 1100, epoch: 1 | loss: 0.9053187
	speed: 0.1066s/iter; left time: 3627.8532s
	iters: 1200, epoch: 1 | loss: 0.4397551
	speed: 0.1116s/iter; left time: 3786.0039s
	iters: 1300, epoch: 1 | loss: 0.4548664
	speed: 0.1071s/iter; left time: 3623.0496s
	iters: 1400, epoch: 1 | loss: 0.4887290
	speed: 0.1081s/iter; left time: 3645.5683s
	iters: 1500, epoch: 1 | loss: 0.3556273
	speed: 0.0973s/iter; left time: 3273.5925s
	iters: 1600, epoch: 1 | loss: 0.4137090
	speed: 0.1060s/iter; left time: 3554.8408s
	iters: 1700, epoch: 1 | loss: 0.4827503
	speed: 0.1140s/iter; left time: 3810.8331s
	iters: 1800, epoch: 1 | loss: 0.2864694
	speed: 0.1124s/iter; left time: 3746.7823s
	iters: 1900, epoch: 1 | loss: 0.3011441
	speed: 0.1045s/iter; left time: 3473.6182s
	iters: 2000, epoch: 1 | loss: 0.5703349
	speed: 0.1146s/iter; left time: 3796.3217s
	iters: 2100, epoch: 1 | loss: 0.3994934
	speed: 0.1039s/iter; left time: 3431.2418s
	iters: 2200, epoch: 1 | loss: 0.6386624
	speed: 0.1063s/iter; left time: 3500.2573s
	iters: 2300, epoch: 1 | loss: 0.4481622
	speed: 0.1069s/iter; left time: 3508.4542s
Epoch: 1 cost time: 252.45513105392456

1it [00:01,  1.21s/it]

Epoch: 1 | Train Loss: 0.5015773 Vali Loss: 0.7578538 Test Loss: 0.4070310 MAE Loss: 0.4286780
lr = 0.0000040000
Updating learning rate to 4.000000000000002e-06

	iters: 100, epoch: 2 | loss: 0.4784694
	speed: 1.1226s/iter; left time: 36697.9294s
	iters: 200, epoch: 2 | loss: 0.3101358
	speed: 0.1064s/iter; left time: 3468.5403s
	iters: 300, epoch: 2 | loss: 0.3509502
	speed: 0.1053s/iter; left time: 3421.8924s
	iters: 400, epoch: 2 | loss: 0.2803729
	speed: 0.1061s/iter; left time: 3437.3630s
	iters: 500, epoch: 2 | loss: 0.2924531
	speed: 0.1072s/iter; left time: 3461.0174s
	iters: 600, epoch: 2 | loss: 0.3937810
	speed: 0.1074s/iter; left time: 3456.4574s
	iters: 700, epoch: 2 | loss: 0.2815570
	speed: 0.1067s/iter; left time: 3422.5237s
	iters: 800, epoch: 2 | loss: 0.6659177
	speed: 0.1060s/iter; left time: 3391.2781s
	iters: 900, epoch: 2 | loss: 0.4613145
	speed: 0.1029s/iter; left time: 3280.8615s
	iters: 1000, epoch: 2 | loss: 0.4418274
	speed: 0.1045s/iter; left time: 3321.6564s
	iters: 1100, epoch: 2 | loss: 0.4154446
	speed: 0.1053s/iter; left time: 3336.6275s
	iters: 1200, epoch: 2 | loss: 0.5328344
	speed: 0.1039s/iter; left time: 3283.0195s
	iters: 1300, epoch: 2 | loss: 0.2893824
	speed: 0.1052s/iter; left time: 3311.2664s
	iters: 1400, epoch: 2 | loss: 0.3436886
	speed: 0.1018s/iter; left time: 3194.0856s
	iters: 1500, epoch: 2 | loss: 0.4545120
	speed: 0.1038s/iter; left time: 3248.6441s
	iters: 1600, epoch: 2 | loss: 0.3771766
	speed: 0.1044s/iter; left time: 3255.7168s
	iters: 1700, epoch: 2 | loss: 0.4736871
	speed: 0.1071s/iter; left time: 3330.7735s
	iters: 1800, epoch: 2 | loss: 0.2793548
	speed: 0.1026s/iter; left time: 3180.3236s
	iters: 1900, epoch: 2 | loss: 0.2915854
	speed: 0.1011s/iter; left time: 3124.2920s
	iters: 2000, epoch: 2 | loss: 0.4875781
	speed: 0.1097s/iter; left time: 3376.9155s
	iters: 2100, epoch: 2 | loss: 0.4030606
	speed: 0.1043s/iter; left time: 3201.0039s
	iters: 2200, epoch: 2 | loss: 0.3436973
	speed: 0.1068s/iter; left time: 3265.3985s
	iters: 2300, epoch: 2 | loss: 0.4260112
	speed: 0.1022s/iter; left time: 3116.8051s
Epoch: 2 cost time: 247.54244542121887


Epoch: 2 | Train Loss: 0.4109716 Vali Loss: 0.7158062 Test Loss: 0.3861823 MAE Loss: 0.4118836
Updating learning rate to 2.000000000000001e-06

	iters: 100, epoch: 3 | loss: 0.4536514
	speed: 1.1337s/iter; left time: 34403.4722s
	iters: 200, epoch: 3 | loss: 0.2797221
	speed: 0.1097s/iter; left time: 3318.0066s
	iters: 300, epoch: 3 | loss: 0.5272344
	speed: 0.1038s/iter; left time: 3129.7394s
	iters: 400, epoch: 3 | loss: 0.4328765
	speed: 0.1013s/iter; left time: 3045.0500s
	iters: 500, epoch: 3 | loss: 0.3621251
	speed: 0.1022s/iter; left time: 3061.3667s
	iters: 600, epoch: 3 | loss: 0.3415762
	speed: 0.1058s/iter; left time: 3159.1249s
	iters: 700, epoch: 3 | loss: 0.4849940
	speed: 0.1062s/iter; left time: 3158.4438s
	iters: 800, epoch: 3 | loss: 0.3956785
	speed: 0.0984s/iter; left time: 2917.7541s
	iters: 900, epoch: 3 | loss: 0.5154231
	speed: 0.1062s/iter; left time: 3137.0319s
	iters: 1000, epoch: 3 | loss: 0.3789702
	speed: 0.1066s/iter; left time: 3138.0214s
	iters: 1100, epoch: 3 | loss: 0.5109061
	speed: 0.1089s/iter; left time: 3196.0211s
	iters: 1200, epoch: 3 | loss: 0.4823073
	speed: 0.1071s/iter; left time: 3131.8554s
	iters: 1300, epoch: 3 | loss: 0.3808458
	speed: 0.1117s/iter; left time: 3257.0187s
	iters: 1400, epoch: 3 | loss: 0.2909691
	speed: 0.1023s/iter; left time: 2970.1627s
	iters: 1500, epoch: 3 | loss: 0.4009208
	speed: 0.1042s/iter; left time: 3017.3839s
	iters: 1600, epoch: 3 | loss: 0.3749441
	speed: 0.1038s/iter; left time: 2992.8990s
	iters: 1700, epoch: 3 | loss: 0.5135528
	speed: 0.1074s/iter; left time: 3088.2398s
	iters: 1800, epoch: 3 | loss: 0.4182245
	speed: 0.1057s/iter; left time: 3028.1795s
	iters: 1900, epoch: 3 | loss: 0.4362823
	speed: 0.1021s/iter; left time: 2915.4870s
	iters: 2000, epoch: 3 | loss: 0.3643519
	speed: 0.1062s/iter; left time: 3022.4556s
	iters: 2100, epoch: 3 | loss: 0.3442379
	speed: 0.1070s/iter; left time: 3032.3375s
	iters: 2200, epoch: 3 | loss: 0.5861840
	speed: 0.1052s/iter; left time: 2971.8560s
	iters: 2300, epoch: 3 | loss: 0.6752796
	speed: 0.0970s/iter; left time: 2729.5911s
Epoch: 3 cost time: 248.06424140930176


Epoch: 3 | Train Loss: 0.3971075 Vali Loss: 0.6993130 Test Loss: 0.3865374 MAE Loss: 0.4122813
Updating learning rate to 1.0000000000000006e-06

	iters: 100, epoch: 4 | loss: 0.4143063
	speed: 1.1645s/iter; left time: 32611.4571s
	iters: 200, epoch: 4 | loss: 0.2686370
	speed: 0.1092s/iter; left time: 3046.5811s
	iters: 300, epoch: 4 | loss: 0.5027524
	speed: 0.1063s/iter; left time: 2955.4841s
	iters: 400, epoch: 4 | loss: 0.4828399
	speed: 0.1126s/iter; left time: 3120.0692s
	iters: 500, epoch: 4 | loss: 0.4702744
	speed: 0.1084s/iter; left time: 2993.0611s
	iters: 600, epoch: 4 | loss: 0.2868004
	speed: 0.1013s/iter; left time: 2787.6256s
	iters: 700, epoch: 4 | loss: 0.3604655
	speed: 0.1136s/iter; left time: 3112.8581s
	iters: 800, epoch: 4 | loss: 0.3693996
	speed: 0.1073s/iter; left time: 2930.8506s
	iters: 900, epoch: 4 | loss: 0.3302963
	speed: 0.1057s/iter; left time: 2875.3849s
	iters: 1000, epoch: 4 | loss: 0.3038734
	speed: 0.1066s/iter; left time: 2889.9099s
	iters: 1100, epoch: 4 | loss: 0.3684835
	speed: 0.1053s/iter; left time: 2844.3398s
	iters: 1200, epoch: 4 | loss: 0.3508347
	speed: 0.1045s/iter; left time: 2812.5924s
	iters: 1300, epoch: 4 | loss: 0.3062679
	speed: 0.1070s/iter; left time: 2869.4133s
	iters: 1400, epoch: 4 | loss: 0.5090715
	speed: 0.1088s/iter; left time: 2906.1885s
	iters: 1500, epoch: 4 | loss: 0.3707307
	speed: 0.1088s/iter; left time: 2893.7850s
	iters: 1600, epoch: 4 | loss: 0.4730406
	speed: 0.0955s/iter; left time: 2530.4811s
	iters: 1700, epoch: 4 | loss: 0.4850582
	speed: 0.1073s/iter; left time: 2833.6607s
	iters: 1800, epoch: 4 | loss: 0.3463080
	speed: 0.1106s/iter; left time: 2910.4091s
	iters: 1900, epoch: 4 | loss: 0.4360712
	speed: 0.1082s/iter; left time: 2834.8019s
	iters: 2000, epoch: 4 | loss: 0.3799232
	speed: 0.1062s/iter; left time: 2771.4423s
	iters: 2100, epoch: 4 | loss: 0.4562793
	speed: 0.1063s/iter; left time: 2763.8319s
	iters: 2200, epoch: 4 | loss: 0.3669100
	speed: 0.1092s/iter; left time: 2828.6068s
	iters: 2300, epoch: 4 | loss: 0.5275265
	speed: 0.1034s/iter; left time: 2668.4973s
Epoch: 4 cost time: 251.69936609268188


Epoch: 4 | Train Loss: 0.3916804 Vali Loss: 0.7007714 Test Loss: 0.3837522 MAE Loss: 0.4100123
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.000000000000003e-07

	iters: 100, epoch: 5 | loss: 0.4214916
	speed: 1.1066s/iter; left time: 28397.7700s
	iters: 200, epoch: 5 | loss: 0.3769958
	speed: 0.1046s/iter; left time: 2674.7164s
	iters: 300, epoch: 5 | loss: 0.3763990
	speed: 0.1069s/iter; left time: 2721.4404s
	iters: 400, epoch: 5 | loss: 0.3026768
	speed: 0.1062s/iter; left time: 2692.8685s
	iters: 500, epoch: 5 | loss: 0.4041429
	speed: 0.0999s/iter; left time: 2525.0278s
	iters: 600, epoch: 5 | loss: 0.4990924
	speed: 0.1070s/iter; left time: 2691.4842s
	iters: 700, epoch: 5 | loss: 0.3589607
	speed: 0.1084s/iter; left time: 2715.8696s
	iters: 800, epoch: 5 | loss: 0.3656476
	speed: 0.1065s/iter; left time: 2659.6640s
	iters: 900, epoch: 5 | loss: 0.3220272
	speed: 0.1060s/iter; left time: 2635.1058s
	iters: 1000, epoch: 5 | loss: 0.3471880
	speed: 0.0971s/iter; left time: 2403.3496s
	iters: 1100, epoch: 5 | loss: 0.2832628
	speed: 0.1031s/iter; left time: 2543.5793s
	iters: 1200, epoch: 5 | loss: 0.3116834
	speed: 0.1047s/iter; left time: 2572.4671s
	iters: 1300, epoch: 5 | loss: 0.4017349
	speed: 0.1091s/iter; left time: 2668.1433s
	iters: 1400, epoch: 5 | loss: 0.4896947
	speed: 0.1082s/iter; left time: 2635.8997s
	iters: 1500, epoch: 5 | loss: 0.4627542
	speed: 0.1049s/iter; left time: 2545.1779s
	iters: 1600, epoch: 5 | loss: 0.4242935
	speed: 0.1066s/iter; left time: 2576.1801s
	iters: 1700, epoch: 5 | loss: 0.4189254
	speed: 0.1086s/iter; left time: 2612.8643s
	iters: 1800, epoch: 5 | loss: 0.3706457
	speed: 0.1063s/iter; left time: 2546.5404s
	iters: 1900, epoch: 5 | loss: 0.2909053
	speed: 0.1032s/iter; left time: 2462.5369s
	iters: 2000, epoch: 5 | loss: 0.3885134
	speed: 0.1015s/iter; left time: 2413.1275s
	iters: 2100, epoch: 5 | loss: 0.3429206
	speed: 0.1070s/iter; left time: 2532.4063s
	iters: 2200, epoch: 5 | loss: 0.2753153
	speed: 0.1064s/iter; left time: 2507.5672s
	iters: 2300, epoch: 5 | loss: 0.4391516
	speed: 0.1104s/iter; left time: 2589.8525s
Epoch: 5 cost time: 248.16192317008972


Epoch: 5 | Train Loss: 0.3891743 Vali Loss: 0.7014708 Test Loss: 0.3812112 MAE Loss: 0.4079201
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5000000000000015e-07

	iters: 100, epoch: 6 | loss: 0.4140712
	speed: 1.1298s/iter; left time: 26349.0894s
	iters: 200, epoch: 6 | loss: 0.4633028
	speed: 0.1050s/iter; left time: 2437.0655s
	iters: 300, epoch: 6 | loss: 0.3432435
	speed: 0.1059s/iter; left time: 2448.1691s
	iters: 400, epoch: 6 | loss: 0.2336458
	speed: 0.1058s/iter; left time: 2434.6643s
	iters: 500, epoch: 6 | loss: 0.5302416
	speed: 0.1026s/iter; left time: 2351.9055s
	iters: 600, epoch: 6 | loss: 0.4105166
	speed: 0.1098s/iter; left time: 2505.6792s
	iters: 700, epoch: 6 | loss: 0.3421384
	speed: 0.1090s/iter; left time: 2475.7025s
	iters: 800, epoch: 6 | loss: 0.4568343
	speed: 0.1053s/iter; left time: 2381.9213s
	iters: 900, epoch: 6 | loss: 0.4229225
	speed: 0.1075s/iter; left time: 2419.9687s
	iters: 1000, epoch: 6 | loss: 0.2761846
	speed: 0.1038s/iter; left time: 2327.1665s
	iters: 1100, epoch: 6 | loss: 0.4707081
	speed: 0.1062s/iter; left time: 2370.7839s
	iters: 1200, epoch: 6 | loss: 0.4135377
	speed: 0.1068s/iter; left time: 2373.3738s
	iters: 1300, epoch: 6 | loss: 0.3830248
	speed: 0.1102s/iter; left time: 2437.7151s
	iters: 1400, epoch: 6 | loss: 0.4393918
	speed: 0.1046s/iter; left time: 2302.7307s
	iters: 1500, epoch: 6 | loss: 0.2840604
	speed: 0.1025s/iter; left time: 2247.0844s
	iters: 1600, epoch: 6 | loss: 0.3638086
	speed: 0.1023s/iter; left time: 2233.1052s
	iters: 1700, epoch: 6 | loss: 0.5026310
	speed: 0.1042s/iter; left time: 2263.1061s
	iters: 1800, epoch: 6 | loss: 0.3375888
	speed: 0.1077s/iter; left time: 2329.1654s
	iters: 1900, epoch: 6 | loss: 0.3131125
	speed: 0.1018s/iter; left time: 2191.8296s
	iters: 2000, epoch: 6 | loss: 0.4583935
	speed: 0.1039s/iter; left time: 2225.6490s
	iters: 2100, epoch: 6 | loss: 0.4507133
	speed: 0.1055s/iter; left time: 2249.8479s
	iters: 2200, epoch: 6 | loss: 0.2538497
	speed: 0.1087s/iter; left time: 2307.1154s
	iters: 2300, epoch: 6 | loss: 0.3417342
	speed: 0.1058s/iter; left time: 2235.2982s
Epoch: 6 cost time: 248.60098814964294


Epoch: 6 | Train Loss: 0.3881918 Vali Loss: 0.6974982 Test Loss: 0.3813144 MAE Loss: 0.4079486
Updating learning rate to 1.2500000000000007e-07

1it [00:01,  1.14s/it]
	iters: 100, epoch: 7 | loss: 0.4871575
	speed: 1.1596s/iter; left time: 24327.9718s
	iters: 200, epoch: 7 | loss: 0.2627058
	speed: 0.1178s/iter; left time: 2460.1312s
	iters: 300, epoch: 7 | loss: 0.4674422
	speed: 0.1144s/iter; left time: 2376.3689s
	iters: 400, epoch: 7 | loss: 0.2861752
	speed: 0.1177s/iter; left time: 2434.4511s
	iters: 500, epoch: 7 | loss: 0.4645396
	speed: 0.1160s/iter; left time: 2386.3338s
	iters: 600, epoch: 7 | loss: 0.4263874
	speed: 0.1202s/iter; left time: 2461.4532s
	iters: 700, epoch: 7 | loss: 0.4127827
	speed: 0.1139s/iter; left time: 2320.9536s
	iters: 800, epoch: 7 | loss: 0.3026163
	speed: 0.1193s/iter; left time: 2419.7137s
	iters: 900, epoch: 7 | loss: 0.3609984
	speed: 0.1167s/iter; left time: 2354.1262s
	iters: 1000, epoch: 7 | loss: 0.4329373
	speed: 0.1171s/iter; left time: 2351.1693s
	iters: 1100, epoch: 7 | loss: 0.3866279
	speed: 0.1184s/iter; left time: 2366.2283s
	iters: 1200, epoch: 7 | loss: 0.2073217
	speed: 0.1137s/iter; left time: 2260.7574s
	iters: 1300, epoch: 7 | loss: 0.3358859
	speed: 0.1133s/iter; left time: 2240.7314s
	iters: 1400, epoch: 7 | loss: 0.5041385
	speed: 0.1127s/iter; left time: 2218.3020s
	iters: 1500, epoch: 7 | loss: 0.5770615
	speed: 0.1157s/iter; left time: 2265.7111s
	iters: 1600, epoch: 7 | loss: 0.4637717
	speed: 0.1154s/iter; left time: 2247.7195s
	iters: 1700, epoch: 7 | loss: 0.5432938
	speed: 0.1169s/iter; left time: 2265.0737s
	iters: 1800, epoch: 7 | loss: 0.3023338
	speed: 0.1142s/iter; left time: 2201.8248s
	iters: 1900, epoch: 7 | loss: 0.4034504
	speed: 0.1111s/iter; left time: 2131.3152s
	iters: 2000, epoch: 7 | loss: 0.3665892
	speed: 0.1155s/iter; left time: 2203.5854s
	iters: 2100, epoch: 7 | loss: 0.3742164
	speed: 0.1178s/iter; left time: 2235.6938s
	iters: 2200, epoch: 7 | loss: 0.3655952
	speed: 0.1121s/iter; left time: 2117.2045s
	iters: 2300, epoch: 7 | loss: 0.4782614
	speed: 0.1147s/iter; left time: 2154.3436s
Epoch: 7 cost time: 272.7978036403656


Epoch: 7 | Train Loss: 0.3877677 Vali Loss: 0.6986995 Test Loss: 0.3821727 MAE Loss: 0.4086950
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.250000000000004e-08

1it [00:01,  1.03s/it]
	iters: 100, epoch: 8 | loss: 0.4939858
	speed: 1.2185s/iter; left time: 22708.6859s
	iters: 200, epoch: 8 | loss: 0.3332251
	speed: 0.1137s/iter; left time: 2107.6863s
	iters: 300, epoch: 8 | loss: 0.3974192
	speed: 0.1018s/iter; left time: 1876.7719s
	iters: 400, epoch: 8 | loss: 0.3255675
	speed: 0.1084s/iter; left time: 1987.4728s
	iters: 500, epoch: 8 | loss: 0.3099812
	speed: 0.1107s/iter; left time: 2018.0905s
	iters: 600, epoch: 8 | loss: 0.2341795
	speed: 0.1106s/iter; left time: 2006.0782s
	iters: 700, epoch: 8 | loss: 0.2098615
	speed: 0.1078s/iter; left time: 1944.5705s
	iters: 800, epoch: 8 | loss: 0.3748512
	speed: 0.1040s/iter; left time: 1865.6411s
	iters: 900, epoch: 8 | loss: 0.4058671
	speed: 0.1071s/iter; left time: 1909.9528s
	iters: 1000, epoch: 8 | loss: 0.3501701
	speed: 0.1081s/iter; left time: 1916.7251s
	iters: 1100, epoch: 8 | loss: 0.2958572
	speed: 0.1061s/iter; left time: 1871.9463s
	iters: 1200, epoch: 8 | loss: 0.4140395
	speed: 0.1051s/iter; left time: 1842.8301s
	iters: 1300, epoch: 8 | loss: 0.3485396
	speed: 0.1072s/iter; left time: 1868.7852s
	iters: 1400, epoch: 8 | loss: 0.4681269
	speed: 0.1111s/iter; left time: 1925.9393s
	iters: 1500, epoch: 8 | loss: 0.3250991
	speed: 0.1075s/iter; left time: 1852.5673s
	iters: 1600, epoch: 8 | loss: 0.4037723
	speed: 0.1112s/iter; left time: 1905.9840s
	iters: 1700, epoch: 8 | loss: 0.3744064
	speed: 0.1109s/iter; left time: 1889.3110s
	iters: 1800, epoch: 8 | loss: 0.3054697
	speed: 0.1070s/iter; left time: 1812.8178s
	iters: 1900, epoch: 8 | loss: 0.3254758
	speed: 0.1069s/iter; left time: 1799.8702s
	iters: 2000, epoch: 8 | loss: 0.3304848
	speed: 0.1103s/iter; left time: 1846.9195s
	iters: 2100, epoch: 8 | loss: 0.4094925
	speed: 0.1058s/iter; left time: 1760.0622s
	iters: 2200, epoch: 8 | loss: 0.3758273
	speed: 0.1036s/iter; left time: 1713.6973s
	iters: 2300, epoch: 8 | loss: 0.3637149
	speed: 0.1144s/iter; left time: 1880.6653s
Epoch: 8 cost time: 255.85671305656433


Epoch: 8 | Train Loss: 0.3875428 Vali Loss: 0.6982000 Test Loss: 0.3814110 MAE Loss: 0.4080286
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125000000000002e-08

1it [00:01,  1.02s/it]
	iters: 100, epoch: 9 | loss: 0.2673180
	speed: 1.1504s/iter; left time: 18745.1055s
	iters: 200, epoch: 9 | loss: 0.2987591
	speed: 0.1069s/iter; left time: 1731.3778s
	iters: 300, epoch: 9 | loss: 0.2803395
	speed: 0.1070s/iter; left time: 1721.9720s
	iters: 400, epoch: 9 | loss: 0.5174639
	speed: 0.1078s/iter; left time: 1723.7882s
	iters: 500, epoch: 9 | loss: 0.3133446
	speed: 0.1097s/iter; left time: 1744.0470s
	iters: 600, epoch: 9 | loss: 0.5138530
	speed: 0.1046s/iter; left time: 1651.6938s
	iters: 700, epoch: 9 | loss: 0.5244649
	speed: 0.1059s/iter; left time: 1662.5974s
	iters: 800, epoch: 9 | loss: 0.6610441
	speed: 0.1063s/iter; left time: 1657.6031s
	iters: 900, epoch: 9 | loss: 0.3321635
	speed: 0.1105s/iter; left time: 1711.5573s
	iters: 1000, epoch: 9 | loss: 0.3547824
	speed: 0.1067s/iter; left time: 1642.1439s
	iters: 1100, epoch: 9 | loss: 0.2927629
	speed: 0.1088s/iter; left time: 1664.6249s
	iters: 1200, epoch: 9 | loss: 0.5093838
	speed: 0.1067s/iter; left time: 1621.4031s
	iters: 1300, epoch: 9 | loss: 0.2753659
	speed: 0.1045s/iter; left time: 1577.5777s
	iters: 1400, epoch: 9 | loss: 0.4785726
	speed: 0.1096s/iter; left time: 1643.5967s
	iters: 1500, epoch: 9 | loss: 0.2871887
	speed: 0.1070s/iter; left time: 1593.6595s
	iters: 1600, epoch: 9 | loss: 0.4582694
	speed: 0.1058s/iter; left time: 1565.7456s
	iters: 1700, epoch: 9 | loss: 0.4141785
	speed: 0.1040s/iter; left time: 1527.9185s
	iters: 1800, epoch: 9 | loss: 0.3983364
	speed: 0.1082s/iter; left time: 1579.8161s
	iters: 1900, epoch: 9 | loss: 0.3119933
	speed: 0.1026s/iter; left time: 1487.6620s
	iters: 2000, epoch: 9 | loss: 0.4217698
	speed: 0.1033s/iter; left time: 1487.5900s
	iters: 2100, epoch: 9 | loss: 0.4086830
	speed: 0.1065s/iter; left time: 1522.9173s
	iters: 2200, epoch: 9 | loss: 0.3415379
	speed: 0.1093s/iter; left time: 1551.7347s
	iters: 2300, epoch: 9 | loss: 0.3150884
	speed: 0.1063s/iter; left time: 1497.9207s
Epoch: 9 cost time: 251.14579510688782


Epoch: 9 | Train Loss: 0.3873368 Vali Loss: 0.6976859 Test Loss: 0.3811581 MAE Loss: 0.4077722
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.562500000000001e-08

	iters: 100, epoch: 10 | loss: 0.4068696
	speed: 1.1360s/iter; left time: 15850.5630s
	iters: 200, epoch: 10 | loss: 0.4692579
	speed: 0.1110s/iter; left time: 1537.2920s
	iters: 300, epoch: 10 | loss: 0.4945488
	speed: 0.1084s/iter; left time: 1490.9703s
	iters: 400, epoch: 10 | loss: 0.6620257
	speed: 0.1075s/iter; left time: 1467.4554s
	iters: 500, epoch: 10 | loss: 0.4703095
	speed: 0.1115s/iter; left time: 1511.7468s
	iters: 600, epoch: 10 | loss: 0.4547473
	speed: 0.1109s/iter; left time: 1491.9888s
	iters: 700, epoch: 10 | loss: 0.3064407
	speed: 0.1140s/iter; left time: 1522.4495s
	iters: 800, epoch: 10 | loss: 0.3748592
	speed: 0.1038s/iter; left time: 1375.5788s
	iters: 900, epoch: 10 | loss: 0.3753185
	speed: 0.1092s/iter; left time: 1436.8995s
	iters: 1000, epoch: 10 | loss: 0.3173365
	speed: 0.1085s/iter; left time: 1416.5509s
	iters: 1100, epoch: 10 | loss: 0.4164267
	speed: 0.1102s/iter; left time: 1427.9134s
	iters: 1200, epoch: 10 | loss: 0.5105352
	speed: 0.1008s/iter; left time: 1295.1303s
	iters: 1300, epoch: 10 | loss: 0.5686902
	speed: 0.1003s/iter; left time: 1279.6666s
	iters: 1400, epoch: 10 | loss: 0.5944030
	speed: 0.1024s/iter; left time: 1296.2683s
	iters: 1500, epoch: 10 | loss: 0.7109547
	speed: 0.1084s/iter; left time: 1360.4645s
	iters: 1600, epoch: 10 | loss: 0.3951087
	speed: 0.1036s/iter; left time: 1289.5911s
	iters: 1700, epoch: 10 | loss: 0.3285423
	speed: 0.1074s/iter; left time: 1326.8897s
	iters: 1800, epoch: 10 | loss: 0.3931045
	speed: 0.1058s/iter; left time: 1296.4170s
	iters: 1900, epoch: 10 | loss: 0.4120590
	speed: 0.1002s/iter; left time: 1217.8501s
	iters: 2000, epoch: 10 | loss: 0.3970083
	speed: 0.1027s/iter; left time: 1237.4892s
	iters: 2100, epoch: 10 | loss: 0.3824126
	speed: 0.1053s/iter; left time: 1258.8989s
	iters: 2200, epoch: 10 | loss: 0.4283749
	speed: 0.1085s/iter; left time: 1285.5542s
	iters: 2300, epoch: 10 | loss: 0.4162222
	speed: 0.1070s/iter; left time: 1257.2142s
Epoch: 10 cost time: 250.70542860031128


Epoch: 10 | Train Loss: 0.3869618 Vali Loss: 0.6981826 Test Loss: 0.3810521 MAE Loss: 0.4077870
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.812500000000005e-09

	iters: 100, epoch: 11 | loss: 0.3205084
	speed: 1.1361s/iter; left time: 13191.5759s
	iters: 200, epoch: 11 | loss: 0.3708896
	speed: 0.1114s/iter; left time: 1282.3509s
	iters: 300, epoch: 11 | loss: 0.3776736
	speed: 0.1082s/iter; left time: 1234.4796s
	iters: 400, epoch: 11 | loss: 0.5292957
	speed: 0.1071s/iter; left time: 1211.0483s
	iters: 500, epoch: 11 | loss: 0.5025937
	speed: 0.1008s/iter; left time: 1129.9349s
	iters: 600, epoch: 11 | loss: 0.4032612
	speed: 0.0995s/iter; left time: 1105.1173s
	iters: 700, epoch: 11 | loss: 0.3875958
	speed: 0.1220s/iter; left time: 1343.6063s
	iters: 800, epoch: 11 | loss: 0.4686431
	speed: 0.1139s/iter; left time: 1242.7602s
	iters: 900, epoch: 11 | loss: 0.2970020
	speed: 0.1152s/iter; left time: 1245.5897s
	iters: 1000, epoch: 11 | loss: 0.3826735
	speed: 0.1167s/iter; left time: 1249.6812s
	iters: 1100, epoch: 11 | loss: 0.4641658
	speed: 0.1187s/iter; left time: 1259.3722s
	iters: 1200, epoch: 11 | loss: 0.2718593
	speed: 0.1247s/iter; left time: 1311.0928s
	iters: 1300, epoch: 11 | loss: 0.3958134
	speed: 0.1169s/iter; left time: 1216.7404s
	iters: 1400, epoch: 11 | loss: 0.3837200
	speed: 0.1170s/iter; left time: 1205.9363s
	iters: 1500, epoch: 11 | loss: 0.3853958
	speed: 0.1221s/iter; left time: 1246.4354s
	iters: 1600, epoch: 11 | loss: 0.3012198
	speed: 0.1167s/iter; left time: 1179.9466s
	iters: 1700, epoch: 11 | loss: 0.2524414
	speed: 0.1190s/iter; left time: 1191.2589s
	iters: 1800, epoch: 11 | loss: 0.4122427
	speed: 0.1169s/iter; left time: 1158.1369s
	iters: 1900, epoch: 11 | loss: 0.2856417
	speed: 0.1169s/iter; left time: 1146.8797s
	iters: 2000, epoch: 11 | loss: 0.3380420
	speed: 0.1142s/iter; left time: 1108.6128s
	iters: 2100, epoch: 11 | loss: 0.3895341
	speed: 0.1205s/iter; left time: 1157.9544s
	iters: 2200, epoch: 11 | loss: 0.5100933
	speed: 0.1156s/iter; left time: 1099.5988s
	iters: 2300, epoch: 11 | loss: 0.3619492
	speed: 0.1200s/iter; left time: 1129.1994s
Epoch: 11 cost time: 269.66916823387146

1it [00:01,  1.03s/it]

1it [00:01,  1.10s/it]
Epoch: 11 | Train Loss: 0.3872393 Vali Loss: 0.6978647 Test Loss: 0.3811315 MAE Loss: 0.4078303
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.906250000000002e-09

1it [00:01,  1.07s/it]
	iters: 100, epoch: 12 | loss: 0.4469961
	speed: 1.2637s/iter; left time: 11712.8186s
	iters: 200, epoch: 12 | loss: 0.5417922
	speed: 0.1200s/iter; left time: 1100.2301s
	iters: 300, epoch: 12 | loss: 0.5032297
	speed: 0.1202s/iter; left time: 1090.0138s
	iters: 400, epoch: 12 | loss: 0.3778447
	speed: 0.1208s/iter; left time: 1083.0590s
	iters: 500, epoch: 12 | loss: 0.3020508
	speed: 0.1207s/iter; left time: 1070.1092s
	iters: 600, epoch: 12 | loss: 0.4632048
	speed: 0.1134s/iter; left time: 994.5039s
	iters: 700, epoch: 12 | loss: 0.3627195
	speed: 0.1178s/iter; left time: 1021.2222s
	iters: 800, epoch: 12 | loss: 0.4154415
	speed: 0.1195s/iter; left time: 1023.7803s
	iters: 900, epoch: 12 | loss: 0.5191025
	speed: 0.1193s/iter; left time: 1010.2654s
	iters: 1000, epoch: 12 | loss: 0.3175645
	speed: 0.1194s/iter; left time: 999.2497s
	iters: 1100, epoch: 12 | loss: 0.3437614
	speed: 0.1198s/iter; left time: 990.8791s
	iters: 1200, epoch: 12 | loss: 0.3978634
	speed: 0.1189s/iter; left time: 971.5540s
	iters: 1300, epoch: 12 | loss: 0.3799991
	speed: 0.1172s/iter; left time: 945.9438s
	iters: 1400, epoch: 12 | loss: 0.2447039
	speed: 0.1156s/iter; left time: 921.3510s
	iters: 1500, epoch: 12 | loss: 0.3397307
	speed: 0.1207s/iter; left time: 949.9838s
	iters: 1600, epoch: 12 | loss: 0.3467035
	speed: 0.1163s/iter; left time: 903.1738s
	iters: 1700, epoch: 12 | loss: 0.6348775
	speed: 0.1157s/iter; left time: 887.1480s
	iters: 1800, epoch: 12 | loss: 0.3383704
	speed: 0.1154s/iter; left time: 873.4799s
	iters: 1900, epoch: 12 | loss: 0.3791066
	speed: 0.1209s/iter; left time: 902.7125s
	iters: 2000, epoch: 12 | loss: 0.2848220
	speed: 0.1220s/iter; left time: 898.8812s
	iters: 2100, epoch: 12 | loss: 0.4053579
	speed: 0.1218s/iter; left time: 885.6745s
	iters: 2200, epoch: 12 | loss: 0.3933846
	speed: 0.1170s/iter; left time: 838.8577s
	iters: 2300, epoch: 12 | loss: 0.4640246
	speed: 0.1189s/iter; left time: 840.2553s
Epoch: 12 cost time: 279.0338613986969


Epoch: 12 | Train Loss: 0.3873224 Vali Loss: 0.6981930 Test Loss: 0.3811551 MAE Loss: 0.4078664
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125000000001e-09

