from math import sqrt

import torch
import torch.nn as nn

from transformers import LlamaConfig, LlamaModel, LlamaTokenizer, GPT2Config, GPT2Model, GPT2Tokenizer, BertConfig, \
    BertModel, BertTokenizer
from layers.Embed import PatchEmbedding, WaveletPatchEmbedding, WISTPatchEmbedding
from layers.FrequencyDecoupledHead import TriBandDecoupledHead, DeepSupervisionLoss
from layers.DualScaleHead import DualScaleResidualHead
from layers.CWPR import CWPRReprogrammingLayer
import transformers
from layers.StandardNorm import Normalize
from utils.seed_word_selector import select_seed_words

transformers.logging.set_verbosity_error()


class FlattenHead(nn.Module):
    def __init__(self, n_vars, nf, target_window, head_dropout=0):
        super().__init__()
        self.n_vars = n_vars
        self.flatten = nn.Flatten(start_dim=-2)
        self.linear = nn.Linear(nf, target_window)
        self.dropout = nn.Dropout(head_dropout)

    def forward(self, x):
        x = self.flatten(x)
        x = self.linear(x)
        x = self.dropout(x)
        return x


class Model(nn.Module):

    def __init__(self, configs, patch_len=16, stride=8):
        super(Model, self).__init__()
        self.task_name = configs.task_name
        self.pred_len = configs.pred_len
        self.seq_len = configs.seq_len
        self.d_ff = configs.d_ff
        self.top_k = 5
        self.d_llm = configs.llm_dim
        self.patch_len = configs.patch_len
        self.stride = configs.stride

        if configs.llm_model == 'LLAMA':
            # self.llama_config = LlamaConfig.from_pretrained('/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/')
            self.llama_config = LlamaConfig.from_pretrained('huggyllama/llama-7b')
            self.llama_config.num_hidden_layers = configs.llm_layers
            self.llama_config.output_attentions = True
            self.llama_config.output_hidden_states = True
            try:
                self.llm_model = LlamaModel.from_pretrained(
                    # "/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/",
                    'huggyllama/llama-7b',
                    trust_remote_code=True,
                    local_files_only=True,
                    config=self.llama_config,
                    # load_in_4bit=True
                )
            except EnvironmentError:  # downloads model from HF is not already done
                print("Local model files not found. Attempting to download...")
                self.llm_model = LlamaModel.from_pretrained(
                    # "/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/",
                    'huggyllama/llama-7b',
                    trust_remote_code=True,
                    local_files_only=False,
                    config=self.llama_config,
                    # load_in_4bit=True
                )
            try:
                self.tokenizer = LlamaTokenizer.from_pretrained(
                    # "/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model",
                    'huggyllama/llama-7b',
                    trust_remote_code=True,
                    local_files_only=True
                )
            except EnvironmentError:  # downloads the tokenizer from HF if not already done
                print("Local tokenizer files not found. Atempting to download them..")
                self.tokenizer = LlamaTokenizer.from_pretrained(
                    # "/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model",
                    'huggyllama/llama-7b',
                    trust_remote_code=True,
                    local_files_only=False
                )
        elif configs.llm_model == 'GPT2':
            self.gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')

            self.gpt2_config.num_hidden_layers = configs.llm_layers
            self.gpt2_config.output_attentions = True
            self.gpt2_config.output_hidden_states = True
            try:
                self.llm_model = GPT2Model.from_pretrained(
                    'openai-community/gpt2',
                    trust_remote_code=True,
                    local_files_only=True,
                    config=self.gpt2_config,
                )
            except EnvironmentError:  # downloads model from HF is not already done
                print("Local model files not found. Attempting to download...")
                self.llm_model = GPT2Model.from_pretrained(
                    'openai-community/gpt2',
                    trust_remote_code=True,
                    local_files_only=False,
                    config=self.gpt2_config,
                )

            try:
                self.tokenizer = GPT2Tokenizer.from_pretrained(
                    'openai-community/gpt2',
                    trust_remote_code=True,
                    local_files_only=True
                )
            except EnvironmentError:  # downloads the tokenizer from HF if not already done
                print("Local tokenizer files not found. Atempting to download them..")
                self.tokenizer = GPT2Tokenizer.from_pretrained(
                    'openai-community/gpt2',
                    trust_remote_code=True,
                    local_files_only=False
                )
        elif configs.llm_model == 'BERT':
            self.bert_config = BertConfig.from_pretrained('google-bert/bert-base-uncased')

            self.bert_config.num_hidden_layers = configs.llm_layers
            self.bert_config.output_attentions = True
            self.bert_config.output_hidden_states = True
            try:
                self.llm_model = BertModel.from_pretrained(
                    'google-bert/bert-base-uncased',
                    trust_remote_code=True,
                    local_files_only=True,
                    config=self.bert_config,
                )
            except EnvironmentError:  # downloads model from HF is not already done
                print("Local model files not found. Attempting to download...")
                self.llm_model = BertModel.from_pretrained(
                    'google-bert/bert-base-uncased',
                    trust_remote_code=True,
                    local_files_only=False,
                    config=self.bert_config,
                )

            try:
                self.tokenizer = BertTokenizer.from_pretrained(
                    'google-bert/bert-base-uncased',
                    trust_remote_code=True,
                    local_files_only=True
                )
            except EnvironmentError:  # downloads the tokenizer from HF if not already done
                print("Local tokenizer files not found. Atempting to download them..")
                self.tokenizer = BertTokenizer.from_pretrained(
                    'google-bert/bert-base-uncased',
                    trust_remote_code=True,
                    local_files_only=False
                )
        else:
            raise Exception('LLM model is not defined')

        if self.tokenizer.eos_token:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        else:
            pad_token = '[PAD]'
            self.tokenizer.add_special_tokens({'pad_token': pad_token})
            self.tokenizer.pad_token = pad_token

        for param in self.llm_model.parameters():
            param.requires_grad = False

        if configs.prompt_domain:
            self.description = configs.content
        else:
            self.description = 'The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment.'

        self.dropout = nn.Dropout(configs.dropout)

        # æ ¹æ®é…ç½®é€‰æ‹© Patch Embedding ç±»å‹
        # wavelet_mode: 'none'=åŸç‰ˆ, 'haar'=Haarå°æ³¢æ–¹æ¡ˆ, 'wist'=æ–°WIST-PEæ–¹æ¡ˆ
        self.wavelet_mode = getattr(configs, 'wavelet_mode', 'none')
        self.use_haar_wavelet = getattr(configs, 'use_haar_wavelet', 0)
        
        # ä¼˜å…ˆä½¿ç”¨ wavelet_mode å‚æ•°ï¼Œå…¼å®¹æ—§çš„ use_haar_wavelet å‚æ•°
        if self.wavelet_mode == 'wist':
            # æ–° WIST-PE æ–¹æ¡ˆï¼šå…¨å±€å› æœå°æ³¢åˆ†è§£ + åŒé€šé“å·®å¼‚åŒ– + é—¨æ§èåˆ
            # æ”¯æŒå¤šçº§åˆ†è§£æ—¶çš„åˆ†å±‚é‡‘å­—å¡”èåˆ
            self.patch_embedding = WISTPatchEmbedding(
                d_model=configs.d_model,
                patch_len=self.patch_len,
                stride=self.stride,
                dropout=configs.dropout,
                wavelet_type=getattr(configs, 'wavelet_type', 'db4'),
                wavelet_level=getattr(configs, 'wavelet_level', 1),
                hf_dropout=getattr(configs, 'hf_dropout', 0.5),
                gate_bias_init=getattr(configs, 'gate_bias_init', 2.0),
                use_soft_threshold=bool(getattr(configs, 'use_soft_threshold', 1)),
                use_causal_conv=bool(getattr(configs, 'use_causal_conv', 1)),
                pyramid_fusion=bool(getattr(configs, 'pyramid_fusion', 1)),
                mf_dropout=getattr(configs, 'mf_dropout', 0.3),
                use_freq_attention=bool(getattr(configs, 'use_freq_attention', 0)),
                freq_attention_version=int(getattr(configs, 'freq_attention_version', 1)),
                freq_attn_kernel_size=int(getattr(configs, 'freq_attn_kernel_size', 3)),
                use_freq_embedding=bool(getattr(configs, 'use_freq_embedding', 0)),
                freq_embed_init_method=getattr(configs, 'freq_embed_init_method', 'random'),
                use_positional_encoding=bool(getattr(configs, 'use_positional_encoding', 0)),
                pos_encoding_max_len=int(getattr(configs, 'pos_encoding_max_len', 5000)),
                use_hf_freq_attention=bool(getattr(configs, 'use_hf_freq_attention', 1)),  # é»˜è®¤ä½¿ç”¨é¢‘ç‡æ³¨æ„åŠ›è¿›è¡Œé«˜é¢‘èåˆ
                configs=configs,
            )
            print("[TimeLLM] ä½¿ç”¨ WISTPatchEmbedding (WIST-PE å…¨å±€å› æœå°æ³¢æ–¹æ¡ˆ)")
        elif self.wavelet_mode == 'haar' or self.use_haar_wavelet:
            # Haar å°æ³¢æ–¹æ¡ˆï¼ˆPatchçº§åˆ«ï¼‰
            self.patch_embedding = WaveletPatchEmbedding(
                configs.d_model, self.patch_len, self.stride, configs.dropout,
                use_soft_threshold=True,
                use_positional_encoding=bool(getattr(configs, 'use_positional_encoding', 0)),
                pos_encoding_max_len=int(getattr(configs, 'pos_encoding_max_len', 5000)))
            print("[TimeLLM] ä½¿ç”¨ WaveletPatchEmbedding (Haarå°æ³¢æ–¹æ¡ˆ)")
        else:
            # åŸç‰ˆ Patch Embedding
            self.patch_embedding = PatchEmbedding(
                configs.d_model, self.patch_len, self.stride, configs.dropout,
                use_positional_encoding=bool(getattr(configs, 'use_positional_encoding', 0)),
                pos_encoding_max_len=int(getattr(configs, 'pos_encoding_max_len', 5000)))
            print("[TimeLLM] ä½¿ç”¨ PatchEmbedding (åŸç‰ˆ)")

        self.word_embeddings = self.llm_model.get_input_embeddings().weight
        self.vocab_size = self.word_embeddings.shape[0]
        self.num_tokens = 1000
        
        # åˆ†ç¦»åŸå‹é…ç½®ï¼ˆä»…ç”¨äºåŸç‰ˆ ReprogrammingLayerï¼Œä¸å½±å“ CWPRï¼‰
        self.use_dual_prototypes = bool(getattr(configs, 'use_dual_prototypes', 0))
        self.use_semantic_filtered_mapping = False  # é»˜è®¤å€¼ï¼Œåœ¨ use_dual_prototypes æ—¶å¯èƒ½è¢«è¦†ç›–
        self.use_full_vocab_split = False  # å…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼ˆæ–°åŠŸèƒ½ï¼‰
        
        if self.use_dual_prototypes:
            # åˆ†ç¦»åŸå‹æ¨¡å¼ï¼šåˆ†åˆ«æŒ‡å®šè¶‹åŠ¿åŸå‹å’Œç»†èŠ‚åŸå‹çš„æ•°é‡
            self.num_trend_tokens = int(getattr(configs, 'dual_proto_trend_tokens', 1000))
            self.num_detail_tokens = int(getattr(configs, 'dual_proto_detail_tokens', 1000))
            
            # å…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼ˆæ–°åŠŸèƒ½ï¼‰ï¼šå°†æ•´ä¸ªè¯è¡¨åˆ‡åˆ†æˆè¶‹åŠ¿æ¡¶å’Œç»†èŠ‚æ¡¶
            self.use_full_vocab_split = bool(getattr(configs, 'use_full_vocab_split', 0))
            
            # è¯­ä¹‰ç­›é€‰æ˜ å°„é…ç½®ï¼ˆæ–°åŠŸèƒ½ï¼‰
            self.use_semantic_filtered_mapping = bool(getattr(configs, 'use_semantic_filtered_mapping', 0))
            
            # å…¨è¯è¡¨åˆ‡åˆ†å’Œè¯­ä¹‰ç­›é€‰æ˜ å°„æ˜¯äº’æ–¥çš„
            if self.use_full_vocab_split and self.use_semantic_filtered_mapping:
                raise ValueError("use_full_vocab_split å’Œ use_semantic_filtered_mapping ä¸èƒ½åŒæ—¶å¯ç”¨ï¼Œè¯·é€‰æ‹©å…¶ä¸€")
            
            if self.use_full_vocab_split:
                # å…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼šå°†æ•´ä¸ªè¯è¡¨é€šè¿‡è¯­ä¹‰è¯„åˆ†åˆ‡åˆ†æˆè¶‹åŠ¿æ¡¶å’Œç»†èŠ‚æ¡¶
                from utils.vocab_splitter import split_full_vocab_by_semantics
                
                print("\n" + "=" * 70)
                print("[TimeLLM] ğŸ”„ å¼€å§‹å…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†...")
                print("=" * 70)
                
                # æ‰§è¡Œå…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†
                trend_vocab_indices, detail_vocab_indices = split_full_vocab_by_semantics(
                    tokenizer=self.tokenizer,
                    word_embeddings=self.word_embeddings,
                    trend_anchors=None,  # ä½¿ç”¨é»˜è®¤é”šç‚¹
                    detail_anchors=None,
                    verbose=True
                )
                
                # æå–åˆ‡åˆ†åçš„ embeddings å¹¶æ³¨å†Œä¸º Bufferï¼ˆå›ºå®šï¼Œä¸æ›´æ–°ï¼‰
                trend_vocab_embeddings = self.word_embeddings[trend_vocab_indices].detach()
                detail_vocab_embeddings = self.word_embeddings[detail_vocab_indices].detach()
                
                self.register_buffer('trend_vocab_embeddings', trend_vocab_embeddings)
                self.register_buffer('detail_vocab_embeddings', detail_vocab_embeddings)
                
                # ä¿å­˜ç´¢å¼•ï¼ˆç”¨äºè°ƒè¯•å’Œå¯è§†åŒ–ï¼‰
                self.register_buffer('trend_vocab_indices', trend_vocab_indices)
                self.register_buffer('detail_vocab_indices', detail_vocab_indices)
                
                # çº¿æ€§æ˜ å°„å±‚ï¼šä»åˆ‡åˆ†åçš„è¯æ•°æ˜ å°„åˆ°åŸå‹æ•°é‡ï¼ˆå’ŒåŸç‰ˆTimeLLMä¸€æ ·ï¼‰
                # è¾“å…¥: (num_trend_vocab, d_llm) -> è½¬ç½®ä¸º (d_llm, num_trend_vocab) -> Linear -> (d_llm, num_trend_tokens) -> è½¬ç½®å› (num_trend_tokens, d_llm)
                self.trend_mapping = nn.Linear(len(trend_vocab_indices), self.num_trend_tokens)
                self.detail_mapping = nn.Linear(len(detail_vocab_indices), self.num_detail_tokens)
                
                self.trend_seed_embeddings = None
                self.detail_seed_embeddings = None
                self.mapping_layer = None
                
                # è®¡ç®—å‚æ•°é‡
                trend_params = len(trend_vocab_indices) * self.num_trend_tokens
                detail_params = len(detail_vocab_indices) * self.num_detail_tokens
                total_params = trend_params + detail_params
                
                print("=" * 70)
                print("[TimeLLM] âœ… å¯ç”¨å…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼ˆåˆ†ç¦»åŸå‹ï¼‰")
                print("=" * 70)
                print(f"  â”œâ”€ è¶‹åŠ¿æ¡¶: {len(trend_vocab_indices):,} ä¸ªè¯ â†’ {self.num_trend_tokens} ä¸ªè¶‹åŠ¿åŸå‹")
                print(f"  â”œâ”€ ç»†èŠ‚æ¡¶: {len(detail_vocab_indices):,} ä¸ªè¯ â†’ {self.num_detail_tokens} ä¸ªç»†èŠ‚åŸå‹")
                print(f"  â”œâ”€ æ˜ å°„å±‚é…ç½®ï¼ˆçº¿æ€§æ˜ å°„ï¼Œå’ŒåŸç‰ˆTimeLLMä¸€æ ·ï¼‰:")
                print(f"  â”‚   â”œâ”€ è¶‹åŠ¿æ˜ å°„: Linear({len(trend_vocab_indices):,} â†’ {self.num_trend_tokens})")
                print(f"  â”‚   â”‚   â””â”€ å‚æ•°é‡: {trend_params:,} ({trend_params/1e6:.2f}M)")
                print(f"  â”‚   â””â”€ ç»†èŠ‚æ˜ å°„: Linear({len(detail_vocab_indices):,} â†’ {self.num_detail_tokens})")
                print(f"  â”‚       â””â”€ å‚æ•°é‡: {detail_params:,} ({detail_params/1e6:.2f}M)")
                print(f"  â”œâ”€ æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
                print(f"  â”œâ”€ BufferçŠ¶æ€: åˆ‡åˆ†åçš„è¯embeddingså·²æ³¨å†Œä¸ºBufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰")
                print(f"  â””â”€ æ•°æ®æµ: å…¨è¯è¡¨åˆ‡åˆ† â†’ è¶‹åŠ¿/ç»†èŠ‚æ¡¶(Buffer) â†’ çº¿æ€§æ˜ å°„å±‚(å¯å­¦ä¹ ) â†’ åŸå‹è¯ â†’ ReprogrammingLayer")
                print("=" * 70)
            elif self.use_semantic_filtered_mapping:
                # è¯­ä¹‰ç­›é€‰æ˜ å°„æ¨¡å¼ï¼šä½¿ç”¨ç­›é€‰å‡ºçš„ç§å­è¯ä½œä¸ºè¾“å…¥æº
                num_trend_seed_words = int(getattr(configs, 'dual_proto_trend_seed_words', 300))
                num_detail_seed_words = int(getattr(configs, 'dual_proto_detail_seed_words', 700))
                use_semantic_filter = bool(getattr(configs, 'dual_proto_seed_semantic_filter', 1))
                
                # ç­›é€‰ç§å­è¯
                trend_seed_indices, detail_seed_indices = select_seed_words(
                    tokenizer=self.tokenizer,
                    word_embeddings=self.word_embeddings,
                    num_trend_words=num_trend_seed_words,
                    num_detail_words=num_detail_seed_words,
                    use_semantic_filter=use_semantic_filter,
                    ensure_disjoint=True
                )
                
                # æå–ç§å­è¯çš„ embeddings å¹¶æ³¨å†Œä¸º Bufferï¼ˆå›ºå®šï¼Œä¸æ›´æ–°ï¼‰
                trend_seed_embeddings = self.word_embeddings[trend_seed_indices].detach()
                detail_seed_embeddings = self.word_embeddings[detail_seed_indices].detach()
                
                self.register_buffer('trend_seed_embeddings', trend_seed_embeddings)
                self.register_buffer('detail_seed_embeddings', detail_seed_embeddings)
                
                # æ˜ å°„å±‚ï¼šä»ç§å­è¯æ•°é‡æ˜ å°„åˆ°åŸå‹æ•°é‡ï¼ˆç­–ç•¥ä¸€ï¼šMLPéçº¿æ€§æ˜ å°„ï¼‰
                # è¾“å…¥: (num_seed_words, d_llm) -> è½¬ç½®ä¸º (d_llm, num_seed_words) -> MLP -> (d_llm, num_prototypes) -> è½¬ç½®å› (num_prototypes, d_llm)
                # ç­–ç•¥ï¼šä½¿ç”¨ MLP(num_seed_words -> hidden_dim -> num_prototypes) å¯¹è½¬ç½®åçš„çŸ©é˜µè¿›è¡Œéçº¿æ€§æ˜ å°„
                # ä¼˜åŠ¿ï¼šéçº¿æ€§æ¿€æ´»ï¼ˆGELUï¼‰å…è®¸æ¨¡å‹å­¦ä¹ æ–‡æœ¬è¯­ä¹‰ç©ºé—´åˆ°æ—¶åºè¯­ä¹‰ç©ºé—´çš„å¤æ‚æ˜ å°„
                
                # MLPé…ç½®å‚æ•°
                mlp_hidden_dim = int(getattr(configs, 'dual_proto_mlp_hidden_dim', 4096))
                mlp_dropout = float(getattr(configs, 'dual_proto_mlp_dropout', 0.1))
                
                # è¶‹åŠ¿æ˜ å°„ï¼šMLP with bottleneck expansion
                self.trend_mapping = nn.Sequential(
                    nn.Linear(len(trend_seed_indices), mlp_hidden_dim),  # å‡ç»´ï¼šå±•å¼€ä¿¡æ¯ç©ºé—´
                    nn.GELU(),                                             # éçº¿æ€§æ¿€æ´»ï¼šæ‰“ç ´è¯­ä¹‰ç©ºé—´åˆšæ€§ç»“æ„
                    nn.Dropout(mlp_dropout),                               # é˜²æ­¢è¿‡æ‹Ÿåˆ
                    nn.Linear(mlp_hidden_dim, self.num_trend_tokens)       # é™ç»´ï¼šæŠ•å½±åˆ°åŸå‹ç©ºé—´
                )
                
                # ç»†èŠ‚æ˜ å°„ï¼šMLP with bottleneck expansion
                self.detail_mapping = nn.Sequential(
                    nn.Linear(len(detail_seed_indices), mlp_hidden_dim),  # å‡ç»´ï¼šå±•å¼€ä¿¡æ¯ç©ºé—´
                    nn.GELU(),                                             # éçº¿æ€§æ¿€æ´»ï¼šæ‰“ç ´è¯­ä¹‰ç©ºé—´åˆšæ€§ç»“æ„
                    nn.Dropout(mlp_dropout),                               # é˜²æ­¢è¿‡æ‹Ÿåˆ
                    nn.Linear(mlp_hidden_dim, self.num_detail_tokens)      # é™ç»´ï¼šæŠ•å½±åˆ°åŸå‹ç©ºé—´
                )
                
                self.mapping_layer = None
                
                # è®¡ç®—å‚æ•°é‡
                trend_mlp_params = (len(trend_seed_indices) * mlp_hidden_dim + 
                                   mlp_hidden_dim * self.num_trend_tokens)
                detail_mlp_params = (len(detail_seed_indices) * mlp_hidden_dim + 
                                     mlp_hidden_dim * self.num_detail_tokens)
                total_mlp_params = trend_mlp_params + detail_mlp_params
                
                print("=" * 70)
                print("[TimeLLM] âœ… å¯ç”¨åˆ†ç¦»åŸå‹æ¨¡å¼ï¼ˆè¯­ä¹‰ç­›é€‰æ˜ å°„ + MLPéçº¿æ€§æ˜ å°„ï¼‰")
                print("=" * 70)
                print(f"  â”œâ”€ è¶‹åŠ¿ç§å­è¯: {len(trend_seed_indices)} ä¸ª â†’ {self.num_trend_tokens} ä¸ªè¶‹åŠ¿åŸå‹")
                print(f"  â”œâ”€ ç»†èŠ‚ç§å­è¯: {len(detail_seed_indices)} ä¸ª â†’ {self.num_detail_tokens} ä¸ªç»†èŠ‚åŸå‹")
                print(f"  â”œâ”€ è¯­ä¹‰è¿‡æ»¤: {'âœ… å¯ç”¨' if use_semantic_filter else 'âŒ å…³é—­'}")
                print(f"  â”œâ”€ æ˜ å°„å±‚é…ç½®ï¼ˆç­–ç•¥ä¸€ï¼šMLPéçº¿æ€§æ˜ å°„ï¼‰:")
                print(f"  â”‚   â”œâ”€ è¶‹åŠ¿æ˜ å°„: MLP({len(trend_seed_indices)} â†’ {mlp_hidden_dim} â†’ {self.num_trend_tokens})")
                print(f"  â”‚   â”‚   â””â”€ å‚æ•°é‡: {trend_mlp_params:,} ({trend_mlp_params/1e6:.2f}M)")
                print(f"  â”‚   â””â”€ ç»†èŠ‚æ˜ å°„: MLP({len(detail_seed_indices)} â†’ {mlp_hidden_dim} â†’ {self.num_detail_tokens})")
                print(f"  â”‚       â””â”€ å‚æ•°é‡: {detail_mlp_params:,} ({detail_mlp_params/1e6:.2f}M)")
                print(f"  â”œâ”€ MLPæ€»å‚æ•°é‡: {total_mlp_params:,} ({total_mlp_params/1e6:.2f}M)")
                print(f"  â”œâ”€ æ¿€æ´»å‡½æ•°: GELU (éçº¿æ€§æ˜ å°„)")
                print(f"  â”œâ”€ Dropoutç‡: {mlp_dropout}")
                print(f"  â”œâ”€ BufferçŠ¶æ€: ç§å­è¯embeddingså·²æ³¨å†Œä¸ºBufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰")
                print(f"  â””â”€ æ•°æ®æµ: ç§å­è¯(Buffer) â†’ MLPæ˜ å°„å±‚(å¯å­¦ä¹ ) â†’ åŸå‹è¯ â†’ ReprogrammingLayer")
                print("=" * 70)
            else:
                # åŸç‰ˆæ˜ å°„æ¨¡å¼ï¼šä½¿ç”¨æ•´ä¸ªè¯è¡¨
                self.trend_mapping = nn.Linear(self.vocab_size, self.num_trend_tokens)
                self.detail_mapping = nn.Linear(self.vocab_size, self.num_detail_tokens)
                self.trend_seed_embeddings = None
                self.detail_seed_embeddings = None
                self.mapping_layer = None
                print(f"[TimeLLM] âœ… å¯ç”¨åˆ†ç¦»åŸå‹æ¨¡å¼: {self.num_trend_tokens} è¶‹åŠ¿ + {self.num_detail_tokens} ç»†èŠ‚")
        else:
            # åŸç‰ˆæ¨¡å¼ï¼š1000 ä¸ªå…±äº«åŸå‹
            self.trend_mapping = None
            self.detail_mapping = None
            self.trend_seed_embeddings = None
            self.detail_seed_embeddings = None
            self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)
            print(f"[TimeLLM] ä½¿ç”¨åŸç‰ˆæ˜ å°„å±‚: 1000 ä¸ªå…±äº«åŸå‹")

        # CWPR é…ç½®
        self.use_cwpr = bool(getattr(configs, 'use_cwpr', 0))
        
        if self.use_cwpr:
            # ä½¿ç”¨ CWPR é‡ç¼–ç¨‹å±‚
            cwpr_num_prototypes = int(getattr(configs, 'cwpr_num_prototypes', 256))
            cwpr_n_heads = int(getattr(configs, 'cwpr_n_heads', configs.n_heads))
            cwpr_dropout = float(getattr(configs, 'cwpr_dropout', 0.1))
            cwpr_gate_bias_init = float(getattr(configs, 'cwpr_gate_bias_init', 2.0))
            cwpr_proto_init = getattr(configs, 'cwpr_proto_init', 'random')
            cwpr_use_kmeans = bool(getattr(configs, 'cwpr_use_kmeans', 0))
            cwpr_top_n_words = getattr(configs, 'cwpr_top_n_words', None)
            if cwpr_top_n_words is not None:
                cwpr_top_n_words = int(cwpr_top_n_words)
            
            # å¦‚æœä½¿ç”¨word_embedåˆå§‹åŒ–ï¼Œéœ€è¦æä¾›è¯åµŒå…¥
            word_embeddings_for_init = None
            if cwpr_proto_init == 'word_embed':
                word_embeddings_for_init = self.word_embeddings
            
            # K-Meansä»…åœ¨word_embedæ¨¡å¼ä¸‹æœ‰æ•ˆ
            if cwpr_use_kmeans and cwpr_proto_init != 'word_embed':
                print(f"[TimeLLM] è­¦å‘Š: cwpr_use_kmeans=True ä½† cwpr_proto_init='{cwpr_proto_init}'ï¼Œ"
                      f"K-Meansä»…åœ¨word_embedæ¨¡å¼ä¸‹æœ‰æ•ˆï¼Œå°†å¿½ç•¥use_kmeanså‚æ•°")
                cwpr_use_kmeans = False
            
            # Top-Nä»…åœ¨K-Meansæ¨¡å¼ä¸‹æœ‰æ•ˆ
            if cwpr_top_n_words is not None and not cwpr_use_kmeans:
                print(f"[TimeLLM] è­¦å‘Š: cwpr_top_n_words={cwpr_top_n_words} ä½† cwpr_use_kmeans=Falseï¼Œ"
                      f"Top-Nä»…åœ¨K-Meansæ¨¡å¼ä¸‹æœ‰æ•ˆï¼Œå°†å¿½ç•¥top_n_wordså‚æ•°")
                cwpr_top_n_words = None
            
            # è¯­ä¹‰è¿‡æ»¤é€‰é¡¹ï¼šé€‰æ‹©ä¸æ—¶é—´åºåˆ—/å°æ³¢ç‰¹å¾ç›¸å…³çš„è¯æ±‡
            cwpr_use_semantic_filter = bool(getattr(configs, 'cwpr_use_semantic_filter', 0))
            
            self.cwpr_layer = CWPRReprogrammingLayer(
                d_model=configs.d_model,
                d_llm=self.d_llm,
                n_heads=cwpr_n_heads,
                num_prototypes=cwpr_num_prototypes,
                d_keys=self.d_ff // cwpr_n_heads,  # ä½¿ç”¨d_ffè®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦
                attention_dropout=cwpr_dropout,
                gate_bias_init=cwpr_gate_bias_init,
                init_method=cwpr_proto_init,
                word_embeddings=word_embeddings_for_init,
                use_kmeans=cwpr_use_kmeans,
                top_n_words=cwpr_top_n_words,
                tokenizer=self.tokenizer if cwpr_top_n_words is not None else None,
                use_semantic_filter=cwpr_use_semantic_filter
            )
            self.reprogramming_layer = None
            print(f"[TimeLLM] âœ… CWPRæ¶æ„å·²å¯ç”¨")
            print(f"[TimeLLM]   ä½¿ç”¨ CWPRReprogrammingLayer (åŸå‹æ•°={cwpr_num_prototypes}, å¤´æ•°={cwpr_n_heads})")
            init_method_desc = f"{cwpr_proto_init}"
            if cwpr_proto_init == 'word_embed' and cwpr_use_kmeans:
                if cwpr_top_n_words is not None:
                    init_method_desc += f" (K-Meansèšç±», Top-{cwpr_top_n_words}å¸¸ç”¨è¯)"
                else:
                    init_method_desc += " (K-Meansèšç±», å…¨è¯è¡¨)"
            elif cwpr_proto_init == 'word_embed':
                init_method_desc += " (éšæœºé‡‡æ ·)"
            print(f"[TimeLLM]   åŸå‹åˆå§‹åŒ–: {init_method_desc}")
            print(f"[TimeLLM]   æ•°æ®æµ: WIST(forward_separated) â†’ e_cA/e_detail â†’ CWPR â†’ LLM")
        else:
            # ä½¿ç”¨åŸç‰ˆ ReprogrammingLayer æˆ– DualReprogrammingLayer
            if self.use_dual_prototypes:
                # ä½¿ç”¨åˆ†ç¦»åŸå‹å±‚
                fusion_method = getattr(configs, 'dual_proto_fusion_method', 'mean')
                # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®é™…è¯»å–åˆ°çš„å€¼ï¼ˆå¸®åŠ©è¯Šæ–­å‚æ•°ä¼ é€’é—®é¢˜ï¼‰
                if hasattr(configs, 'dual_proto_fusion_method'):
                    print(f"[TimeLLM] ğŸ” è°ƒè¯•: configs.dual_proto_fusion_method = '{configs.dual_proto_fusion_method}'")
                else:
                    print(f"[TimeLLM] âš ï¸  è­¦å‘Š: configs ä¸­æ²¡æœ‰ dual_proto_fusion_method å±æ€§ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'mean'")
                print(f"[TimeLLM] ğŸ” è°ƒè¯•: æœ€ç»ˆä½¿ç”¨çš„ fusion_method = '{fusion_method}'")
                gate_bias_init = float(getattr(configs, 'dual_proto_gate_bias_init', 0.0))
                self.reprogramming_layer = DualReprogrammingLayer(
                    configs.d_model, 
                    configs.n_heads, 
                    self.d_ff // configs.n_heads, 
                    self.d_llm,
                    attention_dropout=configs.dropout,
                    fusion_method=fusion_method,
                    gate_bias_init=gate_bias_init
                )
                # ä¿å­˜èåˆæ–¹æ³•ï¼Œç”¨äºè¾“å‡ºå¤´é€‚é…
                self.fusion_method = fusion_method
                self.cwpr_layer = None
                num_trend = getattr(self, 'num_trend_tokens', 1000)
                num_detail = getattr(self, 'num_detail_tokens', 1000)
                print(f"[TimeLLM] ä½¿ç”¨ DualReprogrammingLayer (åˆ†ç¦»åŸå‹: {num_trend}+{num_detail}, èåˆæ–¹æ³•={fusion_method})")
                if fusion_method == 'interleave':
                    print(f"[TimeLLM] âš ï¸  äº¤é”™æ‹¼æ¥æ¨¡å¼ï¼šåºåˆ—é•¿åº¦å°†ç¿»å€ (L â†’ 2L)ï¼Œè¾“å‡ºå¤´å°†è‡ªåŠ¨é€‚é…")
                elif fusion_method == 'channel_concat':
                    print(f"[TimeLLM] âœ… é€šé“æ‹¼æ¥æ¨¡å¼ï¼šåºåˆ—é•¿åº¦ä¿æŒä¸å˜ (L)ï¼Œç‰¹å¾ç»´åº¦æ‹¼æ¥åæŠ•å½±")
            else:
                # ä½¿ç”¨åŸç‰ˆ ReprogrammingLayer
                self.reprogramming_layer = ReprogrammingLayer(configs.d_model, configs.n_heads, self.d_ff, self.d_llm)
                self.fusion_method = None  # åŸç‰ˆä¸ä½¿ç”¨èåˆæ–¹æ³•
                self.cwpr_layer = None
                print("[TimeLLM] ä½¿ç”¨ ReprogrammingLayer (åŸç‰ˆ)")

        self.patch_nums = int((configs.seq_len - self.patch_len) / self.stride + 2)
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº¤é”™æ‹¼æ¥æ¨¡å¼ï¼ˆåºåˆ—é•¿åº¦ç¿»å€ï¼‰
        fusion_method = getattr(configs, 'dual_proto_fusion_method', 'mean') if getattr(configs, 'use_dual_prototypes', 0) else None
        use_interleave = (fusion_method == 'interleave')
        # head_nf éœ€è¦æ ¹æ®æ˜¯å¦ä½¿ç”¨äº¤é”™æ‹¼æ¥æ¥è°ƒæ•´
        self.head_nf = self.d_ff * (2 * self.patch_nums if use_interleave else self.patch_nums)

        # è¾“å‡ºå¤´é€‰æ‹©ï¼šåŒå°ºåº¦æ®‹å·®å¤´ vs é¢‘ç‡è§£è€¦å¤´ vs åŸå§‹ FlattenHead
        self.use_dual_scale_head = getattr(configs, 'use_dual_scale_head', 0)
        self.use_freq_decoupled_head = getattr(configs, 'use_freq_decoupled_head', 0)
        
        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':
            if self.use_dual_scale_head:
                # åŒå°ºåº¦æ®‹å·®è¾“å‡ºå¤´ (Dual-Scale Residual Head)
                # å¦‚æœä½¿ç”¨äº¤é”™æ‹¼æ¥ï¼Œpatch_nums éœ€è¦ç¿»å€
                effective_patch_nums = 2 * self.patch_nums if use_interleave else self.patch_nums
                self.output_projection = DualScaleResidualHead(
                    n_vars=configs.enc_in,
                    d_ff=self.d_ff,
                    patch_nums=effective_patch_nums,
                    target_window=self.pred_len,
                    head_dropout=configs.dropout,
                    detail_dropout=getattr(configs, 'detail_dropout', 0.0),
                )
                print("[TimeLLM] ä½¿ç”¨ DualScaleResidualHead (åŒå°ºåº¦æ®‹å·®è¾“å‡ºå¤´)")
                if use_interleave:
                    print(f"[TimeLLM] âš ï¸  äº¤é”™æ‹¼æ¥æ¨¡å¼ï¼šDualScaleResidualHead å·²é€‚é… 2*patch_nums={effective_patch_nums}")
            elif self.use_freq_decoupled_head:
                # ä¸‰é¢‘å¸¦è§£è€¦è¾“å‡ºå¤´ (Tri-Band Decoupled Head)
                self.output_projection = TriBandDecoupledHead(
                    n_vars=configs.enc_in,
                    nf=self.head_nf,
                    target_window=self.pred_len,
                    head_dropout=configs.dropout,
                    mid_dropout=getattr(configs, 'mid_dropout', 0.2),
                    high_dropout=getattr(configs, 'high_dropout', 0.5),
                    use_soft_threshold=bool(getattr(configs, 'head_soft_threshold', 1)),
                    soft_threshold_init=getattr(configs, 'head_soft_threshold_init', 0.1),
                    use_conv=bool(getattr(configs, 'head_use_conv', 0)),
                )
                print("[TimeLLM] ä½¿ç”¨ TriBandDecoupledHead (ä¸‰é¢‘å¸¦è§£è€¦è¾“å‡ºå¤´)")
            else:
                # åŸå§‹ FlattenHead
                self.output_projection = FlattenHead(configs.enc_in, self.head_nf, self.pred_len,
                                                     head_dropout=configs.dropout)
                print("[TimeLLM] ä½¿ç”¨ FlattenHead (åŸç‰ˆè¾“å‡ºå¤´)")
        else:
            raise NotImplementedError

        self.normalize_layers = Normalize(configs.enc_in, affine=False)
        
        # å°æ³¢Promptå¢å¼ºåŠŸèƒ½é…ç½®
        self.use_wavelet_prompt = getattr(configs, 'use_wavelet_prompt', 0)
        self.wavelet_prompt_method = getattr(configs, 'wavelet_prompt_method', 'haar')
        self.prompt_hfer_threshold = getattr(configs, 'prompt_hfer_threshold', 0.15)
        
        if self.use_wavelet_prompt:
            print(f"[TimeLLM] å°æ³¢Promptå¢å¼ºå·²å¯ç”¨")
            print(f"  - åˆ†ææ–¹æ³•: {self.wavelet_prompt_method}")
            print(f"  - HFERé˜ˆå€¼: {self.prompt_hfer_threshold}")
        else:
            print(f"[TimeLLM] ä½¿ç”¨åŸç‰ˆPromptï¼ˆæ— å°æ³¢ç‰¹å¾ï¼‰")

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None, return_components=False):
        """
        Args:
            return_components: æ˜¯å¦è¿”å›é¢‘ç‡åˆ†é‡ (ç”¨äºæ·±åº¦ç›‘ç£è®­ç»ƒ)
        
        Returns:
            dec_out: é¢„æµ‹ç»“æœ
            components: (å¯é€‰) é¢‘ç‡åˆ†é‡å­—å…¸ï¼Œå½“ return_components=True ä¸”ä½¿ç”¨ TriBandDecoupledHead æ—¶è¿”å›
        """
        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':
            result = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec, return_components)
            if return_components and self.use_freq_decoupled_head:
                dec_out, components = result
                return dec_out[:, -self.pred_len:, :], components
            else:
                dec_out = result
                return dec_out[:, -self.pred_len:, :]
        return None

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec, return_components=False):

        x_enc = self.normalize_layers(x_enc, 'norm')

        B, T, N = x_enc.size()
        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)

        min_values = torch.min(x_enc, dim=1)[0]
        max_values = torch.max(x_enc, dim=1)[0]
        medians = torch.median(x_enc, dim=1).values
        lags = self.calcute_lags(x_enc)
        trends = x_enc.diff(dim=1).sum(dim=1)

        prompt = []
        for b in range(x_enc.shape[0]):
            # æ ¼å¼åŒ–ç»Ÿè®¡å€¼ï¼ˆä¿ç•™åˆç†ç²¾åº¦ï¼‰
            min_val = min_values[b].tolist()[0]
            max_val = max_values[b].tolist()[0]
            median_val = medians[b].tolist()[0]
            
            min_values_str = f"{min_val:.3f}"
            max_values_str = f"{max_val:.3f}"
            median_values_str = f"{median_val:.3f}"
            lags_values_str = str(lags[b].tolist())
            
            # === æ¡ä»¶æ‰§è¡Œï¼šå°æ³¢ç‰¹å¾åˆ†æ ===
            wavelet_desc = ""
            if self.use_wavelet_prompt:
                # è·å–å½“å‰æ ·æœ¬çš„æ—¶é—´åºåˆ— (T,)
                current_x = x_enc[b, :, 0]  # å–ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆå·²ç»æ˜¯å•å˜é‡ï¼‰
                hfer, volatility, smoothness_level = self.analyze_wavelet_features(current_x)
                wavelet_desc = self.get_wavelet_description(hfer, volatility, smoothness_level)
            # ==========================
            
            # æ ¹æ®æ˜¯å¦å¯ç”¨å°æ³¢promptæ„å»ºä¸åŒçš„prompt
            if self.use_wavelet_prompt and wavelet_desc:
                prompt_ = (
                    f"<|start_prompt|>Dataset description: {self.description}"
                    f"Task description: forecast the next {str(self.pred_len)} steps given the previous {str(self.seq_len)} steps information; "
                    "Input statistics: "
                    f"min value {min_values_str}, "
                    f"max value {max_values_str}, "
                    f"median value {median_values_str}, "
                    f"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, "
                    f"top 5 lags are : {lags_values_str}; "
                    f"Frequency characteristics: {wavelet_desc}."
                    f"<|<end_prompt>|>"
                )
            else:
                # åŸç‰ˆpromptï¼ˆæ— å°æ³¢ç‰¹å¾ï¼‰
                prompt_ = (
                    f"<|start_prompt|>Dataset description: {self.description}"
                    f"Task description: forecast the next {str(self.pred_len)} steps given the previous {str(self.seq_len)} steps information; "
                    "Input statistics: "
                    f"min value {min_values_str}, "
                    f"max value {max_values_str}, "
                    f"median value {median_values_str}, "
                    f"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, "
                    f"top 5 lags are : {lags_values_str}<|<end_prompt>|>"
                )

            prompt.append(prompt_)

        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous()

        prompt = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=2048).input_ids
        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)

        x_enc = x_enc.permute(0, 2, 1).contiguous()
        # ç›´æ¥ä½¿ç”¨ float32 é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜
        if self.use_cwpr:
            # CWPR æ¨¡å¼ï¼šä½¿ç”¨åˆ†ç¦»çš„ç‰¹å¾è¾“å‡º
            e_cA, e_detail, n_vars = self.patch_embedding.forward_separated(x_enc.float())
            enc_out = self.cwpr_layer(e_cA, e_detail)
        else:
            # åŸç‰ˆæ¨¡å¼æˆ–åˆ†ç¦»åŸå‹æ¨¡å¼
            if self.use_dual_prototypes and self.wavelet_mode == 'wist':
                # åˆ†ç¦»åŸå‹æ¨¡å¼ + WISTï¼šä½¿ç”¨åˆ†ç¦»çš„ç‰¹å¾è¾“å‡ºï¼Œåˆ†åˆ«å­¦ä¹ 
                e_cA, e_detail, n_vars = self.patch_embedding.forward_separated(x_enc.float())
                # ç”Ÿæˆè¶‹åŠ¿å’Œç»†èŠ‚ä¸¤ä¸ªåŸå‹åº“
                if self.use_full_vocab_split:
                    # å…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼šä½¿ç”¨åˆ‡åˆ†åçš„è¯ embeddings + çº¿æ€§æ˜ å°„ï¼ˆå’ŒåŸç‰ˆTimeLLMä¸€æ ·ï¼‰
                    # trend_vocab_embeddings: (num_trend_vocab, d_llm) -> è½¬ç½® -> (d_llm, num_trend_vocab)
                    # Linear(num_trend_vocab -> num_trend_tokens) -> (d_llm, num_trend_tokens) -> è½¬ç½®å› (num_trend_tokens, d_llm)
                    trend_prototypes = self.trend_mapping(self.trend_vocab_embeddings.permute(1, 0)).permute(1, 0)  # (num_trend_tokens, d_llm)
                    detail_prototypes = self.detail_mapping(self.detail_vocab_embeddings.permute(1, 0)).permute(1, 0)  # (num_detail_tokens, d_llm)
                elif self.use_semantic_filtered_mapping:
                    # è¯­ä¹‰ç­›é€‰æ˜ å°„æ¨¡å¼ï¼šä½¿ç”¨ Buffer ä¸­çš„ç§å­è¯ embeddings + MLPéçº¿æ€§æ˜ å°„
                    # trend_seed_embeddings: (num_trend_seed_words, d_llm) -> è½¬ç½® -> (d_llm, num_trend_seed_words)
                    # MLP(num_trend_seed_words -> hidden_dim -> num_trend_tokens) -> (d_llm, num_trend_tokens) -> è½¬ç½®å› (num_trend_tokens, d_llm)
                    trend_prototypes = self.trend_mapping(self.trend_seed_embeddings.permute(1, 0)).permute(1, 0)  # (num_trend_tokens, d_llm)
                    detail_prototypes = self.detail_mapping(self.detail_seed_embeddings.permute(1, 0)).permute(1, 0)  # (num_detail_tokens, d_llm)
                else:
                    # åŸç‰ˆæ˜ å°„æ¨¡å¼ï¼šä½¿ç”¨æ•´ä¸ªè¯è¡¨
                    trend_prototypes = self.trend_mapping(self.word_embeddings.permute(1, 0)).permute(1, 0)  # (num_trend_tokens, d_llm)
                    detail_prototypes = self.detail_mapping(self.word_embeddings.permute(1, 0)).permute(1, 0)  # (num_detail_tokens, d_llm)
                # åˆ†åˆ«ä½¿ç”¨è¶‹åŠ¿ç‰¹å¾å’Œç»†èŠ‚ç‰¹å¾è¿›è¡Œå­¦ä¹ 
                enc_out = self.reprogramming_layer(e_cA, e_detail, trend_prototypes, detail_prototypes)
            elif self.use_dual_prototypes:
                # åˆ†ç¦»åŸå‹æ¨¡å¼ä½†é WISTï¼šä½¿ç”¨èåˆåçš„ç‰¹å¾ï¼ˆå‘åå…¼å®¹ï¼‰
                enc_out, n_vars = self.patch_embedding(x_enc.float())
                # ç”Ÿæˆè¶‹åŠ¿å’Œç»†èŠ‚ä¸¤ä¸ªåŸå‹åº“
                if self.use_full_vocab_split:
                    # å…¨è¯è¡¨åˆ‡åˆ†æ¨¡å¼ï¼šä½¿ç”¨åˆ‡åˆ†åçš„è¯ embeddings + çº¿æ€§æ˜ å°„ï¼ˆå’ŒåŸç‰ˆTimeLLMä¸€æ ·ï¼‰
                    # trend_vocab_embeddings: (num_trend_vocab, d_llm) -> è½¬ç½® -> (d_llm, num_trend_vocab)
                    # Linear(num_trend_vocab -> num_trend_tokens) -> (d_llm, num_trend_tokens) -> è½¬ç½®å› (num_trend_tokens, d_llm)
                    trend_prototypes = self.trend_mapping(self.trend_vocab_embeddings.permute(1, 0)).permute(1, 0)  # (num_trend_tokens, d_llm)
                    detail_prototypes = self.detail_mapping(self.detail_vocab_embeddings.permute(1, 0)).permute(1, 0)  # (num_detail_tokens, d_llm)
                elif self.use_semantic_filtered_mapping:
                    # è¯­ä¹‰ç­›é€‰æ˜ å°„æ¨¡å¼ï¼šä½¿ç”¨ Buffer ä¸­çš„ç§å­è¯ embeddings + MLPéçº¿æ€§æ˜ å°„
                    # trend_seed_embeddings: (num_trend_seed_words, d_llm) -> è½¬ç½® -> (d_llm, num_trend_seed_words)
                    # MLP(num_trend_seed_words -> hidden_dim -> num_trend_tokens) -> (d_llm, num_trend_tokens) -> è½¬ç½®å› (num_trend_tokens, d_llm)
                    trend_prototypes = self.trend_mapping(self.trend_seed_embeddings.permute(1, 0)).permute(1, 0)  # (num_trend_tokens, d_llm)
                    detail_prototypes = self.detail_mapping(self.detail_seed_embeddings.permute(1, 0)).permute(1, 0)  # (num_detail_tokens, d_llm)
                else:
                    # åŸç‰ˆæ˜ å°„æ¨¡å¼ï¼šä½¿ç”¨æ•´ä¸ªè¯è¡¨
                    trend_prototypes = self.trend_mapping(self.word_embeddings.permute(1, 0)).permute(1, 0)  # (num_trend_tokens, d_llm)
                    detail_prototypes = self.detail_mapping(self.word_embeddings.permute(1, 0)).permute(1, 0)  # (num_detail_tokens, d_llm)
                # ä½¿ç”¨èåˆåçš„ç‰¹å¾ï¼ˆä¸¤ä¸ªæµéƒ½ä½¿ç”¨ç›¸åŒçš„è¾“å…¥ï¼Œä½†åŸå‹åº“ä¸åŒï¼‰
                enc_out = self.reprogramming_layer(enc_out, enc_out, trend_prototypes, detail_prototypes)
            else:
                # åŸç‰ˆæ¨¡å¼ï¼šä½¿ç”¨èåˆåçš„ç‰¹å¾å’Œå•ä¸€åŸå‹åº“
                enc_out, n_vars = self.patch_embedding(x_enc.float())
                source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)  # (1000, d_llm)
                enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)
        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)
        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
        dec_out = dec_out[:, :, :self.d_ff]

        dec_out = torch.reshape(
            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))
        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()

        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº¤é”™æ‹¼æ¥æ¨¡å¼ï¼ˆåºåˆ—é•¿åº¦ç¿»å€ï¼‰
        use_interleave = (hasattr(self, 'fusion_method') and 
                         self.fusion_method == 'interleave')
        
        if use_interleave:
            # äº¤é”™æ‹¼æ¥æ¨¡å¼ï¼šåºåˆ—é•¿åº¦æ˜¯ 2*patch_nums
            # å–å 2*patch_nums ä¸ª tokenï¼ˆåŒ…å«æ‰€æœ‰è¶‹åŠ¿å’Œç»†èŠ‚ä¿¡æ¯ï¼‰
            num_tokens_to_take = 2 * self.patch_nums
        else:
            # æ™®é€šæ¨¡å¼ï¼šåºåˆ—é•¿åº¦æ˜¯ patch_nums
            num_tokens_to_take = self.patch_nums
        
        # è¾“å‡ºæŠ•å½±
        if self.use_freq_decoupled_head and return_components:
            # ä½¿ç”¨ä¸‰é¢‘å¸¦è§£è€¦å¤´ï¼Œè¿”å›åˆ†é‡ç”¨äºæ·±åº¦ç›‘ç£
            dec_out, components = self.output_projection(
                dec_out[:, :, :, -num_tokens_to_take:], 
                return_components=True
            )
            # æ³¨æ„ï¼šTriBandDecoupledHead å·²ç»åšäº† permuteï¼Œè¾“å‡ºæ˜¯ (B, pred_len, N)
            dec_out = self.normalize_layers(dec_out, 'denorm')
            # åˆ†é‡ä¹Ÿéœ€è¦ denorm
            for k in components:
                components[k] = self.normalize_layers(components[k], 'denorm')
            return dec_out, components
        else:
            dec_out = self.output_projection(dec_out[:, :, :, -num_tokens_to_take:])
            if self.use_dual_scale_head or self.use_freq_decoupled_head:
                # DualScaleHead å’Œ TriBandDecoupledHead è¾“å‡ºå·²ç»æ˜¯ (B, pred_len, N)
                dec_out = self.normalize_layers(dec_out, 'denorm')
            else:
                # FlattenHead è¾“å‡ºæ˜¯ (B, N, pred_len)ï¼Œéœ€è¦ permute
                dec_out = dec_out.permute(0, 2, 1).contiguous()
                dec_out = self.normalize_layers(dec_out, 'denorm')
            return dec_out

    def analyze_wavelet_features(self, x_input):
        """
        å¯¹è¾“å…¥åºåˆ—è¿›è¡Œå°æ³¢ç‰¹å¾åˆ†æ
        
        Args:
            x_input: (T,) å•å˜é‡æ—¶é—´åºåˆ—
        
        Returns:
            hfer: é«˜é¢‘èƒ½é‡å æ¯” (High Frequency Energy Ratio)
            volatility: æ³¢åŠ¨æ€§æŒ‡æ ‡
            smoothness_level: å¹³æ»‘åº¦ç­‰çº§ (0-4)
        """
        x = x_input.squeeze()
        
        if self.wavelet_prompt_method == 'haar':
            return self._analyze_haar_features(x)
        elif self.wavelet_prompt_method == 'simple':
            return self._analyze_simple_features(x)
        else:
            # é»˜è®¤ä½¿ç”¨Haaræ–¹æ³•
            return self._analyze_haar_features(x)
    
    def _analyze_haar_features(self, x):
        """
        ä½¿ç”¨Haarå°æ³¢åˆ†æç‰¹å¾
        """
        # ç¡®ä¿åºåˆ—é•¿åº¦ä¸ºå¶æ•°ï¼ˆHaarå°æ³¢è¦æ±‚ï¼‰
        if len(x) % 2 == 1:
            x = x[:-1]  # å»æ‰æœ€åä¸€ä¸ªç‚¹
        
        if len(x) < 4:  # åºåˆ—å¤ªçŸ­ï¼Œè¿”å›é»˜è®¤å€¼
            return 0.1, 0.1, 1
        
        # 1. å•çº§Haarå°æ³¢åˆ†è§£
        # ä½é¢‘åˆ†é‡ï¼ˆè¶‹åŠ¿ï¼‰ï¼šç›¸é‚»ç‚¹å¹³å‡
        approx = (x[0::2] + x[1::2]) / 2
        # é«˜é¢‘åˆ†é‡ï¼ˆç»†èŠ‚ï¼‰ï¼šç›¸é‚»ç‚¹å·®å€¼
        detail = (x[0::2] - x[1::2]) / 2
        
        # 2. è®¡ç®—èƒ½é‡æŒ‡æ ‡
        total_energy = torch.sum(x ** 2) + 1e-8  # é¿å…é™¤é›¶
        detail_energy = torch.sum(detail ** 2)
        
        # é«˜é¢‘èƒ½é‡å æ¯”
        hfer = (detail_energy / total_energy).item()
        
        # 3. è®¡ç®—æ³¢åŠ¨æ€§æŒ‡æ ‡
        # é«˜é¢‘åˆ†é‡çš„æ ‡å‡†å·®ï¼ˆå½’ä¸€åŒ–ï¼‰
        volatility = (torch.std(detail) / (torch.std(x) + 1e-8)).item()
        
        # 4. ä½¿ç”¨å¯é…ç½®çš„é˜ˆå€¼è¿›è¡Œå¹³æ»‘åº¦ç­‰çº§é‡åŒ–
        smoothness_level = self._classify_smoothness(hfer)
        
        return hfer, volatility, smoothness_level
    
    def _analyze_simple_features(self, x):
        """
        ä½¿ç”¨ç®€åŒ–çš„é¢‘åŸŸåˆ†ææ–¹æ³•
        """
        if len(x) < 4:
            return 0.1, 0.1, 1
        
        # 1. ç®€å•çš„å·®åˆ†åˆ†æ
        diff1 = torch.diff(x)  # ä¸€é˜¶å·®åˆ†
        diff2 = torch.diff(diff1)  # äºŒé˜¶å·®åˆ†
        
        # 2. è®¡ç®—å˜åŒ–ç‡èƒ½é‡
        signal_energy = torch.sum(x ** 2) + 1e-8
        diff_energy = torch.sum(diff1 ** 2)
        
        # é«˜é¢‘èƒ½é‡å æ¯”ï¼ˆåŸºäºå·®åˆ†ï¼‰
        hfer = (diff_energy / signal_energy).item()
        
        # 3. æ³¢åŠ¨æ€§ï¼ˆåŸºäºäºŒé˜¶å·®åˆ†ï¼‰
        volatility = (torch.std(diff2) / (torch.std(x) + 1e-8)).item()
        
        # 4. å¹³æ»‘åº¦ç­‰çº§
        smoothness_level = self._classify_smoothness(hfer)
        
        return hfer, volatility, smoothness_level
    
    def _classify_smoothness(self, hfer):
        """
        æ ¹æ®å¯é…ç½®çš„é˜ˆå€¼åˆ†ç±»å¹³æ»‘åº¦ç­‰çº§
        """
        # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.15
        base_threshold = self.prompt_hfer_threshold
        
        if hfer < base_threshold * 0.13:  # 0.02 (when base=0.15)
            return 0  # æå¹³æ»‘
        elif hfer < base_threshold * 0.53:  # 0.08 (when base=0.15)
            return 1  # å¾ˆå¹³æ»‘
        elif hfer < base_threshold * 1.33:  # 0.20 (when base=0.15)
            return 2  # ä¸­ç­‰
        elif hfer < base_threshold * 2.67:  # 0.40 (when base=0.15)
            return 3  # æ³¢åŠ¨
        else:
            return 4  # æå˜ˆæ‚
    
    def get_wavelet_description(self, hfer, volatility, smoothness_level):
        """
        å°†å°æ³¢ç‰¹å¾è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
        
        Args:
            hfer: é«˜é¢‘èƒ½é‡å æ¯”
            volatility: æ³¢åŠ¨æ€§æŒ‡æ ‡
            smoothness_level: å¹³æ»‘åº¦ç­‰çº§
        
        Returns:
            wavelet_desc: å°æ³¢ç‰¹å¾çš„è‡ªç„¶è¯­è¨€æè¿°
        """
        # å¹³æ»‘åº¦æè¿°
        smoothness_terms = [
            "extremely smooth and trend-dominated",      # 0
            "very smooth with minimal fluctuations",     # 1
            "moderately smooth with some variations",    # 2
            "volatile with significant fluctuations",    # 3
            "highly volatile and noise-dominated"        # 4
        ]
        
        smoothness_desc = smoothness_terms[smoothness_level]
        
        # æ³¢åŠ¨æ€§å¼ºåº¦æè¿°
        if volatility < 0.3:
            volatility_desc = "low volatility"
        elif volatility < 0.6:
            volatility_desc = "moderate volatility"
        else:
            volatility_desc = "high volatility"
        
        # ç»„åˆæè¿°
        wavelet_desc = f"The signal is {smoothness_desc} with {volatility_desc} (HF energy: {hfer:.1%})"
        
        return wavelet_desc

    def calcute_lags(self, x_enc):
        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)
        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)
        res = q_fft * torch.conj(k_fft)
        corr = torch.fft.irfft(res, dim=-1)
        mean_value = torch.mean(corr, dim=1)
        _, lags = torch.topk(mean_value, self.top_k, dim=-1)
        return lags


class ReprogrammingLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):
        super(ReprogrammingLayer, self).__init__()

        d_keys = d_keys or (d_model // n_heads)

        self.query_projection = nn.Linear(d_model, d_keys * n_heads)
        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)
        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)
        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)
        self.n_heads = n_heads
        self.dropout = nn.Dropout(attention_dropout)

    def forward(self, target_embedding, source_embedding, value_embedding):
        B, L, _ = target_embedding.shape
        S, _ = source_embedding.shape
        H = self.n_heads

        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)
        source_embedding = self.key_projection(source_embedding).view(S, H, -1)
        value_embedding = self.value_projection(value_embedding).view(S, H, -1)

        out = self.reprogramming(target_embedding, source_embedding, value_embedding)

        out = out.reshape(B, L, -1)

        return self.out_projection(out)

    def reprogramming(self, target_embedding, source_embedding, value_embedding):
        B, L, H, E = target_embedding.shape

        scale = 1. / sqrt(E)

        scores = torch.einsum("blhe,she->bhls", target_embedding, source_embedding)

        A = self.dropout(torch.softmax(scale * scores, dim=-1))
        reprogramming_embedding = torch.einsum("bhls,she->blhe", A, value_embedding)

        return reprogramming_embedding


class AdaptiveFusionGate(nn.Module):
    """
    è‡ªé€‚åº”èåˆé—¨æ§ç½‘ç»œ
    
    åŸºäºåŸå§‹è¶‹åŠ¿å’Œç»†èŠ‚ç‰¹å¾åŠ¨æ€è®¡ç®—æ¯ä¸ªä½ç½®çš„èåˆæƒé‡ã€‚
    ç›¸æ¯”å…¨å±€å•ä¸€æƒé‡ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥ç‰¹å¾è‡ªé€‚åº”è°ƒæ•´èåˆç­–ç•¥ã€‚
    
    Args:
        d_model: è¾“å…¥ç‰¹å¾ç»´åº¦
        gate_bias_init: é—¨æ§åç½®åˆå§‹åŒ–å€¼ï¼ˆæ§åˆ¶åˆå§‹åå‘è¶‹åŠ¿è¿˜æ˜¯ç»†èŠ‚ï¼‰
                       0.0=å¹³è¡¡, >0=åå‘è¶‹åŠ¿, <0=åå‘ç»†èŠ‚
    """
    
    def __init__(self, d_model, gate_bias_init=0.0):
        super(AdaptiveFusionGate, self).__init__()
        
        # MLP: 2*d_model -> d_model -> 1
        # è¾“å…¥æ˜¯æ‹¼æ¥çš„è¶‹åŠ¿å’Œç»†èŠ‚ç‰¹å¾ï¼Œè¾“å‡ºæ˜¯é—¨æ§æƒé‡
        self.gate_mlp = nn.Sequential(
            nn.Linear(2 * d_model, d_model),
            nn.ReLU(),
            nn.Linear(d_model, 1),
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–åç½®ï¼Œæ§åˆ¶åˆå§‹èåˆå€¾å‘
        for m in self.gate_mlp.modules():
            if isinstance(m, nn.Linear):
                if m.out_features == 1:  # æœ€åä¸€å±‚
                    nn.init.constant_(m.bias, gate_bias_init)
                else:
                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, trend_embedding, detail_embedding):
        """
        åŸºäºåŸå§‹ç‰¹å¾è®¡ç®—åŠ¨æ€é—¨æ§æƒé‡
        
        Args:
            trend_embedding: (B, L, d_model) åŸå§‹è¶‹åŠ¿ç‰¹å¾
            detail_embedding: (B, L, d_model) åŸå§‹ç»†èŠ‚ç‰¹å¾
        
        Returns:
            gate: (B, L, 1) é—¨æ§æƒé‡ï¼Œå€¼åœ¨[0,1]ä¹‹é—´
                  gateå€¼å¤§è¡¨ç¤ºæ›´å…³æ³¨è¶‹åŠ¿ï¼Œå€¼å°è¡¨ç¤ºæ›´å…³æ³¨ç»†èŠ‚
        """
        # æ‹¼æ¥ç‰¹å¾
        combined = torch.cat([trend_embedding, detail_embedding], dim=-1)  # (B, L, 2*d_model)
        
        # è®¡ç®—é—¨æ§æƒé‡
        gate = self.gate_mlp(combined)  # (B, L, 1)
        
        return gate


class DualReprogrammingLayer(nn.Module):
    """
    åŒåŸå‹é‡ç¼–ç¨‹å±‚
    
    å°†åŸç‰ˆçš„ 1000 ä¸ªåŸå‹è¯æ‹†åˆ†ä¸º N ä¸ªè¶‹åŠ¿åŸå‹å’Œ N ä¸ªç»†èŠ‚åŸå‹ï¼ˆé»˜è®¤ N=1000ï¼‰ï¼Œ
    åˆ†åˆ«ç”¨äºè¶‹åŠ¿æµå’Œç»†èŠ‚æµçš„ Cross-Attentionã€‚
    
    æ¶æ„ï¼š
    1. è¶‹åŠ¿æµï¼štrend_embedding -> Cross-Attention(trend_prototypes) -> sem_trend
    2. ç»†èŠ‚æµï¼šdetail_embedding -> Cross-Attention(detail_prototypes) -> sem_detail
    3. èåˆï¼šç®€å•å¹³å‡ã€åŠ æƒèåˆã€åŠ¨æ€é—¨æ§èåˆã€äº¤é”™æ‹¼æ¥æˆ–é€šé“æ‹¼æ¥
    
    å½“ä¸ WIST ç»“åˆä½¿ç”¨æ—¶ï¼š
    - trend_embedding: WIST è¾“å‡ºçš„ e_cAï¼ˆä½é¢‘è¶‹åŠ¿ç‰¹å¾ï¼‰
    - detail_embedding: WIST è¾“å‡ºçš„ e_detailï¼ˆé«˜é¢‘ç»†èŠ‚ç‰¹å¾ï¼‰
    
    å½“ä¸ä½¿ç”¨ WIST æ—¶ï¼ˆå‘åå…¼å®¹ï¼‰ï¼š
    - trend_embedding å’Œ detail_embedding å¯ä»¥æ˜¯ç›¸åŒçš„èåˆç‰¹å¾
    - ä¸¤ä¸ªæµä½¿ç”¨ç›¸åŒçš„è¾“å…¥ä½†ä¸åŒçš„åŸå‹åº“
    
    Args:
        d_model: è¾“å…¥ç‰¹å¾ç»´åº¦
        n_heads: æ³¨æ„åŠ›å¤´æ•°
        d_keys: æ¯ä¸ªå¤´çš„é”®ç»´åº¦ï¼ˆé»˜è®¤ d_model // n_headsï¼‰
        d_llm: LLMåµŒå…¥ç»´åº¦
        attention_dropout: Attention dropoutç‡
        fusion_method: èåˆæ–¹æ³• ('mean', 'weighted', 'adaptive_gate', 'interleave', 'channel_concat')
        gate_bias_init: åŠ¨æ€é—¨æ§åç½®åˆå§‹åŒ–å€¼ï¼ˆä»…å½“fusion_method='adaptive_gate'æ—¶æœ‰æ•ˆï¼‰
                       0.0=å¹³è¡¡, >0=åå‘è¶‹åŠ¿, <0=åå‘ç»†èŠ‚
    """
    
    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1, 
                 fusion_method='mean', gate_bias_init=0.0):
        super(DualReprogrammingLayer, self).__init__()
        
        # åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„ ReprogrammingLayer
        self.trend_reprogramming = ReprogrammingLayer(d_model, n_heads, d_keys, d_llm, attention_dropout)
        self.detail_reprogramming = ReprogrammingLayer(d_model, n_heads, d_keys, d_llm, attention_dropout)
        
        self.fusion_method = fusion_method
        
        # æ ¹æ®èåˆæ–¹æ³•åˆå§‹åŒ–ä¸åŒçš„ç»„ä»¶
        if fusion_method == 'weighted':
            # åŠ æƒèåˆï¼šå…¨å±€å•ä¸€å¯å­¦ä¹ æƒé‡
            self.fusion_weight = nn.Parameter(torch.tensor(0.5))  # åˆå§‹æƒé‡ 0.5ï¼ˆå¹³è¡¡ï¼‰
            self.fusion_gate = None
            self.fusion_projection = None
        elif fusion_method == 'adaptive_gate':
            # åŠ¨æ€é—¨æ§èåˆï¼šåŸºäºç‰¹å¾è®¡ç®—æ¯ä¸ªä½ç½®çš„æƒé‡
            self.fusion_weight = None
            self.fusion_gate = AdaptiveFusionGate(d_model, gate_bias_init=gate_bias_init)
            self.fusion_projection = None
        elif fusion_method == 'channel_concat':
            # é€šé“æ‹¼æ¥èåˆï¼šåœ¨ç‰¹å¾ç»´åº¦æ‹¼æ¥åæŠ•å½±å›åŸå§‹ç»´åº¦
            # å°† (B, L, 2*d_llm) æŠ•å½±å› (B, L, d_llm)
            self.fusion_weight = None
            self.fusion_gate = None
            self.fusion_projection = nn.Linear(2 * d_llm, d_llm)
        else:
            # meanèåˆï¼šæ— éœ€é¢å¤–å‚æ•°
            self.fusion_weight = None
            self.fusion_gate = None
            self.fusion_projection = None
    
    def forward(self, trend_embedding, detail_embedding, trend_prototypes, detail_prototypes):
        """
        Args:
            trend_embedding: (B, L, d_model) è¶‹åŠ¿ç‰¹å¾ï¼ˆæ¥è‡ª WIST çš„ e_cAï¼‰
            detail_embedding: (B, L, d_model) ç»†èŠ‚ç‰¹å¾ï¼ˆæ¥è‡ª WIST çš„ e_detailï¼‰
            trend_prototypes: (N, d_llm) è¶‹åŠ¿åŸå‹åº“ï¼ŒN ç”± dual_proto_num_tokens æŒ‡å®š
            detail_prototypes: (N, d_llm) ç»†èŠ‚åŸå‹åº“ï¼ŒN ç”± dual_proto_num_tokens æŒ‡å®š
        
        Returns:
            output: (B, L, d_llm) æˆ– (B, 2L, d_llm) è¯­ä¹‰ç©ºé—´è¡¨ç¤º
                    - å¦‚æœ fusion_method='interleave'ï¼Œè¿”å› (B, 2L, d_llm)
                    - å¦åˆ™è¿”å› (B, L, d_llm)
        """
        B, L, _ = trend_embedding.shape
        
        # è¶‹åŠ¿æµï¼šä½¿ç”¨è¶‹åŠ¿ç‰¹å¾å’Œè¶‹åŠ¿åŸå‹åº“
        sem_trend = self.trend_reprogramming(
            trend_embedding, 
            trend_prototypes, 
            trend_prototypes
        )  # (B, L, d_llm)
        
        # ç»†èŠ‚æµï¼šä½¿ç”¨ç»†èŠ‚ç‰¹å¾å’Œç»†èŠ‚åŸå‹åº“
        sem_detail = self.detail_reprogramming(
            detail_embedding,
            detail_prototypes,
            detail_prototypes
        )  # (B, L, d_llm)
        
        # èåˆ
        if self.fusion_method == 'mean':
            # ç®€å•å¹³å‡ï¼šå›ºå®š50/50åˆ†é…
            output = (sem_trend + sem_detail) / 2  # (B, L, d_llm)
        elif self.fusion_method == 'weighted':
            # åŠ æƒèåˆï¼šå…¨å±€å•ä¸€å¯å­¦ä¹ æƒé‡
            weight = torch.sigmoid(self.fusion_weight)
            output = weight * sem_trend + (1 - weight) * sem_detail  # (B, L, d_llm)
        elif self.fusion_method == 'adaptive_gate':
            # åŠ¨æ€é—¨æ§èåˆï¼šåŸºäºåŸå§‹ç‰¹å¾è®¡ç®—æ¯ä¸ªä½ç½®çš„èåˆæƒé‡
            gate = self.fusion_gate(trend_embedding, detail_embedding)  # (B, L, 1)
            output = gate * sem_trend + (1 - gate) * sem_detail  # (B, L, d_llm)
        elif self.fusion_method == 'interleave':
            # äº¤é”™æ‹¼æ¥ï¼šè®©LLMçš„Self-Attentionå­¦ä¹ è¶‹åŠ¿å’Œç»†èŠ‚çš„å…³ç³»
            # [T1, D1, T2, D2, T3, D3, ...]
            # å°† (B, L, d_llm) å’Œ (B, L, d_llm) äº¤é”™æ‹¼æ¥æˆ (B, 2L, d_llm)
            output = torch.stack([sem_trend, sem_detail], dim=2)  # (B, L, 2, d_llm)
            output = output.view(B, 2*L, -1)  # (B, 2L, d_llm)
        elif self.fusion_method == 'channel_concat':
            # é€šé“æ‹¼æ¥ï¼šåœ¨ç‰¹å¾ç»´åº¦æ‹¼æ¥ï¼Œä¿æŒåºåˆ—é•¿åº¦ä¸å˜
            # å°† (B, L, d_llm) å’Œ (B, L, d_llm) åœ¨ç‰¹å¾ç»´åº¦æ‹¼æ¥æˆ (B, L, 2*d_llm)
            # ç„¶åé€šè¿‡æŠ•å½±å±‚æ˜ å°„å› (B, L, d_llm)
            concat_output = torch.cat([sem_trend, sem_detail], dim=-1)  # (B, L, 2*d_llm)
            output = self.fusion_projection(concat_output)  # (B, L, d_llm)
        else:
            raise ValueError(f"æœªçŸ¥çš„èåˆæ–¹æ³•: {self.fusion_method}ï¼Œæ”¯æŒçš„æ–¹æ³•: 'mean', 'weighted', 'adaptive_gate', 'interleave', 'channel_concat'")
        
        return output
