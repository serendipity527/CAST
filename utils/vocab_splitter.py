#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†å·¥å…·

å°†æ•´ä¸ªè¯è¡¨ï¼ˆ50k+ï¼‰é€šè¿‡è¯­ä¹‰è¯„åˆ†åˆ‡åˆ†æˆè¶‹åŠ¿æ¡¶å’Œç»†èŠ‚æ¡¶ï¼Œç”¨äºå…¨è¯è¡¨æ˜ å°„æ–¹æ¡ˆã€‚
"""

import torch
import torch.nn.functional as F
from typing import List, Optional, Tuple
from transformers import PreTrainedTokenizer


def split_full_vocab_by_semantics(
    tokenizer: PreTrainedTokenizer,
    word_embeddings: torch.Tensor,
    trend_anchors: Optional[List[str]] = None,
    detail_anchors: Optional[List[str]] = None,
    verbose: bool = True
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    å°†æ•´ä¸ªè¯è¡¨é€šè¿‡è¯­ä¹‰è¯„åˆ†åˆ‡åˆ†æˆè¶‹åŠ¿æ¡¶å’Œç»†èŠ‚æ¡¶
    
    åŸºäºé”šç‚¹è¯çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå¯¹è¯è¡¨ä¸­çš„æ¯ä¸ªè¯è¿›è¡Œè¯„åˆ†ï¼Œç„¶åé€šè¿‡ç«ä»·æ’å
    å°†è¯åˆ†é…åˆ°è¶‹åŠ¿æ¡¶æˆ–ç»†èŠ‚æ¡¶ã€‚
    
    Args:
        tokenizer: PreTrainedTokenizer å¯¹è±¡
        word_embeddings: (vocab_size, d_llm) è¯åµŒå…¥çŸ©é˜µ
        trend_anchors: è¶‹åŠ¿é”šç‚¹è¯åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤é”šç‚¹ï¼‰
        detail_anchors: ç»†èŠ‚é”šç‚¹è¯åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤é”šç‚¹ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        trend_indices: (N_trend,) è¶‹åŠ¿è¯ç´¢å¼•ï¼ŒLongTensor
        detail_indices: (N_detail,) ç»†èŠ‚è¯ç´¢å¼•ï¼ŒLongTensor
    """
    vocab_size, d_llm = word_embeddings.shape
    vocab_emb_norm = F.normalize(word_embeddings, dim=1)
    
    # 1. å®šä¹‰æ‰©å±•çš„é”šç‚¹è¯ï¼ˆæ¯”ç§å­è¯æ›´å¤šï¼Œè¦†ç›–æ›´å¹¿ï¼‰
    if trend_anchors is None:
        trend_anchors = [
            # è¶‹åŠ¿ç›¸å…³
            'trend', 'pattern', 'cycle', 'period', 'seasonal',
            'upward', 'downward', 'increase', 'decrease', 'growth',
            'rising', 'falling', 'decline', 'ascend', 'descend',
            # å¹³æ»‘/ç¨³å®šç›¸å…³
            'smooth', 'stable', 'steady', 'consistent', 'constant',
            'gradual', 'linear', 'uniform', 'even', 'regular',
            # æ—¶é—´ç›¸å…³
            'time', 'temporal', 'longterm', 'long-term', 'chronic',
            # ç»Ÿè®¡ç›¸å…³ï¼ˆè¶‹åŠ¿ç±»ï¼‰
            'mean', 'average', 'median', 'baseline', 'level',
            # æ–¹å‘ç›¸å…³
            'direction', 'tendency', 'drift', 'shift', 'movement',
            # å˜åŒ–ç›¸å…³ï¼ˆè¶‹åŠ¿ç±»ï¼‰
            'change', 'variation', 'progression', 'evolution',
            # æŒç»­ç›¸å…³
            'persistent', 'sustained', 'continued', 'ongoing',
            # æ‰©å±•ï¼šæ›´å¤šè¶‹åŠ¿ç›¸å…³è¯
            'trending', 'momentum', 'trajectory', 'path', 'course',
            'flow', 'stream', 'current', 'wave', 'tide'
        ]
    
    if detail_anchors is None:
        detail_anchors = [
            # ç»†èŠ‚/æ³¢åŠ¨ç›¸å…³
            'detail', 'fluctuation', 'oscillation', 'vibration', 'variation',
            'volatility', 'deviation', 'divergence', 'disturbance',
            # é«˜é¢‘/å¿«é€Ÿå˜åŒ–ç›¸å…³
            'rapid', 'fast', 'quick', 'sudden', 'abrupt', 'sharp',
            'instant', 'immediate', 'swift', 'abrupt',
            # ç²—ç³™/ä¸è§„åˆ™ç›¸å…³
            'rough', 'irregular', 'uneven', 'erratic', 'chaotic',
            'random', 'unstable', 'turbulent', 'noisy',
            # å°æ³¢/é¢‘åŸŸç›¸å…³
            'frequency', 'spectrum', 'wavelet', 'signal', 'noise',
            'high-frequency', 'highfrequency', 'detail', 'approximation',
            # å˜åŒ–ç›¸å…³ï¼ˆç»†èŠ‚ç±»ï¼‰
            'change', 'shift', 'transition', 'movement', 'variation',
            # æ‰©å±•ï¼šæ›´å¤šç»†èŠ‚ç›¸å…³è¯
            'spike', 'surge', 'jump', 'leap', 'bounce',
            'ripple', 'pulse', 'beat', 'throb', 'flutter'
        ]
    
    # 2. æ”¶é›†é”šç‚¹è¯çš„ token IDs
    trend_ids = []
    detail_ids = []
    
    for word in trend_anchors:
        try:
            ids = tokenizer.encode(word, add_special_tokens=False)
            trend_ids.extend(ids)
        except:
            continue
    
    for word in detail_anchors:
        try:
            ids = tokenizer.encode(word, add_special_tokens=False)
            detail_ids.extend(ids)
        except:
            continue
    
    # å»é‡å¹¶è¿‡æ»¤æ— æ•ˆID
    trend_ids = list(set([idx for idx in trend_ids if 0 <= idx < vocab_size]))
    detail_ids = list(set([idx for idx in detail_ids if 0 <= idx < vocab_size]))
    
    if len(trend_ids) == 0 or len(detail_ids) == 0:
        raise ValueError(f"é”šç‚¹è¯æ”¶é›†å¤±è´¥ï¼šè¶‹åŠ¿é”šç‚¹ {len(trend_ids)} ä¸ªï¼Œç»†èŠ‚é”šç‚¹ {len(detail_ids)} ä¸ª")
    
    # 3. è®¡ç®—é”šç‚¹ä¸­å¿ƒï¼ˆä½¿ç”¨å¹³å‡åµŒå…¥ï¼‰
    center_t = F.normalize(word_embeddings[trend_ids].mean(0, keepdim=True), dim=1)  # (1, d_llm)
    center_d = F.normalize(word_embeddings[detail_ids].mean(0, keepdim=True), dim=1)  # (1, d_llm)
    
    # 4. å…¨é‡æ‰“åˆ†ï¼ˆç«ä»·æ’åï¼‰
    # è®¡ç®—æ¯ä¸ªè¯åˆ°ä¸¤ä¸ªä¸­å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦
    score_t = torch.matmul(vocab_emb_norm, center_t.t()).squeeze()  # (vocab_size,)
    score_d = torch.matmul(vocab_emb_norm, center_d.t()).squeeze()  # (vocab_size,)
    
    # 5. ç«ä»·åˆ‡åˆ†ï¼šæ¯ä¸ªè¯å½’å…¥å¾—åˆ†æ›´é«˜çš„æ¡¶
    mask_trend = score_t > score_d
    trend_indices = torch.where(mask_trend)[0].long()
    detail_indices = torch.where(~mask_trend)[0].long()
    
    if verbose:
        print("=" * 70)
        print("[VocabSplitter] å…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†å®Œæˆ")
        print("=" * 70)
        print(f"  â”œâ”€ è¯è¡¨å¤§å°: {vocab_size:,}")
        print(f"  â”œâ”€ è¶‹åŠ¿é”šç‚¹è¯: {len(trend_anchors)} ä¸ª â†’ {len(trend_ids)} ä¸ªæœ‰æ•ˆ token")
        print(f"  â”œâ”€ ç»†èŠ‚é”šç‚¹è¯: {len(detail_anchors)} ä¸ª â†’ {len(detail_ids)} ä¸ªæœ‰æ•ˆ token")
        print(f"  â”œâ”€ è¶‹åŠ¿æ¡¶å¤§å°: {len(trend_indices):,} ({len(trend_indices)/vocab_size*100:.1f}%)")
        print(f"  â”œâ”€ ç»†èŠ‚æ¡¶å¤§å°: {len(detail_indices):,} ({len(detail_indices)/vocab_size*100:.1f}%)")
        print(f"  â””â”€ åˆ‡åˆ†æ–¹å¼: åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„ç«ä»·æ’å")
        print("=" * 70)
    
    return trend_indices, detail_indices


def print_vocab_split_samples(
    tokenizer: PreTrainedTokenizer,
    trend_indices: torch.Tensor,
    detail_indices: torch.Tensor,
    max_print: int = 20
):
    """
    æ‰“å°åˆ‡åˆ†åçš„è¯è¡¨æ ·æœ¬
    
    Args:
        tokenizer: PreTrainedTokenizer å¯¹è±¡
        trend_indices: è¶‹åŠ¿è¯ç´¢å¼•
        detail_indices: ç»†èŠ‚è¯ç´¢å¼•
        max_print: æ¯ä¸ªæ¡¶æœ€å¤šæ‰“å°çš„è¯æ•°
    """
    print("\n" + "=" * 70)
    print("åˆ‡åˆ†ç»“æœæ ·æœ¬ï¼ˆå‰ {} ä¸ªè¯ï¼‰".format(max_print))
    print("=" * 70)
    
    print("\nğŸ“ˆ è¶‹åŠ¿æ¡¶æ ·æœ¬:")
    trend_list = trend_indices.cpu().tolist()[:max_print]
    for i, idx in enumerate(trend_list, 1):
        try:
            word = tokenizer.decode([idx])
            print(f"  {i:2d}. [{idx:5d}] {word}")
        except:
            print(f"  {i:2d}. [{idx:5d}] <decode_error>")
    
    print("\nğŸ“Š ç»†èŠ‚æ¡¶æ ·æœ¬:")
    detail_list = detail_indices.cpu().tolist()[:max_print]
    for i, idx in enumerate(detail_list, 1):
        try:
            word = tokenizer.decode([idx])
            print(f"  {i:2d}. [{idx:5d}] {word}")
        except:
            print(f"  {i:2d}. [{idx:5d}] <decode_error>")
    
    print("=" * 70)


if __name__ == '__main__':
    """æµ‹è¯•è„šæœ¬"""
    from transformers import GPT2Tokenizer, GPT2Model
    
    print("=" * 70)
    print("æµ‹è¯•å…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†å·¥å…·")
    print("=" * 70)
    
    # åŠ è½½ tokenizer å’Œ model
    try:
        tokenizer = GPT2Tokenizer.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=True
        )
        model = GPT2Model.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=True
        )
        print("âœ… ä»æœ¬åœ°åŠ è½½æ¨¡å‹æˆåŠŸ")
    except:
        tokenizer = GPT2Tokenizer.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=False
        )
        model = GPT2Model.from_pretrained(
            'openai-community/gpt2',
            trust_remote_code=True,
            local_files_only=False
        )
        print("âœ… ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹æˆåŠŸ")
    
    word_embeddings = model.get_input_embeddings().weight
    print(f"âœ… è¯è¡¨å¤§å°: {len(tokenizer):,}, åµŒå…¥ç»´åº¦: {word_embeddings.shape[1]}")
    
    # æµ‹è¯•åˆ‡åˆ†
    print("\n[æ­¥éª¤1] æ‰§è¡Œå…¨è¯è¡¨è¯­ä¹‰åˆ‡åˆ†...")
    trend_indices, detail_indices = split_full_vocab_by_semantics(
        tokenizer=tokenizer,
        word_embeddings=word_embeddings,
        trend_anchors=None,  # ä½¿ç”¨é»˜è®¤é”šç‚¹
        detail_anchors=None,
        verbose=True
    )
    
    # æ‰“å°æ ·æœ¬
    print("\n[æ­¥éª¤2] æ‰“å°åˆ‡åˆ†ç»“æœæ ·æœ¬...")
    print_vocab_split_samples(tokenizer, trend_indices, detail_indices, max_print=30)
    
    # éªŒè¯ä¸ç›¸äº¤
    trend_set = set(trend_indices.cpu().tolist())
    detail_set = set(detail_indices.cpu().tolist())
    overlap = trend_set & detail_set
    
    if overlap:
        print(f"\nâŒ å‘ç° {len(overlap)} ä¸ªé‡å è¯ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰")
    else:
        print("\nâœ… ä¸¤ä¸ªè¯é›†å®Œå…¨ä¸ç›¸äº¤")
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 70)

