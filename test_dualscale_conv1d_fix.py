#!/usr/bin/env python3
"""
DualScaleResidualHead Conv1dä¿®å¤ç‰ˆæµ‹è¯•
éªŒè¯ç”¨Conv1dç§»åŠ¨å¹³å‡æ›¿æ¢GAPåçš„å®ç°æ­£ç¡®æ€§

æµ‹è¯•å†…å®¹:
1. åŸºæœ¬åŠŸèƒ½æµ‹è¯•ï¼šè¾“å…¥è¾“å‡ºå½¢çŠ¶ã€æ¢¯åº¦ä¼ æ’­
2. æ—¶åºä¿¡æ¯ä¿ç•™æµ‹è¯•ï¼šéªŒè¯Conv1dç¡®å®ä¿ç•™äº†æ—¶é—´ç»´åº¦ä¿¡æ¯
3. ç§»åŠ¨å¹³å‡æ•ˆæœéªŒè¯ï¼šç¡®è®¤è¶‹åŠ¿åˆ†æ”¯èƒ½æå–å¹³æ»‘çš„ä½é¢‘ä¿¡æ¯
4. æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šæ–°ç‰ˆ vs æ—§ç‰ˆGAPçš„æ”¶æ•›æ•ˆæœ
5. è¾¹ç•Œæƒ…å†µæµ‹è¯•ï¼šä¸åŒå‚æ•°é…ç½®çš„é²æ£’æ€§

Author: CAST Project  
Date: 2024-12-19
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/dmx_MT/LZF/project/CAST')
from layers.DualScaleHead import DualScaleResidualHead, FlattenHead

class DualScaleResidualHead_GAP_Old(nn.Module):
    """æ—§ç‰ˆGAPå®ç° - ç”¨äºå¯¹æ¯”æµ‹è¯•"""
    def __init__(self, n_vars, d_ff, patch_nums, target_window, head_dropout=0.1, detail_dropout=0.0):
        super().__init__()
        self.n_vars = n_vars
        self.d_ff = d_ff
        self.patch_nums = patch_nums
        self.target_window = target_window
        
        # æ—§ç‰ˆGAPå®ç°
        self.trend_head = nn.Linear(d_ff, target_window)
        self.flatten = nn.Flatten(start_dim=-2)
        self.detail_head = nn.Linear(d_ff * patch_nums, target_window)
        self.detail_dropout = nn.Dropout(detail_dropout) if detail_dropout > 0 else nn.Identity()
        self.output_dropout = nn.Dropout(head_dropout)
        
    def forward(self, x):
        B, N, D, P = x.shape
        
        # æ—§ç‰ˆGAP: æ—¶é—´ä¿¡æ¯ä¸¢å¤±
        trend_features = x.mean(dim=-1)  # (B, N, D, P) -> (B, N, D)
        trend_pred = self.trend_head(trend_features)
        
        # ç»†èŠ‚åˆ†æ”¯
        detail_features = self.flatten(x)
        detail_features = self.detail_dropout(detail_features)
        detail_pred = self.detail_head(detail_features)
        
        final_pred = trend_pred + detail_pred
        final_pred = self.output_dropout(final_pred)
        return final_pred.permute(0, 2, 1).contiguous()

def create_synthetic_time_series(batch_size, n_vars, seq_len, with_trend=True, noise_level=0.1):
    """åˆ›å»ºå¸¦æœ‰æ˜æ˜¾è¶‹åŠ¿çš„åˆæˆæ—¶é—´åºåˆ—æ•°æ®"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ—¶é—´è½´
    t = torch.linspace(0, 4*np.pi, seq_len, device=device)
    
    # åŸºç¡€ä¿¡å·ï¼šè¶‹åŠ¿ + å­£èŠ‚æ€§ + å™ªå£°
    signals = []
    for _ in range(batch_size):
        for _ in range(n_vars):
            if with_trend:
                # çº¿æ€§è¶‹åŠ¿ + æ­£å¼¦æ³¢ + å™ªå£°
                trend = torch.linspace(0, 2, seq_len, device=device)  # ä¸Šå‡è¶‹åŠ¿
                seasonal = 0.5 * torch.sin(t) + 0.3 * torch.sin(2*t)  # å¤šé¢‘ç‡å­£èŠ‚æ€§
                noise = noise_level * torch.randn(seq_len, device=device)
                signal = trend + seasonal + noise
            else:
                # çº¯å™ªå£°
                signal = torch.randn(seq_len, device=device)
            
            signals.append(signal)
    
    # é‡å¡‘ä¸º (batch_size, n_vars, seq_len)
    data = torch.stack(signals).view(batch_size, n_vars, seq_len)
    return data

def test_basic_functionality():
    """æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½éªŒè¯"""
    print("=" * 70)
    print("æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½éªŒè¯")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æµ‹è¯•é…ç½®
    B, N, D, P, T = 4, 7, 32, 10, 96
    
    # åˆ›å»ºæ¨¡å‹
    conv1d_head = DualScaleResidualHead(
        n_vars=N, d_ff=D, patch_nums=P, target_window=T,
        head_dropout=0.1, detail_dropout=0.0, trend_kernel_size=5
    ).to(device)
    
    gap_head = DualScaleResidualHead_GAP_Old(
        n_vars=N, d_ff=D, patch_nums=P, target_window=T,
        head_dropout=0.1, detail_dropout=0.0
    ).to(device)
    
    # æµ‹è¯•è¾“å…¥
    x = torch.randn(B, N, D, P, device=device, requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    conv1d_output = conv1d_head(x)
    gap_output = gap_head(x.clone().detach().requires_grad_(True))
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (B, T, N)
    assert conv1d_output.shape == expected_shape, f"Conv1dç‰ˆæœ¬è¾“å‡ºå½¢çŠ¶é”™è¯¯: {conv1d_output.shape}"
    assert gap_output.shape == expected_shape, f"GAPç‰ˆæœ¬è¾“å‡ºå½¢çŠ¶é”™è¯¯: {gap_output.shape}"
    
    print(f"âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {conv1d_output.shape}")
    
    # æµ‹è¯•æ¢¯åº¦ä¼ æ’­
    target = torch.randn_like(conv1d_output)
    conv1d_loss = F.mse_loss(conv1d_output, target)
    gap_loss = F.mse_loss(gap_output, target)
    
    conv1d_loss.backward()
    gap_loss.backward()
    
    assert x.grad is not None, "Conv1dç‰ˆæœ¬æ¢¯åº¦ä¼ æ’­å¤±è´¥"
    print(f"âœ… æ¢¯åº¦ä¼ æ’­æ­£å¸¸ï¼Œæ¢¯åº¦èŒƒæ•°: {x.grad.norm().item():.6f}")
    
    # å‚æ•°é‡å¯¹æ¯”
    conv1d_params = sum(p.numel() for p in conv1d_head.parameters())
    gap_params = sum(p.numel() for p in gap_head.parameters())
    
    print(f"Conv1dç‰ˆæœ¬å‚æ•°é‡: {conv1d_params:,}")
    print(f"GAPç‰ˆæœ¬å‚æ•°é‡: {gap_params:,}")
    print(f"å‚æ•°å¢åŠ : {conv1d_params - gap_params:,} ({(conv1d_params/gap_params-1)*100:.2f}%)")
    
def test_temporal_preservation():
    """æµ‹è¯•2: æ—¶åºä¿¡æ¯ä¿ç•™éªŒè¯"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•2: æ—¶åºä¿¡æ¯ä¿ç•™éªŒè¯")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæœ‰æ˜ç¡®æ—¶åºæ¨¡å¼çš„è¾“å…¥
    B, N, D, P = 2, 3, 16, 20
    
    # æ„é€ è¾“å…¥ï¼šæ¯ä¸ªpatchæœ‰ä¸åŒçš„æ—¶åºæ¨¡å¼
    x = torch.zeros(B, N, D, P, device=device)
    
    # ä¸ºæ¯ä¸ªpatchä½ç½®èµ‹äºˆä¸åŒçš„å€¼ï¼Œæ¨¡æ‹Ÿæ—¶åºå˜åŒ–
    for p in range(P):
        x[:, :, :, p] = p / P  # çº¿æ€§é€’å¢æ¨¡å¼
    
    # æ·»åŠ å°‘é‡å™ªå£°
    x += 0.01 * torch.randn_like(x)
    
    # åˆ›å»ºæ¨¡å‹
    conv1d_head = DualScaleResidualHead(
        n_vars=N, d_ff=D, patch_nums=P, target_window=96,
        trend_kernel_size=5
    ).to(device)
    
    gap_head = DualScaleResidualHead_GAP_Old(
        n_vars=N, d_ff=D, patch_nums=P, target_window=96
    ).to(device)
    
    # è·å–è¶‹åŠ¿åˆ†é‡
    conv1d_head.eval()
    gap_head.eval()
    
    with torch.no_grad():
        # Conv1dç‰ˆæœ¬çš„ä¸­é—´ç‰¹å¾
        trend_input = x.view(B * N, D, P)
        conv1d_trend_smooth = conv1d_head.trend_conv(trend_input)  # ä¿ç•™äº†æ—¶åº
        
        # GAPç‰ˆæœ¬çš„ä¸­é—´ç‰¹å¾  
        gap_trend_features = x.mean(dim=-1)  # ä¸¢å¤±äº†æ—¶åº
    
    # éªŒè¯Conv1dä¿ç•™äº†æ—¶åºå˜åŒ–
    conv1d_trend_var = conv1d_trend_smooth.var(dim=-1).mean().item()  # patchç»´åº¦çš„æ–¹å·®
    gap_trend_var = 0  # GAPåæ²¡æœ‰patchç»´åº¦äº†
    
    print(f"Conv1dè¶‹åŠ¿ç‰¹å¾çš„æ—¶åºæ–¹å·®: {conv1d_trend_var:.6f}")
    print(f"GAPè¶‹åŠ¿ç‰¹å¾çš„æ—¶åºæ–¹å·®: {gap_trend_var:.6f}")
    
    assert conv1d_trend_var > 0.001, "Conv1dç‰ˆæœ¬åº”è¯¥ä¿ç•™æ—¶åºå˜åŒ–"
    print("âœ… Conv1dç‰ˆæœ¬æˆåŠŸä¿ç•™äº†æ—¶åºä¿¡æ¯")
    
    # å¯è§†åŒ–å¯¹æ¯” (å¦‚æœåœ¨æœ‰GUIç¯å¢ƒä¸­)
    try:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # åŸå§‹è¾“å…¥çš„æ—¶åºæ¨¡å¼
        sample_patch_evolution = x[0, 0, 0, :].cpu().numpy()  # ç¬¬ä¸€ä¸ªå˜é‡ï¼Œç¬¬ä¸€ä¸ªç‰¹å¾ç»´åº¦
        ax1.plot(sample_patch_evolution, 'o-', label='åŸå§‹patchåºåˆ—')
        ax1.set_title('åŸå§‹è¾“å…¥çš„æ—¶åºæ¨¡å¼')
        ax1.set_xlabel('Patchç´¢å¼•')
        ax1.set_ylabel('ç‰¹å¾å€¼')
        ax1.legend()
        
        # Conv1då¹³æ»‘åçš„æ—¶åºæ¨¡å¼
        conv1d_smooth_evolution = conv1d_trend_smooth[0, 0, :].cpu().numpy()
        ax2.plot(sample_patch_evolution, 'o-', alpha=0.5, label='åŸå§‹')
        ax2.plot(conv1d_smooth_evolution, 's-', label='Conv1då¹³æ»‘å')
        ax2.set_title('Conv1dç§»åŠ¨å¹³å‡æ•ˆæœ')
        ax2.set_xlabel('Patchç´¢å¼•')
        ax2.set_ylabel('ç‰¹å¾å€¼')
        ax2.legend()
        
        plt.tight_layout()
        plt.savefig('/tmp/temporal_preservation_test.png', dpi=150, bbox_inches='tight')
        print("âœ… æ—¶åºä¿ç•™å¯è§†åŒ–å·²ä¿å­˜è‡³ /tmp/temporal_preservation_test.png")
        plt.close()
        
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–è·³è¿‡ (æ— GUIç¯å¢ƒ): {e}")

def test_moving_average_effect():
    """æµ‹è¯•3: ç§»åŠ¨å¹³å‡æ•ˆæœéªŒè¯"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•3: ç§»åŠ¨å¹³å‡æ•ˆæœéªŒè¯") 
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    B, N, D, P = 1, 1, 8, 50
    
    # åˆ›å»ºå«å™ªå£°çš„ä¿¡å·
    clean_signal = torch.sin(torch.linspace(0, 4*np.pi, P, device=device))
    noise = 0.3 * torch.randn(P, device=device)
    noisy_signal = clean_signal + noise
    
    # æ„é€ è¾“å…¥ (é‡å¤åˆ°æ‰€æœ‰ç»´åº¦)
    x = noisy_signal.view(1, 1, 1, P).repeat(B, N, D, 1)
    
    # æµ‹è¯•ä¸åŒå·ç§¯æ ¸å¤§å°çš„å¹³æ»‘æ•ˆæœ
    kernel_sizes = [3, 7, 15, 25]
    
    print("å·ç§¯æ ¸å¤§å° | å¹³æ»‘æ•ˆæœ(ä¸æ¸…æ´ä¿¡å·çš„ç›¸ä¼¼åº¦)")
    print("-" * 50)
    
    best_similarity = 0
    best_kernel = None
    
    for k in kernel_sizes:
        head = DualScaleResidualHead(
            n_vars=N, d_ff=D, patch_nums=P, target_window=96,
            trend_kernel_size=k
        ).to(device)
        
        head.eval()
        with torch.no_grad():
            trend_input = x.view(B * N, D, P)
            smoothed = head.trend_conv(trend_input)
            
        # è®¡ç®—ä¸æ¸…æ´ä¿¡å·çš„ç›¸ä¼¼åº¦
        smoothed_signal = smoothed[0, 0, :].cpu()
        similarity = F.cosine_similarity(
            clean_signal.cpu().unsqueeze(0), 
            smoothed_signal.unsqueeze(0)
        ).item()
        
        print(f"kernel={k:2d}    | {similarity:.4f}")
        
        if similarity > best_similarity:
            best_similarity = similarity
            best_kernel = k
    
    print(f"\nâœ… æœ€ä½³å¹³æ»‘æ ¸å¤§å°: {best_kernel} (ç›¸ä¼¼åº¦: {best_similarity:.4f})")
    
    # éªŒè¯å¹³æ»‘æ•ˆæœç¡®å®é™ä½äº†å™ªå£°
    assert best_similarity > 0.7, f"ç§»åŠ¨å¹³å‡æ•ˆæœä¸ä½³ï¼Œæœ€ä½³ç›¸ä¼¼åº¦ä»… {best_similarity:.4f}"
    print("âœ… ç§»åŠ¨å¹³å‡æˆåŠŸé™ä½å™ªå£°å¹¶ä¿ç•™è¶‹åŠ¿")

def test_convergence_comparison():
    """æµ‹è¯•4: æ”¶æ•›æ•ˆæœå¯¹æ¯”"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•4: æ”¶æ•›æ•ˆæœå¯¹æ¯”")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    torch.manual_seed(42)
    
    B, N, D, P, T = 4, 3, 16, 12, 24
    
    # åˆ›å»ºæ¨¡å‹
    conv1d_head = DualScaleResidualHead(
        n_vars=N, d_ff=D, patch_nums=P, target_window=T,
        trend_kernel_size=7
    ).to(device)
    
    gap_head = DualScaleResidualHead_GAP_Old(
        n_vars=N, d_ff=D, patch_nums=P, target_window=T
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    conv1d_optim = torch.optim.Adam(conv1d_head.parameters(), lr=0.001)
    gap_optim = torch.optim.Adam(gap_head.parameters(), lr=0.001)
    
    # è®­ç»ƒæ•°æ®ï¼šæœ‰æ˜æ˜¾è¶‹åŠ¿çš„æ—¶é—´åºåˆ—
    n_steps = 100
    conv1d_losses = []
    gap_losses = []
    
    for step in range(n_steps):
        # ç”Ÿæˆæœ‰è¶‹åŠ¿çš„æ•°æ®
        target_data = create_synthetic_time_series(B, N, T, with_trend=True, noise_level=0.1)
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å‡ºæ ¼å¼ (B, N, T) -> (B, T, N)
        target_data = target_data.permute(0, 2, 1).contiguous()
        input_data = torch.randn(B, N, D, P, device=device)
        
        # Conv1dç‰ˆæœ¬è®­ç»ƒ
        conv1d_optim.zero_grad()
        conv1d_pred = conv1d_head(input_data)
        conv1d_loss = F.mse_loss(conv1d_pred, target_data)
        conv1d_loss.backward()
        conv1d_optim.step()
        conv1d_losses.append(conv1d_loss.item())
        
        # GAPç‰ˆæœ¬è®­ç»ƒ
        gap_optim.zero_grad()
        gap_pred = gap_head(input_data)
        gap_loss = F.mse_loss(gap_pred, target_data)
        gap_loss.backward()
        gap_optim.step()
        gap_losses.append(gap_loss.item())
    
    # åˆ†ææ”¶æ•›æ•ˆæœ
    conv1d_final = np.mean(conv1d_losses[-10:])
    gap_final = np.mean(gap_losses[-10:])
    
    conv1d_improvement = (conv1d_losses[0] - conv1d_final) / conv1d_losses[0] * 100
    gap_improvement = (gap_losses[0] - gap_final) / gap_losses[0] * 100
    
    print(f"Conv1dç‰ˆæœ¬ - åˆå§‹æŸå¤±: {conv1d_losses[0]:.6f}, æœ€ç»ˆæŸå¤±: {conv1d_final:.6f}")
    print(f"GAPç‰ˆæœ¬   - åˆå§‹æŸå¤±: {gap_losses[0]:.6f}, æœ€ç»ˆæŸå¤±: {gap_final:.6f}")
    print(f"Conv1dæ”¹è¿›: {conv1d_improvement:.2f}%, GAPæ”¹è¿›: {gap_improvement:.2f}%")
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
    if conv1d_final < gap_final:
        improvement = (gap_final - conv1d_final) / gap_final * 100
        print(f"âœ… Conv1dç‰ˆæœ¬æ”¶æ•›æ•ˆæœæ›´å¥½ï¼ŒæŸå¤±é™ä½ {improvement:.2f}%")
    else:
        print("âš ï¸ æœ¬æ¬¡æµ‹è¯•ä¸­GAPç‰ˆæœ¬è¡¨ç°æ›´å¥½ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ­¥éª¤æˆ–è°ƒæ•´è¶…å‚æ•°")
    
    # ä¿å­˜æŸå¤±æ›²çº¿
    try:
        plt.figure(figsize=(10, 6))
        plt.plot(conv1d_losses, label='Conv1dç‰ˆæœ¬', alpha=0.8)
        plt.plot(gap_losses, label='GAPç‰ˆæœ¬', alpha=0.8)
        plt.xlabel('è®­ç»ƒæ­¥éª¤')
        plt.ylabel('MSEæŸå¤±')
        plt.title('æ”¶æ•›æ•ˆæœå¯¹æ¯”')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        plt.savefig('/tmp/convergence_comparison.png', dpi=150, bbox_inches='tight')
        print("âœ… æ”¶æ•›å¯¹æ¯”å›¾å·²ä¿å­˜è‡³ /tmp/convergence_comparison.png")
        plt.close()
    except:
        print("âš ï¸ å›¾è¡¨ä¿å­˜è·³è¿‡")

def test_gradient_flow():
    """æµ‹è¯•5: æ¢¯åº¦æµåˆ†æ"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•5: æ¢¯åº¦æµåˆ†æ")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    B, N, D, P, T = 2, 2, 8, 6, 12
    
    head = DualScaleResidualHead(
        n_vars=N, d_ff=D, patch_nums=P, target_window=T,
        trend_kernel_size=5
    ).to(device)
    
    x = torch.randn(B, N, D, P, device=device, requires_grad=True)
    target = torch.randn(B, T, N, device=device)
    
    # å‰å‘ä¼ æ’­
    output = head(x)
    loss = F.mse_loss(output, target)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥å„å±‚æ¢¯åº¦
    print("æ¢¯åº¦æµæ£€æŸ¥:")
    print("-" * 40)
    
    grad_norms = {}
    
    # è¾“å…¥æ¢¯åº¦
    if x.grad is not None:
        grad_norms['input'] = x.grad.norm().item()
        print(f"è¾“å…¥æ¢¯åº¦èŒƒæ•°:        {grad_norms['input']:.6f}")
    
    # å„å±‚å‚æ•°æ¢¯åº¦
    for name, param in head.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_norms[name] = grad_norm
            print(f"{name:<20}: {grad_norm:.6f}")
    
    # éªŒè¯æ²¡æœ‰æ¢¯åº¦æ¶ˆå¤±
    min_grad = min(grad_norms.values())
    max_grad = max(grad_norms.values())
    
    assert min_grad > 1e-8, f"æ£€æµ‹åˆ°æ¢¯åº¦æ¶ˆå¤±: æœ€å°æ¢¯åº¦ {min_grad}"
    assert max_grad < 1e3, f"æ£€æµ‹åˆ°æ¢¯åº¦çˆ†ç‚¸: æœ€å¤§æ¢¯åº¦ {max_grad}"
    
    print(f"\nâœ… æ¢¯åº¦èŒƒå›´å¥åº·: [{min_grad:.2e}, {max_grad:.2e}]")

def test_edge_cases():
    """æµ‹è¯•6: è¾¹ç•Œæƒ…å†µ"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•6: è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    test_configs = [
        # (B, N, D, P, T, kernel_size, æè¿°)
        (1, 1, 4, 3, 5, 3, "æœ€å°é…ç½®"),
        (2, 3, 8, 5, 12, 5, "å°é…ç½®"),
        (4, 7, 32, 64, 96, 25, "ETTh1æ ‡å‡†é…ç½®"),
        (8, 12, 64, 128, 336, 51, "å¤§é…ç½®"),
    ]
    
    for i, (B, N, D, P, T, k, desc) in enumerate(test_configs):
        print(f"\né…ç½®{i+1}: {desc}")
        print(f"  å½¢çŠ¶: B={B}, N={N}, D={D}, P={P}, T={T}, kernel={k}")
        
        try:
            # åˆ›å»ºæ¨¡å‹
            head = DualScaleResidualHead(
                n_vars=N, d_ff=D, patch_nums=P, target_window=T,
                trend_kernel_size=k
            ).to(device)
            
            # æµ‹è¯•è¾“å…¥
            x = torch.randn(B, N, D, P, device=device)
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                output = head(x)
            
            # éªŒè¯è¾“å‡ºå½¢çŠ¶
            expected_shape = (B, T, N)
            assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape} vs {expected_shape}"
            
            # éªŒè¯æ•°å€¼ç¨³å®šæ€§
            assert torch.isfinite(output).all(), "è¾“å‡ºåŒ…å«NaNæˆ–Inf"
            
            print(f"  âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}, æ•°å€¼èŒƒå›´: [{output.min().item():.3f}, {output.max().item():.3f}]")
            
        except Exception as e:
            print(f"  âŒ é…ç½®{i+1}å¤±è´¥: {e}")
            raise e
    
    print("\nâœ… æ‰€æœ‰è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ DualScaleResidualHead Conv1dä¿®å¤ç‰ˆ - å…¨é¢æµ‹è¯•")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        test_basic_functionality()
        test_temporal_preservation()  
        test_moving_average_effect()
        test_convergence_comparison()
        test_gradient_flow()
        test_edge_cases()
        
        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Conv1dä¿®å¤ç‰ˆå®ç°æ­£ç¡®")
        print("=" * 70)
        
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print("  âœ… åŸºæœ¬åŠŸèƒ½ï¼šè¾“å…¥è¾“å‡ºå½¢çŠ¶ã€å‚æ•°é‡ã€æ¢¯åº¦ä¼ æ’­æ­£å¸¸")
        print("  âœ… æ—¶åºä¿ç•™ï¼šæˆåŠŸæ›¿æ¢GAPï¼Œä¿ç•™äº†patché—´çš„æ—¶é—´å…³ç³»")
        print("  âœ… å¹³æ»‘æ•ˆæœï¼šç§»åŠ¨å¹³å‡æœ‰æ•ˆé™å™ªå¹¶çªå‡ºè¶‹åŠ¿ä¿¡æ¯")
        print("  âœ… æ”¶æ•›æ€§èƒ½ï¼šåœ¨æœ‰è¶‹åŠ¿æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½")
        print("  âœ… æ¢¯åº¦å¥åº·ï¼šæ— æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜")
        print("  âœ… é²æ£’æ€§ï¼šå„ç§é…ç½®ä¸‹éƒ½ç¨³å®šå·¥ä½œ")
        
        print(f"\nğŸ”§ ä¿®å¤è¦ç‚¹å›é¡¾:")
        print(f"  - é—®é¢˜ï¼šåŸç‰ˆGAPå°†æ—¶åºä¿¡æ¯å‹ç¼©ä¸ºå•ä¸ªæ ‡é‡")
        print(f"  - è§£å†³ï¼šä½¿ç”¨depthwise Conv1dè¿›è¡Œç§»åŠ¨å¹³å‡")
        print(f"  - ä¼˜åŠ¿ï¼šä¿ç•™æ—¶åº + å¹³æ»‘å™ªå£° + å‚æ•°é«˜æ•ˆ + ç†è®ºæ­£ç¡®")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise e

if __name__ == "__main__":
    run_all_tests()
